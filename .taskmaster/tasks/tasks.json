{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Initialize TypeScript Project Structure",
        "description": "Set up the TypeScript project foundation including package.json, tsconfig.json, vitest configuration, and directory scaffold. This replaces the existing bash monolith with a maintainable TypeScript CLI.",
        "details": "Create the following files in the project root:\n\n**package.json:**\n```json\n{\n  \"name\": \"ai-helper-tools\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Bootstrap tool for AI-assisted development environments\",\n  \"type\": \"module\",\n  \"bin\": { \"ai-init\": \"./dist/cli.js\" },\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"dev\": \"tsx src/cli.ts\",\n    \"test\": \"vitest run\",\n    \"test:watch\": \"vitest\",\n    \"lint\": \"eslint src test --ext .ts\",\n    \"typecheck\": \"tsc --noEmit\",\n    \"format\": \"prettier --write .\"\n  },\n  \"dependencies\": {\n    \"meow\": \"latest\",\n    \"@inquirer/prompts\": \"latest\"\n  },\n  \"devDependencies\": {\n    \"typescript\": \"latest\",\n    \"vitest\": \"latest\",\n    \"tsx\": \"latest\",\n    \"@types/node\": \"latest\",\n    \"eslint\": \"latest\",\n    \"@typescript-eslint/eslint-plugin\": \"latest\",\n    \"@typescript-eslint/parser\": \"latest\",\n    \"prettier\": \"latest\"\n  }\n}\n```\n\n**tsconfig.json:**\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"bundler\",\n    \"outDir\": \"dist\",\n    \"rootDir\": \"src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"declaration\": true\n  },\n  \"include\": [\"src\"]\n}\n```\n\n**vitest.config.ts:**\n```typescript\nimport { defineConfig } from 'vitest/config';\nexport default defineConfig({\n  test: {\n    globals: true,\n    include: ['test/**/*.test.ts'],\n  },\n});\n```\n\nCreate directory structure:\n```\nsrc/\n  phases/\n  generators/\ntemplates/\n  docs/\n  rules/\n  skills/\n  hooks/\n  commands/\ntest/\n  generators/\n  integration/\n  fixtures/\ndist/  (gitignored)\n```\n\nAdd to .gitignore: `dist/`, `node_modules/`\n\nRun `npm install` after creating package.json.\n\nVerify Node.js >= 20 is available (`node --version`).",
        "testStrategy": "Smoke test: `npm run build` succeeds with zero errors. `npm run typecheck` passes. `npm run test` runs (even with zero tests). Verify `dist/` directory is created after build.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Verify prerequisites and create package.json with dependencies",
            "description": "Check Node.js >= 20 is available, then create package.json with all required dependencies and scripts. Run npm install to fetch packages and generate package-lock.json.",
            "dependencies": [],
            "details": "1. Confirm `node --version` outputs v20+ (v24.11.1 is confirmed available). 2. Create `/workspaces/ai-dev-setup/package.json` with `\"type\": \"module\"`, `\"bin\": { \"ai-init\": \"./dist/cli.js\" }`, scripts (`build`, `dev`, `test`, `test:watch`, `lint`, `typecheck`, `format`), runtime dependencies (`meow@latest`, `@inquirer/prompts@latest`), and devDependencies (`typescript@latest`, `vitest@latest`, `tsx@latest`, `@types/node@latest`, `eslint@latest`, `@typescript-eslint/eslint-plugin@latest`, `@typescript-eslint/parser@latest`, `prettier@latest`). 3. Run `npm install` in the project root to resolve and lock all versions in `package-lock.json`. 4. Verify `node_modules/` is created and key binaries exist (e.g., `npx tsc --version`).",
            "status": "done",
            "testStrategy": "Run `npm list --depth=0` and confirm all 9 devDependencies and 2 runtime dependencies are listed without errors.",
            "updatedAt": "2026-02-17T19:34:56.695Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create tsconfig.json and vitest.config.ts",
            "description": "Add TypeScript compiler configuration targeting ES2022 with ESNext modules and bundler resolution, plus a vitest configuration that picks up tests from the test/ directory.",
            "dependencies": [
              1
            ],
            "details": "1. Create `/workspaces/ai-dev-setup/tsconfig.json` with `compilerOptions`: `target: ES2022`, `module: ESNext`, `moduleResolution: bundler`, `outDir: dist`, `rootDir: src`, `strict: true`, `esModuleInterop: true`, `skipLibCheck: true`, `declaration: true`; `include: [\"src\"]`. 2. Create `/workspaces/ai-dev-setup/vitest.config.ts` importing `defineConfig` from `vitest/config`, exporting config with `test.globals: true` and `test.include: ['test/**/*.test.ts']`. 3. Run `npx tsc --noEmit --project tsconfig.json` (will error on missing src/ but confirms tsconfig is parsed correctly — no syntax errors). 4. Run `npx vitest --version` to confirm vitest CLI resolves.",
            "status": "done",
            "testStrategy": "Run `npx tsc --version` and `npx vitest --version` — both must print version strings without errors. Running `npm run typecheck` may report 'no input files' which is acceptable at this stage (src/ is empty).",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:34:56.708Z"
          },
          {
            "id": 3,
            "title": "Scaffold directory structure under src/, templates/, test/, and dist/",
            "description": "Create all required empty directories so subsequent tasks can place files in the correct locations without needing to create parent directories themselves.",
            "dependencies": [
              1
            ],
            "details": "Create the following directories (use `mkdir -p` for nested paths): `src/phases/`, `src/generators/`, `templates/docs/`, `templates/rules/`, `templates/skills/`, `templates/hooks/`, `templates/commands/`, `test/generators/`, `test/integration/`, `test/fixtures/`. Each directory should contain a `.gitkeep` file so the structure is committed to git. The `dist/` directory will be created by the TypeScript compiler and should NOT contain a `.gitkeep` — it is gitignored.",
            "status": "done",
            "testStrategy": "Run `find src templates test -type d | sort` and verify all 10 directories appear. Confirm `dist/` does not exist yet (it is created only after `npm run build`).",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:34:56.729Z"
          },
          {
            "id": 4,
            "title": "Update .gitignore to exclude dist/ and ensure node_modules/ is covered",
            "description": "Add `dist/` to the existing .gitignore file so compiled output is never committed. Confirm `node_modules/` is already present (it is) so no duplicate entry is needed.",
            "dependencies": [
              1
            ],
            "details": "1. Read the existing `/workspaces/ai-dev-setup/.gitignore`. 2. It already contains `node_modules/` on line 10. 3. Append `dist/` as a new line (and optionally `*.tsbuildinfo` for incremental build cache). 4. Do NOT remove any existing entries — only add the missing `dist/` line. 5. Verify `.vscode` is in .gitignore (it already is on line 18 — no change needed). The final .gitignore should have both `node_modules/` and `dist/` entries.",
            "status": "done",
            "testStrategy": "Run `grep -E '^dist/$' .gitignore` — must output `dist/`. Run `git check-ignore -v dist/` after creating a dummy `dist/` directory to confirm git ignores it.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:34:56.734Z"
          },
          {
            "id": 5,
            "title": "Create a minimal src/cli.ts entry point and verify the full build pipeline",
            "description": "Add a minimal TypeScript entry point so that `npm run build`, `npm run typecheck`, and `npm run test` all succeed with zero errors, satisfying the smoke-test acceptance criteria for this task.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "1. Create `/workspaces/ai-dev-setup/src/cli.ts` with a minimal shebang and main guard: `#!/usr/bin/env node` followed by `console.log('ai-init v0.1.0');`. This gives the TypeScript compiler a valid input file. 2. Run `npm run build` — this executes `tsc` and should produce `dist/cli.js` and `dist/cli.d.ts` with zero errors. 3. Run `npm run typecheck` — must exit 0. 4. Run `npm run test` — vitest will run with no test files matched; this is acceptable (exits 0 with 'No test files found'). 5. Optionally run `node dist/cli.js` to confirm the compiled output executes without runtime errors.",
            "status": "done",
            "testStrategy": "All four commands must succeed: `npm run build` (exit 0, dist/ populated), `npm run typecheck` (exit 0), `npm run test` (exit 0 or 'no tests' warning — not an error), `node dist/cli.js` prints version string.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:36:39.535Z"
          }
        ],
        "updatedAt": "2026-02-17T19:36:39.535Z"
      },
      {
        "id": "2",
        "title": "Define Core Types and ProjectConfig Interface",
        "description": "Create the shared TypeScript types that all generators and phases will use. The central `ProjectConfig` type drives all code generation — it must capture every user choice from the wizard.",
        "details": "Create `src/types.ts`:\n\n```typescript\nexport type TaskTracker = 'taskmaster' | 'beads' | 'markdown';\nexport type Architecture = 'monolith' | '2-tier' | '3-tier' | 'microservices' | 'skip';\n\nexport interface McpServer {\n  name: string;\n  description: string;\n  npmPackage: string;\n  claudeMcpName: string;\n  required: boolean;\n  args?: string[];\n  env?: Record<string, string>;\n}\n\nexport interface FileDescriptor {\n  path: string;\n  content: string;\n  executable?: boolean;\n}\n\nexport interface ProjectConfig {\n  // MCP selections\n  selectedMcps: string[];\n  // Task tracker\n  taskTracker: TaskTracker;\n  // Architecture\n  architecture: Architecture;\n  // PRD\n  prdPath?: string;       // path to existing PRD, or undefined if using template\n  hasPrd: boolean;\n  // Feature flags\n  generateDocs: boolean;\n  generateRules: boolean;\n  generateSkills: boolean;\n  generateHooks: boolean;\n  generateCommands: boolean;\n  agentTeamsEnabled: boolean;\n  runAudit: boolean;\n  // Derived from selections\n  hasApiDocs: boolean;    // whether docs/api.md should be generated\n  hasDatabase: boolean;   // whether database rules should be generated\n  // Project metadata\n  projectName: string;\n  projectRoot: string;\n  // Tracking for audit\n  generatedFiles: string[];\n}\n\nexport interface AuditResult {\n  passes: string[];\n  fills: { file: string; section: string; message: string }[];\n  fixes: { file: string; issue: string; fix: string }[];\n  postSetupChecklist: string[];\n}\n```\n\nCreate `src/defaults.ts`:\n```typescript\nimport { ProjectConfig } from './types.js';\nimport path from 'node:path';\n\nexport function defaultConfig(projectRoot: string): ProjectConfig {\n  return {\n    selectedMcps: ['taskmaster'],\n    taskTracker: 'taskmaster',\n    architecture: 'skip',\n    hasPrd: false,\n    generateDocs: true,\n    generateRules: true,\n    generateSkills: true,\n    generateHooks: true,\n    generateCommands: true,\n    agentTeamsEnabled: false,\n    runAudit: true,\n    hasApiDocs: false,\n    hasDatabase: false,\n    projectName: path.basename(projectRoot),\n    projectRoot,\n    generatedFiles: [],\n  };\n}\n```",
        "testStrategy": "TypeScript type-check (`tsc --noEmit`) must pass with zero errors. Write a unit test `test/generators/types.test.ts` that imports `ProjectConfig` and `FileDescriptor` and verifies default config shape matches expected defaults.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/types.ts with all exported TypeScript type definitions",
            "description": "Create the `src/types.ts` file containing all shared TypeScript types: `TaskTracker`, `Architecture`, `McpServer`, `FileDescriptor`, `ProjectConfig`, and `AuditResult` interfaces as specified in the task details.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/src/types.ts` with strict TypeScript. Export `TaskTracker` as a union type of `'taskmaster' | 'beads' | 'markdown'`. Export `Architecture` as `'monolith' | '2-tier' | '3-tier' | 'microservices' | 'skip'`. Export `McpServer` interface with fields: `name`, `description`, `npmPackage`, `claudeMcpName`, `required`, optional `args?: string[]`, optional `env?: Record<string, string>`. Export `FileDescriptor` interface with `path`, `content`, and optional `executable?: boolean`. Export `ProjectConfig` interface with all fields listed in the task: `selectedMcps`, `taskTracker`, `architecture`, `prdPath?`, `hasPrd`, `generateDocs`, `generateRules`, `generateSkills`, `generateHooks`, `generateCommands`, `agentTeamsEnabled`, `runAudit`, `hasApiDocs`, `hasDatabase`, `projectName`, `projectRoot`, `generatedFiles`. Export `AuditResult` interface with `passes`, `fills` (array of objects with `file`, `section`, `message`), `fixes` (array of objects with `file`, `issue`, `fix`), and `postSetupChecklist`. The file must use ESM syntax compatible with `\"type\": \"module\"` in package.json and `\"module\": \"ESNext\"` in tsconfig.",
            "status": "done",
            "testStrategy": "Run `npm run typecheck` to verify zero TypeScript errors. Import `ProjectConfig` and `FileDescriptor` in a test file to verify type shapes.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:49:36.677Z"
          },
          {
            "id": 2,
            "title": "Create src/defaults.ts with defaultConfig factory function",
            "description": "Create the `src/defaults.ts` file that exports a `defaultConfig(projectRoot: string): ProjectConfig` function returning a fully-populated default configuration object.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/src/defaults.ts`. Import `ProjectConfig` from `'./types.js'` (must use `.js` extension for ESM resolution with `moduleResolution: bundler`). Import `path` from `'node:path'`. Implement and export `defaultConfig(projectRoot: string): ProjectConfig` returning an object with: `selectedMcps: ['taskmaster']`, `taskTracker: 'taskmaster'`, `architecture: 'skip'`, `hasPrd: false`, `generateDocs: true`, `generateRules: true`, `generateSkills: true`, `generateHooks: true`, `generateCommands: true`, `agentTeamsEnabled: false`, `runAudit: true`, `hasApiDocs: false`, `hasDatabase: false`, `projectName: path.basename(projectRoot)`, `projectRoot`, `generatedFiles: []`. Note: `prdPath` is intentionally omitted (optional field, undefined by default). Ensure all required `ProjectConfig` fields are covered so TypeScript does not complain about missing properties.",
            "status": "done",
            "testStrategy": "Call `defaultConfig('/some/path/myproject')` and assert `projectName === 'myproject'`, `taskTracker === 'taskmaster'`, `generatedFiles` is an empty array, and `hasPrd === false`.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:49:48.342Z"
          },
          {
            "id": 3,
            "title": "Verify tsconfig.json covers src/types.ts and src/defaults.ts",
            "description": "Confirm the existing `tsconfig.json` `include` pattern covers the new files and that module resolution settings are compatible with `.js` extension imports in ESM.",
            "dependencies": [
              1,
              2
            ],
            "details": "Read `/workspaces/ai-dev-setup/tsconfig.json`. The current config includes `\"include\": [\"src\"]` and uses `\"moduleResolution\": \"bundler\"` with `\"module\": \"ESNext\"`. Confirm that `src/types.ts` and `src/defaults.ts` are within scope. Verify that using `import { ProjectConfig } from './types.js'` inside `src/defaults.ts` is valid under `moduleResolution: bundler` (it is — bundler mode allows omitting extensions but also accepts explicit `.js`). If any tsconfig issues are found (e.g., `rootDir` mismatch), correct them. Run `npm run typecheck` to confirm zero errors after both files are created.",
            "status": "done",
            "testStrategy": "Run `npx tsc --noEmit` from the project root. Expect exit code 0 with no diagnostic output.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:49:49.047Z"
          },
          {
            "id": 4,
            "title": "Write unit test for types and defaultConfig in test/types.test.ts",
            "description": "Create `test/types.test.ts` using Vitest that imports `ProjectConfig`, `FileDescriptor` from `src/types.ts` and `defaultConfig` from `src/defaults.ts`, then asserts correct shape and values.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create `/workspaces/ai-dev-setup/test/types.test.ts`. Use Vitest (`import { describe, it, expect } from 'vitest'`). Import `defaultConfig` from `'../src/defaults.js'`. Import types `ProjectConfig` and `FileDescriptor` from `'../src/types.js'` (TypeScript imports only — used for type assertions). Write a `describe('defaultConfig', ...)` block with tests: (1) `defaultConfig('/tmp/myproject').projectName` equals `'myproject'`; (2) `defaultConfig('/tmp/myproject').taskTracker` equals `'taskmaster'`; (3) `defaultConfig('/tmp/myproject').generatedFiles` is an empty array; (4) `defaultConfig('/tmp/myproject').hasPrd` is `false`; (5) a `FileDescriptor` shape test that constructs `{ path: 'foo.md', content: 'bar' }` and checks it satisfies the interface (via TypeScript compile check, not runtime assertion). Ensure the test file is not inside `src/` since `tsconfig.json` only includes `src` — Vitest picks up test files from the project root via its own config.",
            "status": "done",
            "testStrategy": "Run `npm test` and verify all assertions in `test/types.test.ts` pass with exit code 0.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:50:10.706Z"
          },
          {
            "id": 5,
            "title": "Run full quality gate: format, lint, typecheck, build, and test",
            "description": "Execute the mandatory pre-completion checklist — format, lint, typecheck, build, and test — fixing any failures before marking the task done.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Run the following commands in order from `/workspaces/ai-dev-setup`: (1) `npm run format` — Prettier formats `src/types.ts`, `src/defaults.ts`, and `test/types.test.ts`. (2) `npm run lint` — ESLint checks `src/**/*.ts` and `test/**/*.ts`; fix any lint errors in the new files. (3) `npm run typecheck` — `tsc --noEmit` must exit with 0 errors. (4) `npm run build` — `tsc` compiles to `dist/`; verify `dist/types.d.ts` and `dist/defaults.d.ts` are emitted. (5) `npm test` — Vitest runs all tests including `test/types.test.ts`; all must pass. If any step fails, fix the issue and re-run from that step. Common issues to watch for: missing `.js` extension on relative imports in ESM, unused type imports triggering lint warnings, or `strict: true` requiring explicit return types.",
            "status": "done",
            "testStrategy": "All five commands (`format`, `lint`, `typecheck`, `build`, `test`) must exit with code 0. Confirm `dist/types.d.ts` exists and exports `ProjectConfig`. Confirm `dist/defaults.js` exports `defaultConfig`.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:50:46.363Z"
          }
        ],
        "updatedAt": "2026-02-17T19:50:46.363Z"
      },
      {
        "id": "3",
        "title": "Implement MCP Registry and Server Definitions",
        "description": "Create `src/registry.ts` with the full MCP server registry including the new beads-mcp entry. This is the single source of truth for all MCP server configuration — drives both `.mcp.json` and `.vscode/mcp.json` generation.",
        "details": "Create `src/registry.ts`:\n\n```typescript\nimport { McpServer } from './types.js';\n\nexport const MCP_REGISTRY: McpServer[] = [\n  {\n    name: 'taskmaster',\n    description: 'Task Master AI — task orchestration, dependency tracking, multi-agent coordination',\n    npmPackage: 'task-master-ai',\n    claudeMcpName: 'taskmaster-ai',\n    required: false,\n    args: ['-y', 'task-master-ai'],\n    env: {\n      TASK_MASTER_TOOLS: 'all',\n      ANTHROPIC_API_KEY: '${ANTHROPIC_API_KEY}',\n      PERPLEXITY_API_KEY: '${PERPLEXITY_API_KEY}',\n    },\n  },\n  {\n    name: 'beads',\n    description: 'Beads — distributed git-backed issue tracking for multi-agent workflows',\n    npmPackage: 'beads-mcp',\n    claudeMcpName: 'beads',\n    required: false,\n    args: ['-y', 'beads-mcp'],\n    env: {},\n  },\n  {\n    name: 'context7',\n    description: 'Context7 — up-to-date library docs and code examples via MCP',\n    npmPackage: '@upstash/context7-mcp',\n    claudeMcpName: 'context7',\n    required: false,\n    args: ['-y', '@upstash/context7-mcp'],\n    env: {},\n  },\n  {\n    name: 'browsermcp',\n    description: 'BrowserMCP — browser automation for testing (navigate, click, screenshots)',\n    npmPackage: '@anthropic-ai/mcp-server-puppeteer',\n    claudeMcpName: 'browsermcp',\n    required: false,\n    args: ['-y', '@anthropic-ai/mcp-server-puppeteer'],\n    env: {},\n  },\n  {\n    name: 'sequential-thinking',\n    description: 'Sequential Thinking — dynamic problem-solving through thought sequences',\n    npmPackage: '@anthropic-ai/mcp-server-sequential-thinking',\n    claudeMcpName: 'sequential-thinking',\n    required: false,\n    args: ['-y', '@anthropic-ai/mcp-server-sequential-thinking'],\n    env: {},\n  },\n];\n\nexport function getMcpByName(name: string): McpServer | undefined {\n  return MCP_REGISTRY.find(s => s.name === name);\n}\n\nexport function getSelectedServers(selectedNames: string[]): McpServer[] {\n  return MCP_REGISTRY.filter(s => selectedNames.includes(s.name));\n}\n```",
        "testStrategy": "Unit test `test/generators/registry.test.ts`: assert registry has 5 entries, beads entry exists with correct npmPackage 'beads-mcp', taskmaster entry has TASK_MASTER_TOOLS env var, `getMcpByName('context7')` returns correct server, `getSelectedServers(['taskmaster', 'beads'])` returns exactly 2 servers.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/registry.ts with MCP_REGISTRY array",
            "description": "Create the main registry file with all 5 MCP server definitions as a typed McpServer[] array, importing from './types.js'.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/src/registry.ts`. Import `McpServer` from `'./types.js'`. Export a const `MCP_REGISTRY: McpServer[]` containing exactly 5 entries: taskmaster (npmPackage: 'task-master-ai', claudeMcpName: 'taskmaster-ai', env with TASK_MASTER_TOOLS, ANTHROPIC_API_KEY, PERPLEXITY_API_KEY using `${...}` template literals), beads (npmPackage: 'beads-mcp', claudeMcpName: 'beads', empty env), context7 (npmPackage: '@upstash/context7-mcp', claudeMcpName: 'context7', empty env), browsermcp (npmPackage: '@anthropic-ai/mcp-server-puppeteer', claudeMcpName: 'browsermcp', empty env), and sequential-thinking (npmPackage: '@anthropic-ai/mcp-server-sequential-thinking', claudeMcpName: 'sequential-thinking', empty env). All entries must have `required: false` and appropriate `args: ['-y', '<npmPackage>']`.",
            "status": "done",
            "testStrategy": null,
            "updatedAt": "2026-02-17T19:53:49.399Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement getMcpByName and getSelectedServers helper functions",
            "description": "Add the two exported lookup/filter functions to src/registry.ts that downstream generators and the wizard will use to query the registry.",
            "dependencies": [
              1
            ],
            "details": "In the same `/workspaces/ai-dev-setup/src/registry.ts` file created in subtask 1, add: (1) `export function getMcpByName(name: string): McpServer | undefined` — returns `MCP_REGISTRY.find(s => s.name === name)`. (2) `export function getSelectedServers(selectedNames: string[]): McpServer[]` — returns `MCP_REGISTRY.filter(s => selectedNames.includes(s.name))`. Both functions must handle edge cases gracefully (unknown name returns undefined, empty array returns []).",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:53:50.108Z"
          },
          {
            "id": 3,
            "title": "Write unit tests for MCP_REGISTRY contents",
            "description": "Create test/registry.test.ts to verify the shape and contents of MCP_REGISTRY match expected values.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create `/workspaces/ai-dev-setup/test/registry.test.ts` using vitest (`describe`/`it`/`expect`). Import `{ MCP_REGISTRY, getMcpByName, getSelectedServers }` from `'../src/registry.js'`. Write tests asserting: (1) `MCP_REGISTRY.length === 5`, (2) beads entry exists with `npmPackage === 'beads-mcp'` and `claudeMcpName === 'beads'`, (3) taskmaster entry has `env.TASK_MASTER_TOOLS === 'all'`, (4) `getMcpByName('context7')` returns the context7 server object, (5) `getMcpByName('nonexistent')` returns `undefined`, (6) `getSelectedServers(['taskmaster', 'beads'])` returns exactly 2 entries, (7) `getSelectedServers([])` returns an empty array. Follow the pattern from existing `test/types.test.ts`.",
            "status": "done",
            "testStrategy": "Run `npm test` and verify all registry test cases pass with zero failures.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:54:16.724Z"
          },
          {
            "id": 4,
            "title": "Run format, lint, typecheck, and build quality gates",
            "description": "Execute the full pre-completion checklist: prettier format, eslint, tsc typecheck, and tsc build — fixing any issues found.",
            "dependencies": [
              3
            ],
            "details": "Run these commands sequentially from `/workspaces/ai-dev-setup`: (1) `npm run format` — auto-fixes formatting in src/registry.ts and test/registry.test.ts. (2) `npm run lint` — fix any eslint errors reported in the new files. (3) `npm run typecheck` — ensure `tsc --noEmit` passes with zero errors; the McpServer import path must use `.js` extension per ESM rules. (4) `npm run build` — ensure `tsc` compiles successfully and `dist/registry.js` is emitted. Fix any type errors: check that all McpServer fields (name, description, npmPackage, claudeMcpName, required, args, env) satisfy the interface defined in `src/types.ts`.",
            "status": "done",
            "testStrategy": "All four commands must exit with code 0 before proceeding.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:54:53.270Z"
          },
          {
            "id": 5,
            "title": "Run full test suite and verify registry tests pass",
            "description": "Execute npm test to confirm the new registry tests pass alongside existing types tests, with zero failures.",
            "dependencies": [
              4
            ],
            "details": "Run `npm test` from `/workspaces/ai-dev-setup`. Verify: (1) All tests in `test/types.test.ts` still pass (no regressions), (2) All 7+ tests in `test/registry.test.ts` pass, (3) Overall test suite exits with code 0. If any test fails, diagnose the failure — common causes include wrong import paths (must use `.js` extension for ESM), incorrect env value strings (the `${ANTHROPIC_API_KEY}` must be a literal string, not an interpolated value), or a mismatch between expected registry length and actual entries. Fix and re-run until clean.",
            "status": "done",
            "testStrategy": "npm test must report 0 failing tests across all test files.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:54:53.872Z"
          }
        ],
        "updatedAt": "2026-02-17T19:54:53.872Z"
      },
      {
        "id": "4",
        "title": "Implement File I/O Utilities",
        "description": "Create `src/utils.ts` with shared helpers for file writing, shell execution, and path manipulation. The key constraint is that generators never touch the filesystem — all I/O goes through `writeFiles()`.",
        "details": "Create `src/utils.ts`:\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { execFile } from 'node:child_process';\nimport { promisify } from 'node:util';\nimport { FileDescriptor } from './types.js';\n\nconst execFileAsync = promisify(execFile);\n\n/**\n * Write all file descriptors to disk. Creates parent directories as needed.\n * Skips writing if the file already exists and overwrite is false.\n * Returns the list of paths actually written.\n */\nexport async function writeFiles(\n  files: FileDescriptor[],\n  root: string,\n  overwrite = true\n): Promise<string[]> {\n  const written: string[] = [];\n  for (const file of files) {\n    const fullPath = path.resolve(root, file.path);\n    await fs.mkdir(path.dirname(fullPath), { recursive: true });\n    if (!overwrite) {\n      try {\n        await fs.access(fullPath);\n        continue; // skip existing file\n      } catch {\n        // file doesn't exist, proceed\n      }\n    }\n    await fs.writeFile(fullPath, file.content, 'utf8');\n    if (file.executable) {\n      await fs.chmod(fullPath, 0o755);\n    }\n    written.push(file.path);\n  }\n  return written;\n}\n\n/**\n * Run a shell command and return stdout. Throws on non-zero exit.\n */\nexport async function run(\n  cmd: string,\n  args: string[],\n  cwd?: string\n): Promise<string> {\n  const { stdout } = await execFileAsync(cmd, args, { cwd });\n  return stdout.trim();\n}\n\n/**\n * Check if a command is available on PATH.\n */\nexport async function commandExists(cmd: string): Promise<boolean> {\n  try {\n    await execFileAsync('which', [cmd]);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Replace {{PLACEHOLDER}} markers in a template string.\n */\nexport function fillTemplate(\n  template: string,\n  vars: Record<string, string>\n): string {\n  return template.replace(/\\{\\{(\\w+)\\}\\}/g, (_, key) => vars[key] ?? `{{${key}}}`);\n}\n\n/**\n * Read a file relative to project root, return null if missing.\n */\nexport async function readOptional(\n  filePath: string\n): Promise<string | null> {\n  try {\n    return await fs.readFile(filePath, 'utf8');\n  } catch {\n    return null;\n  }\n}\n```",
        "testStrategy": "Unit test `test/generators/utils.test.ts` using a temp directory (use `os.tmpdir()` + random suffix, clean up in afterEach): test `writeFiles` creates nested directories and files, test `fillTemplate` replaces all placeholders, test `writeFiles` with `overwrite=false` skips existing files, test `commandExists('node')` returns true.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-17T20:25:50.805Z"
      },
      {
        "id": "5",
        "title": "Create Document Template Files",
        "description": "Create all template files in `templates/` directory. These are plain markdown files with `{{PLACEHOLDER}}` markers. They are the content foundation for F2 (Document Scaffolding). No templating engine — just string replacement via `fillTemplate()`.",
        "details": "Create the following template files:\n\n**`templates/docs/doc_format.md`** — the meta-standard all docs follow:\n- TLDR section (1-3 sentences)\n- TOC with `#section` anchors\n- Sections < 30 lines\n- Tables over prose for structured data\n- Cross-references as relative links\n- Max ~500 lines; split into sub-docs if larger\n\n**`templates/docs/prd.md`** — PRD template with:\n- Problem / Solution / Features / Phases sections\n- Feature template includes: Business outcome, Demo test (single command proving feature works), Acceptance criteria\n- `{{PROJECT_NAME}}` placeholder\n\n**`templates/docs/architecture.md`** — Architecture overview:\n- TLDR, tier overview (adapts to chosen architecture: monolith/2-tier/3-tier/microservices)\n- Component map, links to detail docs\n- `{{ARCHITECTURE}}` and `{{PROJECT_NAME}}` placeholders\n\n**`templates/docs/api.md`** — API surface:\n- Table format: Endpoint | Method | Description | ADR ref | Source file\n- Auth section\n- Error shapes section\n- `{{PROJECT_NAME}}` placeholder\n\n**`templates/docs/cuj.md`** — Critical user journeys:\n- Step-by-step flows agents should understand\n- One section per major user journey\n\n**`templates/docs/testing_strategy.md`** — Testing philosophy:\n- Integration-first principle\n- Demo-test definition\n- Mock justification rules\n- Quality gate steps\n\n**`templates/docs/onboarding.md`** — Quick-start guide:\n- Project context (1 para)\n- Key commands table\n- Where to find things (map of important files/dirs)\n- First steps for a new developer or agent\n\n**`templates/docs/adr_template.md`** — ADR format:\n```markdown\n# ADR-{{NUMBER}}: {{TITLE}}\n- **Status:** Proposed\n- **Context:** Why this decision was needed\n- **Decision:** What was decided\n- **Consequences:** Trade-offs accepted\n```\n\n**`templates/docs/tasks_simple.md`** — Simple task tracker:\n```markdown\n# Task Tracker\n## Summary\n| # | Task | Status | Depends |\n|---|------|--------|---------|\n| 1 | Example task | [ ] | — |\n\n## Tasks\n### Task 1: Example task\n- **Status:** Pending\n- **Depends on:** —\n- **Success:** App runs without errors\n- **Demo command:** `npm start`\n```\n\nAll templates should follow doc_format.md standard themselves.",
        "testStrategy": "Test that all template files exist via `fs.access()`. Test that `fillTemplate` correctly substitutes `{{PROJECT_NAME}}` and `{{ARCHITECTURE}}` in the architecture template. Test that prd.md template contains the 'Demo test' field placeholder. Test that no template exceeds 500 lines.",
        "priority": "high",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create doc_format.md meta-standard template",
            "description": "Create `templates/docs/doc_format.md` defining the documentation standard that all other templates must follow.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/doc_format.md` with: TLDR section (1-3 sentences), TOC using `#section` anchors, rules for sections < 30 lines, preference for tables over prose, cross-references as relative links, and a max ~500 lines guideline with sub-doc splitting instructions. This file itself must follow its own standard. Remove the existing `.gitkeep` from `templates/docs/` if present after creating the first file.",
            "status": "done",
            "testStrategy": "Check that the file exists via `fs.access()`. Verify it contains 'TLDR', 'TOC', '500 lines', and 'tables' sections. Verify line count is under 500.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:04.441Z"
          },
          {
            "id": 2,
            "title": "Create prd.md PRD template",
            "description": "Create `templates/docs/prd.md` containing the PRD template with Problem/Solution/Features/Phases sections and a `{{PROJECT_NAME}}` placeholder.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/prd.md`. Must include: a TLDR at the top, a TOC, sections for Problem, Solution, Features, and Phases. Each feature entry should have sub-fields: Business outcome, Demo test (single-command proof), and Acceptance criteria. Insert `{{PROJECT_NAME}}` placeholder in the header. Follow doc_format.md standards (sections < 30 lines, tables where applicable).",
            "status": "done",
            "testStrategy": "Verify file exists. Verify `{{PROJECT_NAME}}` placeholder is present. Verify 'Demo test' field appears in the features section. Verify file is under 500 lines.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:12.402Z"
          },
          {
            "id": 3,
            "title": "Create architecture.md architecture overview template",
            "description": "Create `templates/docs/architecture.md` with TLDR, tier overview, component map, and `{{ARCHITECTURE}}` and `{{PROJECT_NAME}}` placeholders.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/architecture.md`. Must include: TLDR section, a tier overview that acknowledges monolith/2-tier/3-tier/microservices options (using `{{ARCHITECTURE}}` placeholder), a component map section, and links to detail docs. Insert both `{{PROJECT_NAME}}` and `{{ARCHITECTURE}}` placeholders. The `fillTemplate()` function in `src/utils.ts` will substitute these at scaffold time. Follow doc_format.md standards.",
            "status": "done",
            "testStrategy": "Verify file exists. Verify both `{{ARCHITECTURE}}` and `{{PROJECT_NAME}}` placeholders are present. Verify `fillTemplate()` correctly substitutes them. Verify file is under 500 lines.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:18.797Z"
          },
          {
            "id": 4,
            "title": "Create api.md and cuj.md template files",
            "description": "Create `templates/docs/api.md` (API surface in table format with auth and error sections) and `templates/docs/cuj.md` (critical user journeys with step-by-step flows).",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/api.md` with: TLDR, a table with columns Endpoint | Method | Description | ADR ref | Source file, an Auth section, an Error shapes section, and `{{PROJECT_NAME}}` placeholder. Create `/workspaces/ai-dev-setup/templates/docs/cuj.md` with: TLDR, instructions on step-by-step user flow documentation, and one example journey section showing the expected format. Both must follow doc_format.md standards.",
            "status": "done",
            "testStrategy": "Verify both files exist. Verify api.md contains the 5-column table header and `{{PROJECT_NAME}}`. Verify cuj.md contains a sample journey section with steps.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:25.134Z"
          },
          {
            "id": 5,
            "title": "Create testing_strategy.md and onboarding.md template files",
            "description": "Create `templates/docs/testing_strategy.md` (testing philosophy with integration-first, demo-test definition, mock rules, quality gate) and `templates/docs/onboarding.md` (quick-start guide for new devs/agents).",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/testing_strategy.md` with: TLDR, Integration-first principle section, Demo-test definition, Mock justification rules (when mocks are acceptable), and Quality gate steps listing all mandatory pre-completion checks. Create `/workspaces/ai-dev-setup/templates/docs/onboarding.md` with: TLDR, Project context paragraph, Key commands table (Command | Description), a 'Where to find things' section mapping important files/dirs, and a First steps checklist. Both follow doc_format.md standards.",
            "status": "done",
            "testStrategy": "Verify both files exist. Verify testing_strategy.md contains 'Integration', 'Demo', 'Mock', and 'Quality gate' sections. Verify onboarding.md contains a commands table and a file map section.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:31.274Z"
          },
          {
            "id": 6,
            "title": "Create adr_template.md ADR format template",
            "description": "Create `templates/docs/adr_template.md` with the standard ADR structure including `{{NUMBER}}` and `{{TITLE}}` placeholders.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/adr_template.md` with exactly the structure: `# ADR-{{NUMBER}}: {{TITLE}}`, followed by fields: **Status:** Proposed, **Context:** (why decision was needed), **Decision:** (what was decided), **Consequences:** (trade-offs accepted). Keep the template concise — it is a reusable scaffold for individual ADR documents. Follow doc_format.md layout conventions.",
            "status": "done",
            "testStrategy": "Verify file exists. Verify `{{NUMBER}}` and `{{TITLE}}` placeholders are present. Verify all four required fields (Status, Context, Decision, Consequences) appear.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:37.422Z"
          },
          {
            "id": 7,
            "title": "Create tasks_simple.md simple task tracker template",
            "description": "Create `templates/docs/tasks_simple.md` providing a markdown-based task tracker with a summary table and individual task sections.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/tasks_simple.md` with exactly the structure: `# Task Tracker` heading, a `## Summary` section containing a table with columns # | Task | Status | Depends, and a `## Tasks` section with a `### Task 1: Example task` entry showing fields Status, Depends on, Success criteria, and Demo command. The Status column uses `[ ]` / `[x]` checkbox notation. Follow doc_format.md standards.",
            "status": "done",
            "testStrategy": "Verify file exists. Verify summary table with correct columns is present. Verify task entry contains 'Demo command' field. Verify `[ ]` checkbox notation is used.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:43.428Z"
          },
          {
            "id": 8,
            "title": "Write tests verifying all template files exist and meet constraints",
            "description": "Add a Vitest test file `test/templates.test.ts` that verifies all 9 template files exist, contain required placeholders, and none exceed 500 lines.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Create `/workspaces/ai-dev-setup/test/templates.test.ts`. Tests must: (1) use `fs.access()` to verify each of the 9 template files in `templates/docs/` exists; (2) read each file and assert line count <= 500; (3) assert prd.md contains 'Demo test'; (4) assert architecture.md contains `{{ARCHITECTURE}}` and `{{PROJECT_NAME}}`; (5) import `fillTemplate` from `src/utils.ts` and verify it substitutes `{{PROJECT_NAME}}` and `{{ARCHITECTURE}}` correctly using architecture.md content; (6) assert adr_template.md contains `{{NUMBER}}` and `{{TITLE}}`. Run `npm run format && npm run lint && npm run typecheck && npm run build && npm test` to pass quality gate before marking done.",
            "status": "done",
            "testStrategy": "Run `npm test` — all assertions in `test/templates.test.ts` must pass with zero failures. Run `npm run typecheck` to confirm no TypeScript errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:32:45.636Z"
          }
        ],
        "updatedAt": "2026-02-17T20:32:45.636Z"
      },
      {
        "id": "6",
        "title": "Create Rules, Skills, Hooks, and Commands Template Files",
        "description": "Create all template files for `.claude/rules/`, `.claude/skills/`, `.claude/hooks/`, and `.claude/commands/` directories. These implement F3 (Rules, Skills & Hooks) and F8 (Custom Claude Commands) template content.",
        "details": "**Rules templates (`templates/rules/`):**\n\n`general.md` — global scope: language version, package manager, coding style placeholders\n\n`docs.md` — scope: `docs/**`: how to read/update docs, follow doc_format.md standard\n\n`testing.md` — scope: `**/*.test.*`, `**/*.spec.*`:\n```markdown\n---\npaths:\n  - \"**/*.test.*\"\n  - \"**/*.spec.*\"\n---\n# Testing Rules\n## Default: Integration tests\n- Write tests that exercise real code paths. Use actual database connections, real HTTP requests, real file I/O.\n- Only mock external 3rd-party services. Add a comment: `// Mock: <service> — no local instance available`\n- If mocking more than 2 dependencies, reconsider: the test may be testing the wrong layer.\n## Demo checkpoints\n- Each feature task must produce at least one integration test demonstrating the feature end-to-end.\n- Name: `it('demo: user can sign up and access protected route')`\n## Smoke tests\n- Setup tasks get minimal smoke tests: app starts, health check passes, key dependencies connect.\n- Mark: `it('smoke: ...')`\n## Quality gate\n1. Format → 2. Lint → 3. Type-check → 4. Build → 5. ALL tests pass\n- Never delete a test to make the suite pass.\n```\n\n`git.md` — global scope: branch naming `feat/<task-id>-<desc>`, commit format `<task-id>: <what> — <value>`, one feature branch at a time\n\n`security.md` — scope: `src/auth/**`, `src/middleware/**`, `**/*secret*`: input validation, no credential logging, OWASP basics\n\n`api.md` — scope: `src/api/**`, `src/routes/**`: RESTful conventions, standard error shapes, input validation, references `@docs/api.md`\n\n`database.md` — scope: `src/db/**`, `src/models/**`, `**/migrations/**`: parameterized queries, migration discipline, no raw SQL\n\n`config.md` — scope: `**/*.config.*`, `**/.env*`: never hardcode secrets, use env vars, document in .env.example\n\n`agent-teams.md` — global scope: when to use teams, when not to, coordination rules\n\n**Skills templates (`templates/skills/`):**\n\n`testing.md` — activates on: test, coverage, demo — integration-first philosophy, demo-test patterns\n\n`commit.md` — activates on: commit, push, branch — full commit workflow: quality gate → format → lint → type-check → build → test → commit\n\n`task-workflow.md` — activates on: next task, pick up, start working — how to pick, implement, verify, and close a task\n\n**Hooks (`templates/hooks/`):**\n\n`pre-commit.sh`:\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\necho \"Running quality gate before commit...\"\nnpm run format --if-present 2>/dev/null || true\nnpm run lint --if-present || { echo \"BLOCK: Lint errors found.\"; exit 1; }\nnpm run typecheck --if-present || { echo \"BLOCK: Type errors found.\"; exit 1; }\nnpm run build --if-present || { echo \"BLOCK: Build failed.\"; exit 1; }\nnpm test --if-present || { echo \"BLOCK: Tests failing.\"; exit 1; }\necho \"Quality gate passed.\"\n```\n\n**Commands (`templates/commands/`):**\n\n`dev-next.md` — `/dev-next` command:\n1. Read docs/prd.md, docs/architecture.md, docs/adr/ for context\n2. Check dependency chain and last git commit\n3. Get next task from tracker\n4. Implement following .claude/rules/\n5. Commit: `<task-id>: <change> — <value>`\n6. Report and ask to continue\n\n`review.md` — `/review` command:\n1. Run `git diff`\n2. Check each changed file against applicable .claude/rules/\n3. Verify integration tests exist\n4. Run full quality gate\n5. Report: ready/needs-fixing\n\n**Boot prompt (`templates/boot-prompt.txt`):** Session startup instructions referencing project docs and chosen task tracker (uses `{{TASK_TRACKER}}` placeholder).",
        "testStrategy": "Test that all template files exist. Test `testing.md` rule contains 'Integration tests', 'Demo checkpoints', and 'Quality gate' sections. Test `pre-commit.sh` contains `--if-present` flag and correct exit codes. Test `dev-next.md` references `docs/prd.md` and `docs/adr/`. Test `git.md` contains the branch naming pattern `feat/<task-id>`.",
        "priority": "high",
        "dependencies": [
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create rules template files: general.md, docs.md, and git.md",
            "description": "Create three foundational rules template files in templates/rules/ covering global coding standards, documentation conventions, and git workflow rules.",
            "dependencies": [],
            "details": "Create templates/rules/general.md with frontmatter (global scope, no paths filter), language version placeholder ({{LANGUAGE_VERSION}}), package manager placeholder ({{PACKAGE_MANAGER}}), and coding style placeholders. Create templates/rules/docs.md with paths frontmatter scoped to docs/**, rules for reading/updating docs referencing doc_format.md standard. Create templates/rules/git.md with global scope, branch naming pattern feat/<task-id>-<desc>, commit format <task-id>: <what> — <value>, and one-feature-branch-at-a-time rule. All files follow the doc_format.md standard (TLDR, short sections). Use --- YAML frontmatter for path scoping where applicable.",
            "status": "pending",
            "testStrategy": "Verify all three files exist under templates/rules/. Check general.md contains {{LANGUAGE_VERSION}} and {{PACKAGE_MANAGER}} placeholders. Check docs.md has a paths frontmatter with docs/**. Check git.md contains 'feat/<task-id>' branch naming pattern.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create rules template files: testing.md, security.md, api.md, database.md, config.md",
            "description": "Create the remaining five rules template files covering testing philosophy, security requirements, API conventions, database safety, and configuration management.",
            "dependencies": [
              1
            ],
            "details": "Create templates/rules/testing.md with YAML frontmatter paths: ['**/*.test.*', '**/*.spec.*'], sections: Default Integration tests (real code paths, mock only external 3rd-party with comment), Demo checkpoints (feature task → at least one integration test, naming 'demo: ...'), Smoke tests ('smoke: ...' prefix), and Quality gate sequence (Format → Lint → Type-check → Build → ALL tests pass, never delete a test). Create templates/rules/security.md scoped to src/auth/**, src/middleware/**, **/*secret*: input validation, no credential logging, OWASP basics. Create templates/rules/api.md scoped to src/api/**, src/routes/**: RESTful conventions, standard error shapes, input validation, reference @docs/api.md. Create templates/rules/database.md scoped to src/db/**, src/models/**, **/migrations/**: parameterized queries, migration discipline, no raw SQL. Create templates/rules/config.md scoped to **/*.config.*, **/.env*: never hardcode secrets, use env vars, document in .env.example.",
            "status": "pending",
            "testStrategy": "Verify all five files exist. Check testing.md contains 'Integration tests', 'Demo checkpoints', and 'Quality gate' sections. Check security.md has path scope for src/auth/**. Check api.md references @docs/api.md. Check database.md contains 'parameterized queries'. Check config.md contains '.env.example'.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create rules template file: agent-teams.md",
            "description": "Create the agent-teams.md rules template file defining when and how to use multi-agent team coordination in Claude Code.",
            "dependencies": [
              1
            ],
            "details": "Create templates/rules/agent-teams.md with global scope (no path filter). Content should include: when to use agent teams (large parallel tasks, independent sub-problems), when NOT to use teams (sequential dependencies, small tasks, shared mutable state), coordination rules (orchestrator pattern, task hand-off format, status reporting conventions), and reference to the three-tier system (task-orchestrator, task-executor, task-checker). Follow doc_format.md: TLDR at top, TOC, sections under 30 lines, tables preferred for structured data.",
            "status": "pending",
            "testStrategy": "Verify templates/rules/agent-teams.md exists. Check it contains sections for 'when to use' and 'when not to use' teams. Check it references the orchestrator/executor/checker pattern.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create skills template files: testing.md, commit.md, and task-workflow.md",
            "description": "Create three skills template files in templates/skills/ that define reusable Claude Code skill activations for testing, committing, and task workflow.",
            "dependencies": [],
            "details": "Create templates/skills/testing.md: frontmatter with activateOn: [test, coverage, demo], content covering integration-first philosophy, how to write demo tests (naming: 'demo: user can ...'), how to write smoke tests, coverage expectations. Create templates/skills/commit.md: frontmatter with activateOn: [commit, push, branch], full commit workflow steps: 1) run quality gate (format → lint → typecheck → build → test), 2) stage changes, 3) write commit message in format '<task-id>: <what> — <value>', 4) push to feature branch. Create templates/skills/task-workflow.md: frontmatter with activateOn: [next task, pick up, start working], steps: pick next available task from tracker, read task details, explore affected code, implement following .claude/rules/, self-verify against test strategy, mark done.",
            "status": "pending",
            "testStrategy": "Verify all three files exist under templates/skills/. Check testing.md has activateOn including 'demo'. Check commit.md includes the quality gate steps sequence. Check task-workflow.md references .claude/rules/ and task tracker.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create hooks template file: pre-commit.sh and boot-prompt.txt",
            "description": "Create the pre-commit.sh hook template and boot-prompt.txt session startup file in their respective template directories.",
            "dependencies": [],
            "details": "Create templates/hooks/pre-commit.sh as an executable bash script with shebang #!/usr/bin/env bash and set -euo pipefail. Script content: echo 'Running quality gate before commit...', then run each step with --if-present: npm run format (|| true, non-blocking), npm run lint (exit 1 on failure with BLOCK message), npm run typecheck (exit 1 on failure), npm run build (exit 1 on failure), npm test (exit 1 on failure). End with echo 'Quality gate passed.'. The FileDescriptor for this file must set executable: true so writeFiles() calls chmod 755. Create templates/boot-prompt.txt (or .md) with session startup instructions: read docs/prd.md, docs/architecture.md, docs/adr/ for context, identify current task from {{TASK_TRACKER}}, apply .claude/rules/, check last git commit, report ready status.",
            "status": "pending",
            "testStrategy": "Verify templates/hooks/pre-commit.sh exists and is executable (chmod 755). Check it contains --if-present flag for all npm run commands. Check it uses correct exit 1 with BLOCK messages. Verify boot-prompt.txt exists and contains {{TASK_TRACKER}} placeholder.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create commands template files: dev-next.md and review.md",
            "description": "Create two custom Claude Code slash command template files in templates/commands/ for the /dev-next and /review workflow commands.",
            "dependencies": [],
            "details": "Create templates/commands/dev-next.md defining the /dev-next command: step 1 read docs/prd.md, docs/architecture.md, and docs/adr/ for context; step 2 check dependency chain and last git commit; step 3 get next task from tracker (references {{TASK_TRACKER}}); step 4 implement following .claude/rules/; step 5 commit using format '<task-id>: <change> — <value>'; step 6 report summary and ask user to continue. Create templates/commands/review.md defining the /review command: step 1 run git diff; step 2 check each changed file against applicable .claude/rules/; step 3 verify integration tests exist for changed functionality; step 4 run full quality gate (format → lint → typecheck → build → test); step 5 report result as either 'ready to merge' or 'needs-fixing' with specific issues listed.",
            "status": "pending",
            "testStrategy": "Verify both files exist under templates/commands/. Check dev-next.md references docs/prd.md and docs/adr/. Check dev-next.md contains the commit format pattern '<task-id>'. Check review.md references .claude/rules/ and includes quality gate steps. Check review.md produces a ready/needs-fixing report format.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-02-17T20:50:49.340Z"
      },
      {
        "id": "7",
        "title": "Implement MCP Configuration Generator",
        "description": "Create `src/generators/mcp-json.ts` — a pure function that takes `ProjectConfig` and returns `FileDescriptor[]` for both `.mcp.json` (Claude Code) and `.vscode/mcp.json` (VS Code/Copilot). These two files use different JSON schemas.",
        "details": "Create `src/generators/mcp-json.ts`:\n\n```typescript\nimport { ProjectConfig, FileDescriptor } from '../types.js';\nimport { getSelectedServers } from '../registry.js';\n\ninterface McpServerConfig {\n  command: string;\n  args: string[];\n  env?: Record<string, string>;\n}\n\nfunction buildClaudeCodeMcpConfig(\n  config: ProjectConfig\n): Record<string, McpServerConfig> {\n  const servers = getSelectedServers(config.selectedMcps);\n  const result: Record<string, McpServerConfig> = {};\n  for (const server of servers) {\n    result[server.claudeMcpName] = {\n      command: 'npx',\n      args: server.args ?? ['-y', server.npmPackage],\n      ...(server.env && Object.keys(server.env).length > 0\n        ? { env: server.env }\n        : {}),\n    };\n  }\n  return result;\n}\n\nfunction buildVscodeMcpConfig(\n  config: ProjectConfig\n): Record<string, unknown> {\n  const servers = getSelectedServers(config.selectedMcps);\n  const result: Record<string, unknown> = {};\n  for (const server of servers) {\n    const envBlock: Record<string, string> = {};\n    for (const [key] of Object.entries(server.env ?? {})) {\n      envBlock[key] = `\\${env:${key}}`;\n    }\n    result[server.claudeMcpName] = {\n      command: 'npx',\n      args: server.args ?? ['-y', server.npmPackage],\n      cwd: '${workspaceFolder}',\n      ...(Object.keys(envBlock).length > 0 ? { env: envBlock } : {}),\n      envFile: '${workspaceFolder}/.env',\n      type: 'stdio',\n    };\n  }\n  return result;\n}\n\nexport function generateMcpJson(config: ProjectConfig): FileDescriptor[] {\n  return [\n    {\n      path: '.mcp.json',\n      content: JSON.stringify(\n        { mcpServers: buildClaudeCodeMcpConfig(config) },\n        null,\n        2\n      ),\n    },\n    {\n      path: '.vscode/mcp.json',\n      content: JSON.stringify(\n        { servers: buildVscodeMcpConfig(config) },\n        null,\n        2\n      ),\n    },\n  ];\n}\n```\n\nKey design constraints:\n- `.mcp.json` root key: `\"mcpServers\"`\n- `.vscode/mcp.json` root key: `\"servers\"`, adds `cwd`, `envFile`, `type: \"stdio\"`, env vars use `${env:VAR}` syntax\n- Both use `npx` as command\n- Pure function — no filesystem access",
        "testStrategy": "Unit test `test/generators/mcp-json.test.ts`:\n1. With `selectedMcps: ['taskmaster']` → `.mcp.json` has `mcpServers.taskmaster-ai`, `.vscode/mcp.json` has `servers.taskmaster-ai` with `cwd: '${workspaceFolder}'`\n2. With `selectedMcps: ['taskmaster', 'beads']` → both configs have 2 entries\n3. `.mcp.json` does NOT have `cwd` field\n4. `.vscode/mcp.json` env values use `${env:ANTHROPIC_API_KEY}` format\n5. Both outputs are valid JSON (JSON.parse succeeds)\n6. Generator returns exactly 2 FileDescriptors",
        "priority": "high",
        "dependencies": [
          "3",
          "4"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-17T21:08:12.934Z"
      },
      {
        "id": "8",
        "title": "Implement CLAUDE.md Generator",
        "description": "Create `src/generators/claude-md.ts` — generates `CLAUDE.md` and `CLAUDE_MCP.md` tailored to the user's chosen task tracker and selected MCPs. The generated CLAUDE.md must reference the actual docs and rules that were generated.",
        "details": "Create `src/generators/claude-md.ts`:\n\n```typescript\nimport { ProjectConfig, FileDescriptor } from '../types.js';\nimport { getSelectedServers } from '../registry.js';\n\nfunction buildTaskTrackerInstructions(config: ProjectConfig): string {\n  switch (config.taskTracker) {\n    case 'taskmaster':\n      return `## Task Tracker: Task Master AI\n\n**Import Task Master's workflow commands:**\n@./.taskmaster/CLAUDE.md\n\n- \\`task-master next\\` — Get next task\n- \\`task-master show <id>\\` — View task details  \n- \\`task-master set-status --id=<id> --status=done\\` — Mark complete\n- \\`task-master update-subtask --id=<id> --prompt=\"...\"\\` — Log progress`;\n\n    case 'beads':\n      return `## Task Tracker: Beads\n\n**Beads tools:** beads_ready, beads_create, beads_show, beads_update, beads_close, beads_dep_add, beads_dep_tree, beads_sync\n\n- \\`bd show\\` — View current tasks\n- \\`bd next\\` — Get next task\n- Reference tasks in commits: \\`bd-<hash>: <change> — <value>\\`\n- Run \\`bd sync\\` before push`;\n\n    case 'markdown':\n      return `## Task Tracker: Simple Markdown\n\n- Edit \\`TASKS.md\\` directly\n- Mark tasks with \\`[x]\\` when done\n- Add demo command for each task before marking done`;\n  }\n}\n\nfunction buildMcpSection(config: ProjectConfig): string {\n  const servers = getSelectedServers(config.selectedMcps);\n  if (servers.length === 0) return '';\n  const lines = servers.map(s => `- **${s.claudeMcpName}**: ${s.description}`);\n  return `## MCP Servers\\n\\n${lines.join('\\n')}`;\n}\n\nexport function generateClaudeMd(config: ProjectConfig): FileDescriptor[] {\n  const docImports = config.generateDocs\n    ? `\n## Project Documentation\n@docs/doc_format.md\n@docs/prd.md\n@docs/architecture.md\n@docs/testing_strategy.md\n@docs/onboarding.md\n`.trim()\n    : '';\n\n  const rulesRef = config.generateRules\n    ? `\\n## Agent Rules\\nPath-scoped rules in \\`.claude/rules/\\` auto-load based on the file being edited.`\n    : '';\n\n  const claudeMdContent = `<!-- SETUP-AI-MANAGED — regenerated by ai-init -->\n\n# Project Instructions for Claude Code\n\n${docImports}\n\n${buildTaskTrackerInstructions(config)}\n\n${buildMcpSection(config)}\n\n${rulesRef}\n\n## Quality Gate\n\nBefore marking any task done:\n1. Format: \\`npm run format\\`\n2. Lint: \\`npm run lint\\`  \n3. Type-check: \\`npm run typecheck\\`\n4. Build: \\`npm run build\\`\n5. Test: \\`npm test\\`\n`.trim();\n\n  const files: FileDescriptor[] = [\n    { path: 'CLAUDE.md', content: claudeMdContent },\n  ];\n\n  // Generate CLAUDE_MCP.md with MCP tool docs\n  const servers = getSelectedServers(config.selectedMcps);\n  if (servers.length > 0) {\n    const mcpDocs = servers\n      .map(s => `## ${s.claudeMcpName}\\n\\n${s.description}\\n\\n**Package:** \\`${s.npmPackage}\\``)\n      .join('\\n\\n---\\n\\n');\n    files.push({\n      path: 'CLAUDE_MCP.md',\n      content: `# MCP Servers Available\\n\\n${mcpDocs}\\n`,\n    });\n  }\n\n  return files;\n}\n```\n\nThe CLAUDE.md must use `@import` syntax for doc references so Claude Code auto-loads them. Tracker-specific instructions must be accurate — agents rely on these to know which commands to use.",
        "testStrategy": "Unit test `test/generators/claude-md.test.ts`:\n1. With `taskTracker: 'taskmaster'` → output contains `@./.taskmaster/CLAUDE.md` and `task-master next`\n2. With `taskTracker: 'beads'` → output contains `beads_ready` and `bd sync`\n3. With `taskTracker: 'markdown'` → output contains `TASKS.md`\n4. With `generateDocs: true` → CLAUDE.md contains `@docs/prd.md`\n5. With `selectedMcps: ['taskmaster', 'context7']` → CLAUDE_MCP.md lists both servers\n6. With `selectedMcps: []` → only CLAUDE.md returned (no CLAUDE_MCP.md)\n7. CLAUDE.md contains the Quality Gate section",
        "priority": "high",
        "dependencies": [
          "3",
          "4",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/generators/claude-md.ts with core generator function",
            "description": "Implement the main generateClaudeMd function and helper functions in src/generators/claude-md.ts following the pattern established by mcp-json.ts.",
            "dependencies": [],
            "details": "Create /workspaces/ai-dev-setup/src/generators/claude-md.ts with:\n1. Import ProjectConfig, FileDescriptor from '../types.js' and getSelectedServers from '../registry.js'\n2. Implement buildTaskTrackerInstructions(config: ProjectConfig): string with switch on config.taskTracker for 'taskmaster' (includes @./.taskmaster/CLAUDE.md import and task-master commands), 'beads' (includes beads_ready, bd sync, commit format), and 'markdown' (TASKS.md editing with [x] markers)\n3. Implement buildMcpSection(config: ProjectConfig): string that calls getSelectedServers and formats each server's claudeMcpName and description\n4. Implement exported generateClaudeMd(config: ProjectConfig): FileDescriptor[] that:\n   - Conditionally builds docImports block (if config.generateDocs) with @docs/doc_format.md, @docs/prd.md, @docs/architecture.md, @docs/testing_strategy.md, @docs/onboarding.md\n   - Conditionally builds rulesRef block (if config.generateRules) mentioning .claude/rules/ path-scoped rules\n   - Assembles CLAUDE.md string with managed header comment, doc imports, task tracker section, MCP section, rules ref, and Quality Gate (format, lint, typecheck, build, test steps)\n   - Returns FileDescriptor array with CLAUDE.md always included\n   - Adds CLAUDE_MCP.md only when config.selectedMcps is non-empty, using getSelectedServers to build per-server docs with claudeMcpName, description, and npmPackage",
            "status": "done",
            "testStrategy": "Manual inspection of output for each tracker type before writing automated tests",
            "updatedAt": "2026-02-17T21:12:39.898Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add accurate beads tracker instructions with correct command syntax",
            "description": "Ensure the beads task tracker branch in buildTaskTrackerInstructions contains accurate beads MCP tool names and bd CLI commands as documented in the beads MCP server entry in registry.ts.",
            "dependencies": [
              1
            ],
            "details": "Read /workspaces/ai-dev-setup/src/registry.ts to find the exact beads server description and any documented commands. Read the beads entry in MCP_REGISTRY to get claudeMcpName, description, and any tool listings. Update the 'beads' case in buildTaskTrackerInstructions to include:\n- All beads MCP tool names: beads_ready, beads_create, beads_show, beads_update, beads_close, beads_dep_add, beads_dep_tree, beads_sync\n- CLI equivalents: bd show, bd next, bd sync\n- Commit message format: bd-<hash>: <change> — <value>\n- Instruction to run bd sync before push\nAlso verify the 'taskmaster' case references @./.taskmaster/CLAUDE.md with exact @ import syntax Claude Code recognizes, and the 'markdown' case references TASKS.md with [x] completion format and demo command requirement.",
            "status": "done",
            "testStrategy": "Check each tracker branch output against known command documentation",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:12:46.368Z"
          },
          {
            "id": 3,
            "title": "Write unit tests in test/generators/claude-md.test.ts",
            "description": "Create comprehensive unit tests covering all 7 required scenarios from the task specification, following the patterns in test/generators/mcp-json.test.ts.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create /workspaces/ai-dev-setup/test/generators/claude-md.test.ts:\n1. Import generateClaudeMd from '../../src/generators/claude-md.js' and defaultConfig from '../../src/defaults.js'\n2. Test 1: taskTracker 'taskmaster' → CLAUDE.md content contains '@./.taskmaster/CLAUDE.md' and 'task-master next'\n3. Test 2: taskTracker 'beads' → CLAUDE.md content contains 'beads_ready' and 'bd sync'\n4. Test 3: taskTracker 'markdown' → CLAUDE.md content contains 'TASKS.md'\n5. Test 4: generateDocs true → CLAUDE.md content contains '@docs/prd.md'\n6. Test 5: selectedMcps ['taskmaster', 'context7'] → result has 2 files, CLAUDE_MCP.md content lists both server names\n7. Test 6: selectedMcps [] → result has exactly 1 file (only CLAUDE.md, no CLAUDE_MCP.md)\n8. Test 7: CLAUDE.md always contains a Quality Gate section with 'npm run format' and 'npm run lint'\n9. Additional tests: generateRules true → content contains '.claude/rules/', generateDocs false → content does NOT contain '@docs/prd.md'\nUse defaultConfig() with spread overrides for each test case as done in mcp-json.test.ts.",
            "status": "done",
            "testStrategy": "Run npm test after creating the file to verify all tests pass",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:13:37.381Z"
          },
          {
            "id": 4,
            "title": "Run format, lint, typecheck, and build to verify implementation",
            "description": "Execute the full quality gate pipeline to ensure the new generator file and test file have no errors before marking the task done.",
            "dependencies": [
              3
            ],
            "details": "Run the following commands in sequence from /workspaces/ai-dev-setup:\n1. npm run format — auto-format src/generators/claude-md.ts and test/generators/claude-md.test.ts with Prettier\n2. npm run lint — run ESLint and fix any reported errors (not just warnings)\n3. npm run typecheck — run tsc --noEmit to catch any TypeScript type errors; common issues include missing return types, wrong import paths (.js vs .ts), or ProjectConfig field mismatches\n4. npm run build — compile the project; ensure no compilation errors in the new file\nIf any step fails, fix the issue in the relevant source file and re-run from that step. Pay particular attention to: import paths using .js extension (ESM requirement), correct ProjectConfig field names matching src/types.ts, and no implicit any types.",
            "status": "done",
            "testStrategy": "All four commands must exit with code 0",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:14:24.350Z"
          },
          {
            "id": 5,
            "title": "Run test suite and fix any failing tests",
            "description": "Execute npm test to run all unit tests including the new claude-md.test.ts, verify all 7+ new tests pass along with existing tests.",
            "dependencies": [
              4
            ],
            "details": "Run 'npm test' from /workspaces/ai-dev-setup and verify:\n1. All existing tests (mcp-json, utils, templates) continue to pass — no regressions\n2. All 7+ new tests in test/generators/claude-md.test.ts pass\n3. If any test fails, identify the mismatch: either fix the generator logic in src/generators/claude-md.ts or fix incorrect test assertions\nCommon failure modes to watch for:\n- Content assertion using wrong substring (check exact casing and spacing in output)\n- CLAUDE_MCP.md not being generated when selectedMcps is non-empty (check getSelectedServers return value)\n- @ import syntax not matching exactly what Claude Code expects\n- Quality Gate section missing or incomplete\nAfter all tests pass, use update_subtask to log that implementation is complete and all checks passed, then set task 8 status to done.",
            "status": "done",
            "testStrategy": "npm test must exit with code 0 and show all new tests as passing",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:14:24.361Z"
          }
        ],
        "updatedAt": "2026-02-17T21:14:24.361Z"
      },
      {
        "id": "9",
        "title": "Implement Document Scaffolding Generator",
        "description": "Create `src/generators/docs.ts` — reads templates from `templates/docs/` and returns `FileDescriptor[]` for all project documentation files. Applies `fillTemplate()` with project-specific values. Implements F2.",
        "details": "Create `src/generators/docs.ts`:\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { ProjectConfig, FileDescriptor } from '../types.js';\nimport { fillTemplate } from '../utils.js';\n\nconst TEMPLATES_DIR = new URL('../../templates/docs', import.meta.url).pathname;\n\nasync function readTemplate(name: string): Promise<string> {\n  return fs.readFile(path.join(TEMPLATES_DIR, name), 'utf8');\n}\n\nexport async function generateDocs(config: ProjectConfig): Promise<FileDescriptor[]> {\n  const vars: Record<string, string> = {\n    PROJECT_NAME: config.projectName,\n    ARCHITECTURE: config.architecture,\n    YEAR: new Date().getFullYear().toString(),\n    TASK_TRACKER: config.taskTracker,\n  };\n\n  const files: FileDescriptor[] = [];\n\n  // Core docs always generated\n  const coreDocs = [\n    { template: 'doc_format.md', output: 'docs/doc_format.md' },\n    { template: 'prd.md', output: 'docs/prd.md' },\n    { template: 'architecture.md', output: 'docs/architecture.md' },\n    { template: 'cuj.md', output: 'docs/cuj.md' },\n    { template: 'testing_strategy.md', output: 'docs/testing_strategy.md' },\n    { template: 'onboarding.md', output: 'docs/onboarding.md' },\n  ];\n\n  for (const { template, output } of coreDocs) {\n    const content = await readTemplate(template);\n    files.push({ path: output, content: fillTemplate(content, vars) });\n  }\n\n  // API docs — only if hasApiDocs\n  if (config.hasApiDocs) {\n    const content = await readTemplate('api.md');\n    files.push({ path: 'docs/api.md', content: fillTemplate(content, vars) });\n  }\n\n  // ADR directory with example ADR and template\n  const adrTemplate = await readTemplate('adr_template.md');\n  files.push({\n    path: 'docs/adr/adr_template.md',\n    content: fillTemplate(adrTemplate, { ...vars, NUMBER: 'NNN', TITLE: 'Decision Title' }),\n  });\n\n  // Task tracker file — only for simple markdown\n  if (config.taskTracker === 'markdown') {\n    const content = await readTemplate('tasks_simple.md');\n    files.push({\n      path: 'TASKS.md',\n      content: fillTemplate(content, vars),\n    });\n  }\n\n  return files;\n}\n```\n\nThe generator must be async because it reads template files. Note: `import.meta.url` requires `\"type\": \"module\"` in package.json and ESM output from TypeScript.",
        "testStrategy": "Unit test `test/generators/docs.test.ts`:\n1. With `generateDocs: true, hasApiDocs: false` → output contains `docs/doc_format.md`, `docs/prd.md`, `docs/architecture.md` but NOT `docs/api.md`\n2. With `hasApiDocs: true` → output contains `docs/api.md`\n3. With `taskTracker: 'markdown'` → output contains `TASKS.md`\n4. With `taskTracker: 'taskmaster'` → output does NOT contain `TASKS.md`\n5. `{{PROJECT_NAME}}` is replaced with config.projectName in all generated content\n6. ADR template file is always included",
        "priority": "high",
        "dependencies": [
          "4",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/generators/docs.ts with core template reading infrastructure",
            "description": "Create the docs generator file with the TEMPLATES_DIR constant using import.meta.url and the private readTemplate helper function. Set up the module structure and imports.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/src/generators/docs.ts` with the following structure:\n\n1. Import `fs` from `node:fs/promises`, `path` from `node:path`, `ProjectConfig` and `FileDescriptor` from `../types.js`, and `fillTemplate` from `../utils.js`.\n2. Define `TEMPLATES_DIR` as `new URL('../../templates/docs', import.meta.url).pathname` — this resolves to the `templates/docs/` directory relative to the compiled output.\n3. Implement `async function readTemplate(name: string): Promise<string>` that calls `fs.readFile(path.join(TEMPLATES_DIR, name), 'utf8')`.\n4. Export an empty stub `generateDocs` function returning `Promise<FileDescriptor[]>` for now.\n\nVerify `package.json` already has `\"type\": \"module\"` and that the TypeScript config emits ESM (check `tsconfig.json` for `\"module\": \"NodeNext\"` or similar). The templates already exist at `templates/docs/` with these files: `doc_format.md`, `prd.md`, `architecture.md`, `cuj.md`, `testing_strategy.md`, `onboarding.md`, `api.md`, `adr_template.md`, `tasks_simple.md`.",
            "status": "done",
            "testStrategy": "Manually verify the file compiles with `npx tsc --noEmit`. Confirm `TEMPLATES_DIR` resolves to the correct absolute path by checking the URL construction logic.",
            "updatedAt": "2026-02-17T21:18:55.429Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement core docs generation loop in generateDocs",
            "description": "Implement the main generateDocs export function: build the vars record from ProjectConfig, iterate over the six core doc templates, read each, apply fillTemplate, and return FileDescriptor[].",
            "dependencies": [
              1
            ],
            "details": "Fill in the `generateDocs(config: ProjectConfig): Promise<FileDescriptor[]>` function body:\n\n1. Build the vars record:\n```typescript\nconst vars: Record<string, string> = {\n  PROJECT_NAME: config.projectName,\n  ARCHITECTURE: config.architecture,\n  YEAR: new Date().getFullYear().toString(),\n  TASK_TRACKER: config.taskTracker,\n};\n```\n2. Define the six core docs array:\n```typescript\nconst coreDocs = [\n  { template: 'doc_format.md', output: 'docs/doc_format.md' },\n  { template: 'prd.md', output: 'docs/prd.md' },\n  { template: 'architecture.md', output: 'docs/architecture.md' },\n  { template: 'cuj.md', output: 'docs/cuj.md' },\n  { template: 'testing_strategy.md', output: 'docs/testing_strategy.md' },\n  { template: 'onboarding.md', output: 'docs/onboarding.md' },\n];\n```\n3. Loop over coreDocs: for each, call `readTemplate(template)`, then `fillTemplate(content, vars)`, then push `{ path: output, content }` to the files array.\n4. Return the files array (without optional entries — those come next).\n\nNote: `fillTemplate` replaces `{{PLACEHOLDER}}` markers. Inspect `templates/docs/prd.md` and others to confirm which placeholder variables they use, and ensure all are covered by the vars record.",
            "status": "done",
            "testStrategy": "Write a quick smoke test: call generateDocs with a minimal ProjectConfig and assert the result contains exactly 6 FileDescriptors with paths starting with 'docs/'.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:20:02.233Z"
          },
          {
            "id": 3,
            "title": "Add conditional API docs, ADR template, and markdown task tracker entries",
            "description": "Extend generateDocs to conditionally push docs/api.md when hasApiDocs is true, always push the ADR template to docs/adr/adr_template.md, and push TASKS.md when taskTracker is 'markdown'.",
            "dependencies": [
              2
            ],
            "details": "After the core docs loop, add three conditional blocks:\n\n1. **API docs** (conditional on `config.hasApiDocs`):\n```typescript\nif (config.hasApiDocs) {\n  const content = await readTemplate('api.md');\n  files.push({ path: 'docs/api.md', content: fillTemplate(content, vars) });\n}\n```\n\n2. **ADR template** (always included — gives a starting template for Architecture Decision Records):\n```typescript\nconst adrTemplate = await readTemplate('adr_template.md');\nfiles.push({\n  path: 'docs/adr/adr_template.md',\n  content: fillTemplate(adrTemplate, { ...vars, NUMBER: 'NNN', TITLE: 'Decision Title' }),\n});\n```\nNote: `adr_template.md` contains `{{NUMBER}}` and `{{TITLE}}` placeholders in addition to standard vars — these must be merged into the vars record for the fillTemplate call.\n\n3. **Markdown task tracker** (conditional on `config.taskTracker === 'markdown'`):\n```typescript\nif (config.taskTracker === 'markdown') {\n  const content = await readTemplate('tasks_simple.md');\n  files.push({ path: 'TASKS.md', content: fillTemplate(content, vars) });\n}\n```\n\n`tasks_simple.md` uses `{{PROJECT_NAME}}` which is already in vars.",
            "status": "done",
            "testStrategy": "Test the three conditional branches: (1) hasApiDocs=false → no api.md; (2) hasApiDocs=true → api.md present; (3) taskTracker='markdown' → TASKS.md present; (4) adr_template.md always present with 'NNN' placeholder resolved.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:20:02.241Z"
          },
          {
            "id": 4,
            "title": "Create test/generators/docs.test.ts with full unit test suite",
            "description": "Create the unit test file for the docs generator, covering all documented test cases: core docs presence, API docs conditionality, ADR template, and markdown task tracker file.",
            "dependencies": [
              3
            ],
            "details": "Create `/workspaces/ai-dev-setup/test/generators/docs.test.ts` following the same pattern as `test/generators/claude-md.test.ts`:\n\n```typescript\nimport { describe, it, expect } from 'vitest';\nimport { generateDocs } from '../../src/generators/docs.js';\nimport { defaultConfig } from '../../src/defaults.js';\nimport type { ProjectConfig } from '../../src/types.js';\n\nfunction makeConfig(overrides: Partial<ProjectConfig> = {}): ProjectConfig {\n  return { ...defaultConfig('/tmp/test-project'), ...overrides };\n}\n\ndescribe('generateDocs', () => {\n  it('returns core 6 docs when generateDocs=true, hasApiDocs=false', async () => {\n    const result = await generateDocs(makeConfig({ generateDocs: true, hasApiDocs: false }));\n    const paths = result.map(f => f.path);\n    expect(paths).toContain('docs/doc_format.md');\n    expect(paths).toContain('docs/prd.md');\n    expect(paths).toContain('docs/architecture.md');\n    expect(paths).not.toContain('docs/api.md');\n  });\n\n  it('includes docs/api.md when hasApiDocs=true', async () => {\n    const result = await generateDocs(makeConfig({ hasApiDocs: true }));\n    expect(result.map(f => f.path)).toContain('docs/api.md');\n  });\n\n  it('always includes docs/adr/adr_template.md', async () => {\n    const result = await generateDocs(makeConfig());\n    expect(result.map(f => f.path)).toContain('docs/adr/adr_template.md');\n  });\n\n  it('includes TASKS.md when taskTracker is markdown', async () => {\n    const result = await generateDocs(makeConfig({ taskTracker: 'markdown' }));\n    expect(result.map(f => f.path)).toContain('TASKS.md');\n  });\n\n  it('does NOT include TASKS.md when taskTracker is not markdown', async () => {\n    const result = await generateDocs(makeConfig({ taskTracker: 'taskmaster' }));\n    expect(result.map(f => f.path)).not.toContain('TASKS.md');\n  });\n\n  it('applies PROJECT_NAME substitution in template content', async () => {\n    const result = await generateDocs(makeConfig({ projectName: 'my-cool-app' }));\n    const tasksFile = result.find(f => f.path === 'TASKS.md');\n    // tasks_simple.md uses {{PROJECT_NAME}}\n    // This test only runs if taskTracker='markdown'; adjust makeConfig accordingly\n  });\n});\n```\n\nAlso verify the test works with the actual template files on disk (no mocking needed since it's a unit test that reads real files).",
            "status": "done",
            "testStrategy": "Run `npm test -- test/generators/docs.test.ts` and confirm all tests pass. Verify real template files are read from disk without errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:20:02.246Z"
          },
          {
            "id": 5,
            "title": "Run quality gate checks and fix any issues",
            "description": "Run format, lint, type-check, build, and full test suite. Fix any failures before marking the task done.",
            "dependencies": [
              4
            ],
            "details": "Execute the full quality gate sequence as required by project standards:\n\n1. `npm run format` — run Prettier to format all files\n2. `npm run lint` — run ESLint and fix all errors\n3. `npm run typecheck` — run `tsc --noEmit` and fix any type errors\n4. `npm run build` — compile TypeScript to the output directory and verify zero errors\n5. `npm test` — run the full Vitest suite including the new `test/generators/docs.test.ts`\n\nCommon issues to watch for:\n- ESM import paths must use `.js` extension (e.g., `from '../types.js'`)\n- `import.meta.url` requires the file to be compiled as ESM; verify tsconfig `module` and `moduleResolution` settings\n- Template file path resolution: in tests, the TEMPLATES_DIR will be computed relative to the compiled JS output location, so verify dist/ output structure has access to templates/ (may require tsconfig `include` or npm `files` config to copy templates)\n- If templates are not copied to dist/, the test will fail with ENOENT — in that case, add a build step to copy templates or use a path relative to the project root\n\nLog results using `update_subtask` and set status to done only after all checks pass.",
            "status": "done",
            "testStrategy": "All 5 quality gate steps must exit with code 0. The full test suite must pass with no failures. Zero TypeScript errors. Zero ESLint errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:20:02.254Z"
          }
        ],
        "updatedAt": "2026-02-17T21:20:02.254Z"
      },
      {
        "id": "10",
        "title": "Implement Rules, Skills, and Hooks Generators",
        "description": "Create `src/generators/rules.ts`, `src/generators/skills.ts`, and `src/generators/hooks.ts` — pure functions generating the `.claude/` subdirectory files that implement F3. Rules are path-scoped and conditionally generated based on project config.",
        "details": "**`src/generators/rules.ts`:**\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { ProjectConfig, FileDescriptor } from '../types.js';\nimport { fillTemplate } from '../utils.js';\n\nconst RULES_DIR = new URL('../../templates/rules', import.meta.url).pathname;\n\nasync function readRule(name: string): Promise<string> {\n  return fs.readFile(path.join(RULES_DIR, name), 'utf8');\n}\n\nexport async function generateRules(config: ProjectConfig): Promise<FileDescriptor[]> {\n  const vars = { PROJECT_NAME: config.projectName, TASK_TRACKER: config.taskTracker };\n  const files: FileDescriptor[] = [];\n\n  // Always generated\n  const alwaysRules = ['general.md', 'docs.md', 'testing.md', 'git.md', 'security.md', 'config.md'];\n  for (const name of alwaysRules) {\n    files.push({\n      path: `.claude/rules/${name}`,\n      content: fillTemplate(await readRule(name), vars),\n    });\n  }\n\n  // Conditional — API rules only if api docs selected\n  if (config.hasApiDocs) {\n    const content = await readRule('api.md');\n    const withImport = config.generateDocs\n      ? content + '\\n\\n@docs/api.md'\n      : content;\n    files.push({ path: '.claude/rules/api.md', content: fillTemplate(withImport, vars) });\n  }\n\n  // Conditional — database rules only if project has DB\n  if (config.hasDatabase) {\n    files.push({\n      path: '.claude/rules/database.md',\n      content: fillTemplate(await readRule('database.md'), vars),\n    });\n  }\n\n  // Conditional — agent teams rule only if opted in\n  if (config.agentTeamsEnabled) {\n    files.push({\n      path: '.claude/rules/agent-teams.md',\n      content: fillTemplate(await readRule('agent-teams.md'), vars),\n    });\n  }\n\n  return files;\n}\n```\n\n**`src/generators/skills.ts`:**\n```typescript\nexport async function generateSkills(config: ProjectConfig): Promise<FileDescriptor[]> {\n  // Read skill templates and return FileDescriptors\n  // All three skills always generated: testing.md, commit.md, task-workflow.md\n  // task-workflow.md gets TASK_TRACKER substituted\n}\n```\n\n**`src/generators/hooks.ts`:**\n```typescript\nexport async function generateHooks(config: ProjectConfig): Promise<FileDescriptor[]> {\n  // Generate .claude/hooks/pre-commit.sh (executable)\n  // Generate .claude/settings.json entry for PreToolUse hook\n  return [\n    {\n      path: '.claude/hooks/pre-commit.sh',\n      content: preCommitContent,\n      executable: true,\n    },\n  ];\n}\n```\n\n`.claude/settings.json` update: add the hook matcher. Read existing settings.json if present, merge the hooks entry.",
        "testStrategy": "Unit tests `test/generators/rules.test.ts`, `skills.test.ts`, `hooks.test.ts`:\n1. Rules: With `hasApiDocs: false` → no `api.md` rule generated\n2. Rules: With `hasApiDocs: true, generateDocs: true` → `api.md` rule contains `@docs/api.md`\n3. Rules: With `agentTeamsEnabled: true` → `agent-teams.md` rule generated\n4. Rules: Always includes `testing.md` with 'Integration tests' and 'Demo checkpoints' sections\n5. Skills: All 3 skill files generated, `task-workflow.md` has tracker-specific content\n6. Hooks: `pre-commit.sh` is marked executable, contains `--if-present` flags\n7. Hooks: Pre-commit content has all 5 quality gate steps in order",
        "priority": "high",
        "dependencies": [
          "4",
          "6"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-17T21:28:51.186Z"
      },
      {
        "id": "11",
        "title": "Implement Devcontainer Generator",
        "description": "Create `src/generators/devcontainer.ts` — generates `.devcontainer/devcontainer.json` with lifecycle hooks calling `ai-init` phases. This enables automatic setup when a Codespace is created or rebuilt.",
        "details": "Create `src/generators/devcontainer.ts`:\n\n```typescript\nimport { ProjectConfig, FileDescriptor } from '../types.js';\n\ninterface DevcontainerConfig {\n  name: string;\n  image: string;\n  features: Record<string, unknown>;\n  onCreateCommand: string;\n  postCreateCommand: string;\n  postStartCommand: string;\n  customizations: {\n    vscode: {\n      extensions: string[];\n    };\n  };\n  secrets: Record<string, { description: string; documentationUrl: string }>;\n  containerEnv: Record<string, string>;\n  remoteEnv: Record<string, string>;\n}\n\nexport function generateDevcontainer(config: ProjectConfig): FileDescriptor[] {\n  const devcontainer: DevcontainerConfig = {\n    name: config.projectName,\n    image: 'mcr.microsoft.com/devcontainers/universal:2',\n    features: {\n      'ghcr.io/devcontainers/features/node:1': { version: '20' },\n    },\n    onCreateCommand: 'ai-init on-create',\n    postCreateCommand: 'ai-init post-create',\n    postStartCommand: 'ai-init post-start',\n    customizations: {\n      vscode: {\n        extensions: [\n          'GitHub.copilot',\n          'GitHub.copilot-chat',\n        ],\n      },\n    },\n    secrets: {\n      ANTHROPIC_API_KEY: {\n        description: 'Anthropic API key for Claude Code',\n        documentationUrl: 'https://console.anthropic.com/settings/keys',\n      },\n    },\n    containerEnv: { ANTHROPIC_API_KEY: '${localEnv:ANTHROPIC_API_KEY}' },\n    remoteEnv: { ANTHROPIC_API_KEY: '${localEnv:ANTHROPIC_API_KEY}' },\n  };\n\n  return [\n    {\n      path: '.devcontainer/devcontainer.json',\n      content: JSON.stringify(devcontainer, null, 2),\n    },\n  ];\n}\n```\n\nThis is a pure function (sync) since it doesn't read templates — it builds the config object directly. The lifecycle commands use `ai-init` (the symlinked CLI binary) rather than `setup-ai.sh`, enabling the TypeScript tool to replace the bash bootstrap entirely.",
        "testStrategy": "Unit test `test/generators/devcontainer.test.ts`:\n1. Output contains exactly one FileDescriptor with path `.devcontainer/devcontainer.json`\n2. Parsed JSON has `onCreateCommand: 'ai-init on-create'`\n3. Parsed JSON has `postCreateCommand: 'ai-init post-create'`\n4. Parsed JSON has `postStartCommand: 'ai-init post-start'`\n5. JSON is valid (JSON.parse succeeds without throwing)\n6. `config.projectName` appears as the `name` field",
        "priority": "medium",
        "dependencies": [
          "2",
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/generators/devcontainer.ts with core generator function",
            "description": "Implement the pure synchronous `generateDevcontainer` function that builds the `.devcontainer/devcontainer.json` FileDescriptor with lifecycle hooks, VS Code extensions, secrets, and env vars.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/src/generators/devcontainer.ts` following the same module structure as `mcp-json.ts` (pure function, no fs calls). Define the `DevcontainerConfig` interface locally. The function signature must be `export function generateDevcontainer(config: ProjectConfig): FileDescriptor[]`. Hard-code the base config: `image: 'mcr.microsoft.com/devcontainers/universal:2'`, `features: { 'ghcr.io/devcontainers/features/node:1': { version: '20' } }`, lifecycle commands `onCreateCommand: 'ai-init on-create'`, `postCreateCommand: 'ai-init post-create'`, `postStartCommand: 'ai-init post-start'`. Always include `GitHub.copilot` and `GitHub.copilot-chat` in `customizations.vscode.extensions`. Use `config.projectName` for the `name` field. Return exactly one `FileDescriptor` with `path: '.devcontainer/devcontainer.json'` and `content: JSON.stringify(devcontainer, null, 2)`.",
            "status": "done",
            "testStrategy": null,
            "updatedAt": "2026-02-17T21:35:34.659Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add MCP-driven secrets and containerEnv to devcontainer config",
            "description": "Extend `generateDevcontainer` to dynamically append secrets and container environment variables for each selected MCP server that has env vars defined in the registry.",
            "dependencies": [
              1
            ],
            "details": "Import `getSelectedServers` from `../registry.js`. After building the base `DevcontainerConfig`, iterate `getSelectedServers(config.selectedMcps)` and for each server that has a non-empty `env` record, add an entry to `secrets` with `{ description: 'API key for <server.description>', documentationUrl: '' }` and add matching entries to both `containerEnv` and `remoteEnv` using `'${localEnv:VAR_NAME}'` syntax. Always include the base `ANTHROPIC_API_KEY` secret and env entry (already in the static config), so skip it when iterating to avoid duplication. This mirrors the pattern used in `mcp-json.ts` where `getSelectedServers` drives env generation. The function remains synchronous since `getSelectedServers` is a pure registry lookup.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:35:41.544Z"
          },
          {
            "id": 3,
            "title": "Write unit tests in test/generators/devcontainer.test.ts",
            "description": "Create a comprehensive Vitest test file for `generateDevcontainer` covering output shape, lifecycle commands, MCP-driven secrets, and JSON validity.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create `/workspaces/ai-dev-setup/test/generators/devcontainer.test.ts` mirroring the style of `test/generators/mcp-json.test.ts`. Use `defaultConfig('/tmp/test-project')` via a `makeConfig` helper. Required test cases:\n1. Returns exactly one `FileDescriptor` with `path === '.devcontainer/devcontainer.json'`.\n2. Content is valid JSON (use `JSON.parse`).\n3. Parsed JSON has `onCreateCommand: 'ai-init on-create'`.\n4. Parsed JSON has `postCreateCommand: 'ai-init post-create'`.\n5. Parsed JSON has `postStartCommand: 'ai-init post-start'`.\n6. `name` equals `config.projectName`.\n7. `customizations.vscode.extensions` contains `'GitHub.copilot'` and `'GitHub.copilot-chat'`.\n8. When `selectedMcps` includes `'taskmaster'`, `secrets` includes `ANTHROPIC_API_KEY` and `PERPLEXITY_API_KEY` entries.\n9. `containerEnv.ANTHROPIC_API_KEY` equals `'${localEnv:ANTHROPIC_API_KEY}'`.\n10. Empty `selectedMcps` still produces the base `ANTHROPIC_API_KEY` secret.",
            "status": "done",
            "testStrategy": "Run `npm test -- test/generators/devcontainer.test.ts` — all cases must pass with zero failures.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:36:57.988Z"
          },
          {
            "id": 4,
            "title": "Export generateDevcontainer from the generators barrel",
            "description": "Add `generateDevcontainer` to the centralized exports so other modules (orchestrator, phases) can import it from a single entry point.",
            "dependencies": [
              1
            ],
            "details": "Check whether `/workspaces/ai-dev-setup/src/generators/index.ts` exists. If it does, add `export { generateDevcontainer } from './devcontainer.js';` alongside existing exports. If it does not exist yet, create it and export all current generators: `generateMcpJson` from `./mcp-json.js`, `generateClaudeMd` from `./claude-md.js`, `generateDocs` from `./docs.js`, `generateRules` from `./rules.js`, `generateSkills` from `./skills.js`, `generateHooks` from `./hooks.js`, and `generateDevcontainer` from `./devcontainer.js`. Also check `src/index.ts` (the main entry) and add a re-export if generators are surfaced there. Follow existing import patterns using `.js` extensions for ESM compatibility.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:36:08.942Z"
          },
          {
            "id": 5,
            "title": "Run full quality gate: format, lint, typecheck, build, and test",
            "description": "Execute the mandatory pre-completion checklist — format, lint, typecheck, build, and full test suite — fixing any issues found.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Run the following commands in sequence from `/workspaces/ai-dev-setup` and fix any failures before proceeding to the next step:\n1. `npm run format` — prettify all changed files.\n2. `npm run lint` (or `npx eslint . --fix`) — fix all lint errors.\n3. `npm run typecheck` (or `npx tsc --noEmit`) — ensure zero TypeScript errors; pay attention to the `DevcontainerConfig` interface shape and any missing `?` on optional fields.\n4. `npm run build` — verify the project compiles cleanly.\n5. `npm test` — run the full test suite and confirm all tests pass, including the new `devcontainer.test.ts`.\nIf any step fails, fix the root cause and re-run from that step. Log findings to the subtask via `update_subtask`.",
            "status": "done",
            "testStrategy": "All five commands must exit with code 0. The `devcontainer.test.ts` suite must show all tests as passing in the `npm test` output.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:38:16.760Z"
          }
        ],
        "updatedAt": "2026-02-17T21:38:16.760Z"
      },
      {
        "id": "12",
        "title": "Implement Commands Generator",
        "description": "Create `src/generators/commands.ts` — generates the `.claude/commands/` slash command files and `boot-prompt.txt`. Implements F8 (Custom Claude Commands).",
        "details": "Create `src/generators/commands.ts`:\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { ProjectConfig, FileDescriptor } from '../types.js';\nimport { fillTemplate } from '../utils.js';\n\nconst COMMANDS_DIR = new URL('../../templates/commands', import.meta.url).pathname;\nconst TEMPLATES_DIR = new URL('../../templates', import.meta.url).pathname;\n\nexport async function generateCommands(config: ProjectConfig): Promise<FileDescriptor[]> {\n  const vars: Record<string, string> = {\n    PROJECT_NAME: config.projectName,\n    TASK_TRACKER: config.taskTracker,\n    TASK_TRACKER_NEXT: getTrackerNextCommand(config.taskTracker),\n    TASK_TRACKER_DONE: getTrackerDoneCommand(config.taskTracker),\n  };\n\n  const devNext = await fs.readFile(path.join(COMMANDS_DIR, 'dev-next.md'), 'utf8');\n  const review = await fs.readFile(path.join(COMMANDS_DIR, 'review.md'), 'utf8');\n  const bootPrompt = await fs.readFile(path.join(TEMPLATES_DIR, 'boot-prompt.txt'), 'utf8');\n\n  return [\n    { path: '.claude/commands/dev-next.md', content: fillTemplate(devNext, vars) },\n    { path: '.claude/commands/review.md', content: fillTemplate(review, vars) },\n    { path: '.claude/boot-prompt.txt', content: fillTemplate(bootPrompt, vars) },\n  ];\n}\n\nfunction getTrackerNextCommand(tracker: string): string {\n  switch (tracker) {\n    case 'taskmaster': return 'task-master next';\n    case 'beads': return 'bd show';\n    case 'markdown': return 'Read TASKS.md and find the next pending task';\n    default: return 'task-master next';\n  }\n}\n\nfunction getTrackerDoneCommand(tracker: string): string {\n  switch (tracker) {\n    case 'taskmaster': return 'task-master set-status --id=<id> --status=done';\n    case 'beads': return 'bd update <id> --status done && bd sync';\n    case 'markdown': return 'Edit TASKS.md and mark task as [x]';\n    default: return 'task-master set-status --id=<id> --status=done';\n  }\n}\n```\n\nThe `dev-next.md` command references docs/adr/ for architecture decisions — this cross-reference makes the command self-updating as the project adds ADRs.",
        "testStrategy": "Unit test `test/generators/commands.test.ts`:\n1. With `taskTracker: 'taskmaster'` → `dev-next.md` contains `task-master next`\n2. With `taskTracker: 'beads'` → `dev-next.md` contains `bd show`\n3. Output always includes 3 files: `dev-next.md`, `review.md`, `boot-prompt.txt`\n4. `review.md` contains `git diff` reference\n5. `boot-prompt.txt` has tracker-specific content substituted",
        "priority": "medium",
        "dependencies": [
          "4",
          "6"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-17T21:44:20.714Z"
      },
      {
        "id": "13",
        "title": "Implement Lifecycle Phases (on-create, post-create, post-start)",
        "description": "Create `src/phases/on-create.ts`, `src/phases/post-create.ts`, and `src/phases/post-start.ts`. These handle the three Codespace lifecycle events, porting the existing bash phase logic into TypeScript.",
        "details": "**`src/phases/on-create.ts`** — Heavy installs, called once during Codespace creation:\n```typescript\nimport { run, commandExists } from '../utils.js';\n\nexport async function runOnCreate(): Promise<void> {\n  console.log('[ai-init] Phase: on-create — installing global tools...');\n  \n  // Install Claude Code if not present\n  if (!(await commandExists('claude'))) {\n    await run('npm', ['install', '-g', '@anthropic-ai/claude-code']);\n  }\n  \n  // Install Task Master if not present\n  if (!(await commandExists('task-master'))) {\n    await run('npm', ['install', '-g', 'task-master-ai']);\n  }\n  \n  console.log('[ai-init] on-create complete.');\n}\n```\n\n**`src/phases/post-create.ts`** — Project configuration, orchestrates all generators:\n```typescript\nimport { ProjectConfig } from '../types.js';\nimport { writeFiles } from '../utils.js';\nimport { generateMcpJson } from '../generators/mcp-json.js';\nimport { generateClaudeMd } from '../generators/claude-md.js';\nimport { generateDocs } from '../generators/docs.js';\nimport { generateRules } from '../generators/rules.js';\nimport { generateSkills } from '../generators/skills.js';\nimport { generateHooks } from '../generators/hooks.js';\nimport { generateDevcontainer } from '../generators/devcontainer.js';\nimport { generateCommands } from '../generators/commands.js';\n\nexport async function runPostCreate(\n  config: ProjectConfig,\n  overwrite = true\n): Promise<string[]> {\n  const allFiles = [\n    ...generateMcpJson(config),\n    ...generateClaudeMd(config),\n    ...(config.generateDocs ? await generateDocs(config) : []),\n    ...(config.generateRules ? await generateRules(config) : []),\n    ...(config.generateSkills ? await generateSkills(config) : []),\n    ...(config.generateHooks ? await generateHooks(config) : []),\n    ...generateDevcontainer(config),\n    ...(config.generateCommands ? await generateCommands(config) : []),\n  ];\n\n  const written = await writeFiles(allFiles, config.projectRoot, overwrite);\n  return written;\n}\n```\n\n**`src/phases/post-start.ts`** — Per-session setup:\n```typescript\nexport async function runPostStart(config: ProjectConfig): Promise<void> {\n  // 1. Sync .env from secrets (copy ANTHROPIC_API_KEY etc. to .env if not present)\n  // 2. Print welcome banner with task progress\n  // 3. Show pending task count from tasks.json or TASKS.md\n}\n```\n\nThe post-start banner reads `.taskmaster/tasks/tasks.json` if using Task Master, or `TASKS.md` if using simple markdown, and prints a summary of pending tasks.",
        "testStrategy": "Integration test `test/integration/phases.test.ts`:\n1. `runPostCreate` in a temp directory creates expected files for a minimal config\n2. With `overwrite: false`, existing files are not overwritten\n3. `runPostCreate` returns the list of written file paths\n4. All file paths in the return value actually exist on disk after the call\n5. Smoke test: `runOnCreate` doesn't throw when claude/task-master already exist",
        "priority": "high",
        "dependencies": [
          "4",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/phases/on-create.ts with global tool installation logic",
            "description": "Implement the on-create phase module that installs Claude Code and Task Master globally if not already present, using the existing `run` and `commandExists` utilities from `src/utils.ts`.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/src/phases/on-create.ts`. Import `run` and `commandExists` from `../utils.js`. Export an async `runOnCreate(): Promise<void>` function. Log `[ai-init] Phase: on-create — installing global tools...` at the start. Use `commandExists('claude')` to conditionally run `npm install -g @anthropic-ai/claude-code`. Use `commandExists('task-master')` to conditionally run `npm install -g task-master-ai`. Log `[ai-init] on-create complete.` at the end. Do not hardcode package versions — use `@latest` or no version pin so npm resolves the current version.",
            "status": "done",
            "testStrategy": "Unit test in `test/phases/on-create.test.ts` using vitest. Mock `commandExists` and `run` from `../../src/utils.js`. Verify: (1) when both commands exist, `run` is never called; (2) when `claude` is missing, `run` is called with `['install', '-g', '@anthropic-ai/claude-code']`; (3) when `task-master` is missing, `run` is called with `['install', '-g', 'task-master-ai']`; (4) function resolves without throwing when all installs succeed.",
            "updatedAt": "2026-02-17T21:54:35.375Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create src/phases/post-create.ts orchestrating all generators",
            "description": "Implement the post-create phase module that imports and invokes all 8 generators, collects their FileDescriptor arrays, and writes everything to disk via `writeFiles`, returning the list of written paths.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/src/phases/post-create.ts`. Import `ProjectConfig` from `../types.js` and `writeFiles` from `../utils.js`. Import all 8 generators: `generateMcpJson` from `../generators/mcp-json.js`, `generateClaudeMd` from `../generators/claude-md.js`, `generateDocs` from `../generators/docs.js`, `generateRules` from `../generators/rules.js`, `generateSkills` from `../generators/skills.js`, `generateHooks` from `../generators/hooks.js`, `generateDevcontainer` from `../generators/devcontainer.js`, and `generateCommands` from `../generators/commands.js`. Export an async `runPostCreate(config: ProjectConfig, overwrite = true): Promise<string[]>` function. Conditionally await async generators based on config feature flags (e.g., `config.generateDocs`, `config.generateRules`, etc.) while always running `generateMcpJson`, `generateClaudeMd`, and `generateDevcontainer`. Spread all FileDescriptor arrays into `allFiles`, call `writeFiles(allFiles, config.projectRoot, overwrite)`, and return the result.",
            "status": "done",
            "testStrategy": "Integration test in `test/integration/phases.test.ts` using a temp directory (`os.tmpdir()` + random suffix, cleaned in `afterEach`). Verify: (1) `runPostCreate` with a minimal config creates expected files; (2) with `overwrite: false`, existing files are not overwritten; (3) return value contains relative paths of written files; (4) conditional generators (docs, rules, skills, hooks, commands) are skipped when their flags are false; (5) all required generators always run regardless of flags.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:54:44.451Z"
          },
          {
            "id": 3,
            "title": "Create src/phases/post-start.ts with env sync and welcome banner",
            "description": "Implement the post-start phase module that reads API key environment variables and syncs them to `.env` if missing, then reads task progress from `.taskmaster/tasks/tasks.json` or `TASKS.md` and prints a formatted welcome banner.",
            "dependencies": [
              2
            ],
            "details": "Create `/workspaces/ai-dev-setup/src/phases/post-start.ts`. Import `ProjectConfig` from `../types.js`, `readOptional` from `../utils.js`, and `fs` from `node:fs/promises` and `path` from `node:path`. Export an async `runPostStart(config: ProjectConfig): Promise<void>` function. Step 1 — env sync: define a list of well-known API key names (`ANTHROPIC_API_KEY`, `PERPLEXITY_API_KEY`, `OPENAI_API_KEY`, `GOOGLE_API_KEY`). Read `.env` from `config.projectRoot` using `readOptional`. For each key present in `process.env` but absent from the `.env` file content, append `KEY=VALUE` lines. Write the updated `.env` back only if changes were made. Step 2 — task banner: if `config.taskTracker === 'taskmaster'`, read `.taskmaster/tasks/tasks.json` and count tasks by status (`pending`, `in-progress`, `done`); otherwise read `TASKS.md` and count unchecked `- [ ]` lines vs checked `- [x]` lines. Print a banner with `[ai-init] Welcome back!`, project name, and task counts. Gracefully handle missing files (print zero counts or skip).",
            "status": "done",
            "testStrategy": "Unit test in `test/phases/post-start.test.ts` using a temp directory. Verify: (1) when `.env` is missing and env vars are set, a `.env` file is created with those vars; (2) when `.env` already contains the key, it is not duplicated; (3) with a valid `tasks.json` containing mixed-status tasks, the banner prints correct pending/done counts; (4) with a `TASKS.md` containing `- [ ]` and `- [x]` lines, markdown counts are reported; (5) when neither file exists, function completes without throwing.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:54:51.071Z"
          },
          {
            "id": 4,
            "title": "Add phases barrel export and integrate phases into src/cli.ts",
            "description": "Create a `src/phases/index.ts` barrel that re-exports all three phase functions, then wire them into `src/cli.ts` so that `ai-init on-create`, `ai-init post-create`, and `ai-init post-start` subcommands invoke the correct phase.",
            "dependencies": [
              3
            ],
            "details": "Create `/workspaces/ai-dev-setup/src/phases/index.ts` with three named re-exports: `export { runOnCreate } from './on-create.js'`, `export { runPostCreate } from './post-create.js'`, and `export { runPostStart } from './post-start.js'`. Then read the current `src/cli.ts`. Add a `switch` (or `if/else`) on `process.argv[2]` to handle three subcommands: `'on-create'` → `await runOnCreate()`, `'post-create'` → load config (from `.taskmaster/tasks/tasks.json` or a persisted `ai-init.config.json` if it exists, else use `defaultConfig(process.cwd())`) then `await runPostCreate(config)`, `'post-start'` → similarly load config then `await runPostStart(config)`. Print usage and exit with code 1 for unknown subcommands. Ensure the CLI entry is the file listed as `bin` in `package.json`.",
            "status": "done",
            "testStrategy": "Smoke test by running `npx tsx src/cli.ts --help` (or the built binary) and verifying it does not throw. Integration test: spawn `tsx src/cli.ts on-create` in a subprocess with mocked PATH (where `claude` and `task-master` are fake scripts) and assert exit code 0. Also test that an unknown subcommand exits with code 1.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:54:57.308Z"
          },
          {
            "id": 5,
            "title": "Run quality gates and fix all lint, type, build, and test failures",
            "description": "Run the full quality pipeline (format, lint, typecheck, build, test) against the new phase files and integration tests, and fix any issues found before the task can be marked done.",
            "dependencies": [
              4
            ],
            "details": "From `/workspaces/ai-dev-setup`, run the following commands in order and fix all errors before proceeding to the next step: (1) `npm run format` — applies Prettier to all source and test files; (2) `npm run lint` — runs ESLint with auto-fix, resolve any remaining errors; (3) `npm run typecheck` — runs `tsc --noEmit`, fix all TypeScript type errors in the three new phase files, the barrel export, and any updated cli.ts; (4) `npm run build` — compiles TypeScript to `dist/`, resolve any compilation errors; (5) `npm test` — runs the full Vitest suite including the new phase tests and existing generator tests, ensure all 300+ existing tests still pass and new phase tests pass. Log findings using `update_subtask` before marking the task chain done.",
            "status": "done",
            "testStrategy": "All five npm scripts must exit with code 0. Specifically: `npm run format` makes no further changes when re-run; `npm run lint` reports zero errors; `npm run typecheck` reports zero errors; `npm run build` produces `dist/phases/on-create.js`, `dist/phases/post-create.js`, `dist/phases/post-start.js`, and `dist/phases/index.js`; `npm test` reports all tests passing with no skipped phase tests.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:55:03.588Z"
          }
        ],
        "updatedAt": "2026-02-17T21:55:03.588Z"
      },
      {
        "id": "14",
        "title": "Implement Interactive Wizard",
        "description": "Create `src/wizard.ts` — the interactive 10-step setup flow using `@inquirer/prompts`. Each step is skippable. The wizard collects a `ProjectConfig` from user input, respecting environment variable overrides for non-interactive mode.",
        "details": "Create `src/wizard.ts`:\n\n```typescript\nimport { select, checkbox, confirm, input } from '@inquirer/prompts';\nimport { ProjectConfig, TaskTracker, Architecture } from './types.js';\nimport { MCP_REGISTRY } from './registry.js';\nimport { defaultConfig } from './defaults.js';\n\nconst NON_INTERACTIVE = process.env.SETUP_AI_NONINTERACTIVE === '1';\n\nfunction fromEnv<T>(key: string, fallback: T): T {\n  const val = process.env[key];\n  return val !== undefined ? (val as unknown as T) : fallback;\n}\n\nexport async function runWizard(projectRoot: string): Promise<ProjectConfig> {\n  const config = defaultConfig(projectRoot);\n\n  console.log('\\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━');\n  console.log('  AI Project Init — Setup Wizard');\n  console.log('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n');\n\n  // Step 0: Claude Code Bootstrap (handled in on-create phase, just verify here)\n  // Step 1: MCP Server Selection\n  if (NON_INTERACTIVE) {\n    const envMcps = process.env.SETUP_AI_MCPS;\n    config.selectedMcps = envMcps ? envMcps.split(',') : ['taskmaster'];\n  } else {\n    const choices = MCP_REGISTRY.map(s => ({\n      name: `${s.name} — ${s.description}`,\n      value: s.name,\n      checked: s.name === 'taskmaster',\n    }));\n    config.selectedMcps = await checkbox({\n      message: 'Step 1: Select MCP servers to configure:',\n      choices,\n    });\n    // Ensure taskmaster is included if selected as tracker\n  }\n\n  // Step 2: Task Tracker\n  if (NON_INTERACTIVE) {\n    config.taskTracker = fromEnv<TaskTracker>('SETUP_AI_TRACKER', 'taskmaster');\n  } else {\n    config.taskTracker = await select({\n      message: 'Step 2: Choose a task tracker:',\n      choices: [\n        { name: 'Task Master (recommended — subtasks, research, complexity analysis)', value: 'taskmaster' },\n        { name: 'Beads (multi-agent, git-native issue tracking)', value: 'beads' },\n        { name: 'Simple Markdown (for small projects, ≤20 tasks)', value: 'markdown' },\n      ],\n      default: 'taskmaster',\n    }) as TaskTracker;\n  }\n\n  // Step 3: PRD\n  // Step 4: Architecture\n  if (NON_INTERACTIVE) {\n    config.architecture = fromEnv<Architecture>('SETUP_AI_ARCH', 'skip');\n  } else {\n    config.architecture = await select({\n      message: 'Step 4: Project architecture (populates docs/architecture.md):',\n      choices: [\n        { name: 'Skip', value: 'skip' },\n        { name: 'Monolith', value: 'monolith' },\n        { name: '2-tier (frontend + backend)', value: '2-tier' },\n        { name: '3-tier (frontend + API + database)', value: '3-tier' },\n        { name: 'Microservices', value: 'microservices' },\n      ],\n    }) as Architecture;\n  }\n\n  // Steps 5-6: API and Doc generation\n  if (!NON_INTERACTIVE) {\n    config.hasApiDocs = await confirm({ message: 'Step 5: Generate API docs template?', default: config.architecture !== 'skip' && config.architecture !== 'monolith' });\n    config.hasDatabase = await confirm({ message: 'Does this project use a database?', default: config.architecture === '3-tier' });\n  }\n\n  // Step 7: Generate docs/rules/skills/hooks confirmation\n  // Step 8: Agent Teams\n  if (NON_INTERACTIVE) {\n    config.agentTeamsEnabled = process.env.SETUP_AI_AGENT_TEAMS === '1';\n  } else {\n    config.agentTeamsEnabled = await confirm({\n      message: 'Step 8 (Optional): Enable Claude Code experimental agent teams mode?',\n      default: false,\n    });\n  }\n\n  // Step 9: Audit\n  if (NON_INTERACTIVE) {\n    config.runAudit = process.env.SETUP_AI_SKIP_AUDIT !== '1';\n  } else {\n    config.runAudit = await confirm({\n      message: 'Step 9: Run AI-powered audit of generated files? (uses API credits)',\n      default: true,\n    });\n  }\n\n  return config;\n}\n```\n\n**Non-interactive env vars supported:**\n- `SETUP_AI_NONINTERACTIVE=1` — skip all prompts\n- `SETUP_AI_MCPS=taskmaster,context7` — pre-select MCPs\n- `SETUP_AI_TRACKER=taskmaster|beads|markdown`\n- `SETUP_AI_ARCH=monolith|2-tier|3-tier|microservices|skip`\n- `SETUP_AI_AGENT_TEAMS=1`\n- `SETUP_AI_SKIP_AUDIT=1`",
        "testStrategy": "Integration test `test/integration/wizard.test.ts` using `@inquirer/testing` to automate prompt responses:\n1. Wizard returns valid `ProjectConfig` with all required fields\n2. Non-interactive mode (`SETUP_AI_NONINTERACTIVE=1`) returns defaults without prompting\n3. `SETUP_AI_MCPS=taskmaster,context7` → `selectedMcps` contains both\n4. `SETUP_AI_TRACKER=beads` → `taskTracker === 'beads'`\n5. `SETUP_AI_SKIP_AUDIT=1` → `runAudit === false`",
        "priority": "high",
        "dependencies": [
          "3",
          "13"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/wizard.ts with all 10 interactive steps",
            "description": "Implement the full wizard module with all 10 setup steps using @inquirer/prompts, supporting both interactive and non-interactive (env var) modes.",
            "dependencies": [],
            "details": "Create /workspaces/ai-dev-setup/src/wizard.ts exactly as specified in the task details. The file must:\n1. Import select, checkbox, confirm, input from '@inquirer/prompts'\n2. Import ProjectConfig, TaskTracker, Architecture from './types.js'\n3. Import MCP_REGISTRY from './registry.js'\n4. Import defaultConfig from './defaults.js'\n5. Define NON_INTERACTIVE constant from env var SETUP_AI_NONINTERACTIVE\n6. Implement fromEnv<T> helper for env var overrides\n7. Export async runWizard(projectRoot: string): Promise<ProjectConfig>\n8. Step 1: MCP server selection — multi-select checkbox from MCP_REGISTRY entries, default taskmaster checked. Env override: SETUP_AI_MCPS (comma-separated)\n9. Step 2: Task tracker selection — select from taskmaster/beads/markdown. Env override: SETUP_AI_TRACKER\n10. Step 3: PRD path — input prompt asking for optional path to existing PRD file, sets config.hasPrd and config.prdPath. Env override: SETUP_AI_PRD_PATH\n11. Step 4: Architecture picker — select from skip/monolith/2-tier/3-tier/microservices. Env override: SETUP_AI_ARCH\n12. Step 5: API docs toggle — confirm prompt, default true when architecture is 2-tier/3-tier/microservices. Env override: SETUP_AI_API_DOCS\n13. Step 6: Database toggle — confirm prompt, default true when architecture is 3-tier. Env override: SETUP_AI_DB\n14. Step 7: Docs/rules/skills/hooks/commands generation flags — confirm prompts for each. Env overrides: SETUP_AI_GEN_DOCS, etc.\n15. Step 8: Agent teams opt-in — confirm, default false. Env override: SETUP_AI_AGENT_TEAMS=1\n16. Step 9: Audit toggle — confirm, default true. Env override: SETUP_AI_SKIP_AUDIT=1\n17. Step 10: Display summary of collected config (projectName, tracker, architecture, selected MCPs, feature flags) and any next-step instructions\n18. Return the populated config object",
            "status": "done",
            "testStrategy": "Manual smoke test: run `npx tsx src/cli.ts` and verify wizard prompts appear and return valid ProjectConfig.",
            "updatedAt": "2026-02-17T22:04:27.435Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate wizard into src/cli.ts as default command",
            "description": "Update cli.ts to invoke runWizard() when no lifecycle subcommand is provided, then call post-create phase with the resulting config.",
            "dependencies": [
              1
            ],
            "details": "Read /workspaces/ai-dev-setup/src/cli.ts and update the default case (currently prints version). Changes:\n1. Import runWizard from './wizard.js'\n2. Import the post-create phase runner (or relevant orchestration function) from './phases/post-create.js'\n3. In the default command handler (the else/fallthrough branch after lifecycle commands), call: const config = await runWizard(process.cwd()); then invoke the appropriate orchestration to generate and write files using that config\n4. Ensure the wizard path is async (wrap in async IIFE or mark the handler async if needed)\n5. Add a --interactive / -i flag alias if a non-interactive path also exists, for clarity\n6. Preserve existing on-create, post-create, post-start subcommand handling unchanged",
            "status": "done",
            "testStrategy": "Run `npx tsx src/cli.ts --help` to verify help output, run `npx tsx src/cli.ts on-create` to verify lifecycle commands still work.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:05:06.988Z"
          },
          {
            "id": 3,
            "title": "Write unit tests for wizard non-interactive mode",
            "description": "Create test/wizard.test.ts with tests covering non-interactive env var mode to ensure all config fields are correctly set without any prompts.",
            "dependencies": [
              1
            ],
            "details": "Create /workspaces/ai-dev-setup/test/wizard.test.ts:\n1. Import runWizard from '../src/wizard.js'\n2. Import defaultConfig from '../src/defaults.js'\n3. Use a makeConfig helper consistent with other test files\n4. Test: SETUP_AI_NONINTERACTIVE=1 with no other vars returns config with defaults (selectedMcps=['taskmaster'], taskTracker='taskmaster', architecture='skip', agentTeamsEnabled=false, runAudit=true)\n5. Test: SETUP_AI_MCPS=taskmaster,context7 sets selectedMcps to ['taskmaster','context7']\n6. Test: SETUP_AI_TRACKER=beads sets taskTracker to 'beads'\n7. Test: SETUP_AI_ARCH=3-tier sets architecture to '3-tier'\n8. Test: SETUP_AI_AGENT_TEAMS=1 sets agentTeamsEnabled to true\n9. Test: SETUP_AI_SKIP_AUDIT=1 sets runAudit to false\n10. Each test must set process.env vars before calling runWizard and restore/delete them in afterEach\n11. Use vitest describe/it/expect/beforeEach/afterEach patterns consistent with existing tests\n12. Verify returned object satisfies ProjectConfig shape (all required fields present)",
            "status": "done",
            "testStrategy": "Run `npm test` and verify all new wizard tests pass with no errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:06:00.315Z"
          },
          {
            "id": 4,
            "title": "Run formatter, linter, and type-checker",
            "description": "Execute npm run format, npm run lint, and npx tsc --noEmit to ensure wizard.ts and cli.ts changes meet project code quality standards.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Execute the project's quality gate commands in order:\n1. Run `npm run format` (prettier --write) — auto-fixes formatting issues in all files including src/wizard.ts, src/cli.ts, and test/wizard.test.ts\n2. Run `npm run lint` (eslint --fix) — auto-fixes lint errors; review any remaining warnings\n3. Run `npx tsc --noEmit` — verify TypeScript compilation passes with zero errors; fix any type errors in wizard.ts (especially the fromEnv<T> helper and type assertions for TaskTracker/Architecture)\n4. Common type issues to watch: ensuring select() return values are cast to TaskTracker/Architecture, ensuring checkbox returns string[], ensuring confirm returns boolean\n5. Fix any issues found and re-run the respective command until it passes",
            "status": "done",
            "testStrategy": "All three commands exit with code 0 and produce no errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:07:02.666Z"
          },
          {
            "id": 5,
            "title": "Run build and full test suite",
            "description": "Execute npm run build and npm test to verify the wizard implementation compiles successfully and all tests pass.",
            "dependencies": [
              4
            ],
            "details": "Execute the build and test commands:\n1. Run `npm run build` — compiles TypeScript to dist/; verify exit code 0 and dist/wizard.js is emitted\n2. Run `npm test` (vitest run) — runs all tests in test/**/*.test.ts; verify all pass including the new test/wizard.test.ts\n3. If build fails: check for missing imports, incorrect module paths (must use .js extensions for ESM), or unresolved types\n4. If tests fail: review error output, fix logic in wizard.ts or test assertions, re-run\n5. Pay attention to: ESM import paths must use .js extension (e.g., './types.js' not './types'), async/await correctness, env var cleanup between tests",
            "status": "done",
            "testStrategy": "npm run build exits 0, npm test exits 0 with all test suites passing.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:07:09.393Z"
          },
          {
            "id": 6,
            "title": "Log implementation findings and mark task complete",
            "description": "Use update_subtask to record implementation notes, confirm all quality gates passed, then set task 14 status to done.",
            "dependencies": [
              5
            ],
            "details": "Perform final task completion steps:\n1. Use the taskmaster-ai MCP tool `update_subtask` (or task-master CLI) to log implementation notes on task 14, including: what was implemented, any design decisions made (e.g. how fromEnv<T> handles type coercion), any issues encountered and fixed during format/lint/typecheck/build/test, and confirmation all checks passed\n2. Verify the summary/next-steps display (Step 10 of wizard) works correctly by reviewing the console.log output format\n3. Confirm test/wizard.test.ts covers all env var overrides defined in the task spec\n4. Call `set_task_status --id=14 --status=done` via MCP or CLI to mark the task complete\n5. If any quality gate failed and could not be fixed, set status to 'blocked' with explanation instead",
            "status": "done",
            "testStrategy": "Task 14 status transitions to 'done' in .taskmaster/tasks/tasks.json after all prior steps succeed.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:07:53.245Z"
          }
        ],
        "updatedAt": "2026-02-17T22:07:53.245Z"
      },
      {
        "id": "15",
        "title": "Implement Claude Code Bootstrap and Audit Runner",
        "description": "Create `src/audit.ts` — the Claude Code headless audit runner implementing F11. Checks for Claude Code installation, runs a structured audit of generated files, and saves results to `.ai-init-audit.md`.",
        "details": "Create `src/audit.ts`:\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { commandExists, run } from './utils.js';\nimport { ProjectConfig } from './types.js';\n\nconst AUDIT_PROMPT_TEMPLATE = `You are auditing the output of the ai-init project bootstrap tool.\nReview ONLY the files listed below — these were just generated by the setup wizard.\nDo NOT review or comment on any other files in the project.\n\nGenerated files:\n{{GENERATED_FILES}}\n\nAudit checklist:\n1. STRUCTURE: Are all generated docs following the format in docs/doc_format.md?\n   Check: TOC present, sections <30 lines, tables for structured data, TLDR at top.\n\n2. CROSS-REFERENCES: Does CLAUDE.md accurately reference all generated docs?\n   Check: Every @import points to a file that exists. No broken references.\n\n3. RULES CONSISTENCY: Do .claude/rules/ files reference correct path patterns?\n   Check: Rules that @import docs reference docs that were actually generated.\n\n4. MCP CONFIG: Is .mcp.json valid JSON with correct package names?\n   Check: All selected MCP servers present. No duplicates.\n\n5. TEMPLATE COMPLETENESS: Which sections still have placeholder content the user MUST fill?\n   Flag each file and specific section.\n\n6. GAPS: What is missing that the user should address manually?\n\n7. AGENT INSTRUCTIONS: Are CLAUDE.md and rules well-structured and actionable?\n   Flag any vague or generic instructions.\n\nOutput format:\n- ✅ PASS: <area> — <one-line summary>\n- ⚠️  FILL: <file>:<section> — <what the user needs to add>\n- ❌ FIX: <file>:<issue> — <what's wrong and how to fix it>\n\nEnd with a numbered \"Post-Setup Checklist\" of manual actions before starting development.`;\n\nexport async function checkClaudeCodeAvailable(): Promise<boolean> {\n  if (!(await commandExists('claude'))) {\n    return false;\n  }\n  try {\n    await run('claude', ['--version']);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\nexport async function installClaudeCode(): Promise<void> {\n  console.log('[ai-init] Installing Claude Code...');\n  await run('npm', ['install', '-g', '@anthropic-ai/claude-code']);\n}\n\nexport async function runAudit(\n  config: ProjectConfig,\n  generatedFiles: string[]\n): Promise<void> {\n  if (!(await checkClaudeCodeAvailable())) {\n    console.warn('[ai-init] Skipping audit — Claude Code not available');\n    return;\n  }\n\n  const filesList = generatedFiles.map(f => `  - ${f}`).join('\\n');\n  const prompt = AUDIT_PROMPT_TEMPLATE.replace('{{GENERATED_FILES}}', filesList);\n\n  console.log('[ai-init] Running AI-powered audit of generated files...');\n\n  let auditOutput: string;\n  try {\n    auditOutput = await run('claude', ['--headless', '--print', prompt], config.projectRoot);\n  } catch (err) {\n    console.warn('[ai-init] Audit failed — review generated files manually');\n    console.warn(err);\n    return;\n  }\n\n  // Save audit results\n  const auditPath = path.join(config.projectRoot, '.ai-init-audit.md');\n  await fs.writeFile(auditPath, `# AI Init Audit Results\\n\\n${auditOutput}\\n`, 'utf8');\n  \n  console.log('\\n--- AUDIT RESULTS ---');\n  console.log(auditOutput);\n  console.log('\\nAudit saved to: .ai-init-audit.md');\n}\n```\n\nAdd `.ai-init-audit.md` to `.gitignore`.\n\n**Graceful degradation:**\n- Claude not installed → skip with message\n- No API credentials → skip with message\n- Audit errors → catch, warn, continue\n- Never block wizard completion",
        "testStrategy": "Unit test `test/generators/audit.test.ts`:\n1. `checkClaudeCodeAvailable()` returns false when 'claude' not on PATH (mock commandExists)\n2. `runAudit()` with unavailable Claude → prints warning, does NOT throw\n3. `runAudit()` with audit error → catches error, does NOT propagate\n4. Audit prompt template contains all 7 audit checklist items\n5. `{{GENERATED_FILES}}` placeholder is replaced with actual file list",
        "priority": "medium",
        "dependencies": [
          "4",
          "13"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/audit.ts with checkClaudeCodeAvailable and installClaudeCode functions",
            "description": "Implement the core module `src/audit.ts` with the `checkClaudeCodeAvailable()` and `installClaudeCode()` exported functions using the existing `commandExists` and `run` utilities from `src/utils.ts`.",
            "dependencies": [],
            "details": "Create `src/audit.ts` importing `fs` from `node:fs/promises`, `path` from `node:path`, `commandExists` and `run` from `./utils.js`, and `ProjectConfig` from `./types.js`. Implement `checkClaudeCodeAvailable(): Promise<boolean>` — calls `commandExists('claude')` first; if false returns false; otherwise calls `run('claude', ['--version'])` and returns true, catching any error to return false. Implement `installClaudeCode(): Promise<void>` — logs `[ai-init] Installing Claude Code...` then calls `run('npm', ['install', '-g', '@anthropic-ai/claude-code'])`. Define the `AUDIT_PROMPT_TEMPLATE` constant with the full audit prompt containing the `{{GENERATED_FILES}}` placeholder as specified in the task details. All three must be exported. Follow the same import pattern as other source files in the project (e.g., `src/generators/commands.ts`).",
            "status": "done",
            "testStrategy": "Unit test in `test/generators/audit.test.ts`: mock `commandExists` to return false and verify `checkClaudeCodeAvailable()` returns false without calling `run`; mock `commandExists` to return true and mock `run` to succeed and verify returns true; mock `run` to throw and verify returns false.",
            "updatedAt": "2026-02-17T22:14:11.274Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement runAudit function in src/audit.ts with graceful degradation",
            "description": "Add the `runAudit(config: ProjectConfig, generatedFiles: string[]): Promise<void>` exported function to `src/audit.ts`, implementing all graceful-degradation paths: Claude not available, run error, and the happy path that saves the audit output to `.ai-init-audit.md`.",
            "dependencies": [
              1
            ],
            "details": "Inside `runAudit`: (1) Call `checkClaudeCodeAvailable()`; if false, log `[ai-init] Skipping audit — Claude Code not available` and return. (2) Build `filesList` by mapping `generatedFiles` to `  - ${f}` lines joined by `\\n`, substitute into `AUDIT_PROMPT_TEMPLATE` via `.replace('{{GENERATED_FILES}}', filesList)`. (3) Log `[ai-init] Running AI-powered audit of generated files...`. (4) Call `run('claude', ['--headless', '--print', prompt], config.projectRoot)` in a try/catch — on error, log `[ai-init] Audit failed — review generated files manually`, log the error, and return without throwing. (5) On success, compute `auditPath = path.join(config.projectRoot, '.ai-init-audit.md')`, write `# AI Init Audit Results\\n\\n${auditOutput}\\n` with `fs.writeFile`. (6) Log the audit output to stdout and log `Audit saved to: .ai-init-audit.md`. Follow the error handling pattern from `src/phases/post-create.ts`.",
            "status": "done",
            "testStrategy": "Unit tests: (1) `runAudit` with unavailable Claude prints warning and returns without throwing; (2) `runAudit` with `run` throwing catches the error and does NOT propagate; (3) `runAudit` happy path writes `.ai-init-audit.md` with correct header and content; (4) Audit prompt template has `{{GENERATED_FILES}}` replaced with the file list.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:14:16.891Z"
          },
          {
            "id": 3,
            "title": "Wire runAudit into post-create phase in src/phases/post-create.ts",
            "description": "Update `src/phases/post-create.ts` to import `runAudit` from `../audit.js` and call it after `writeFiles()` when `config.runAudit` is true, passing `config` and the written file list.",
            "dependencies": [
              1,
              2
            ],
            "details": "In `src/phases/post-create.ts`: add `import { runAudit } from '../audit.js';` alongside existing imports. After the existing `config.generatedFiles.push(...written)` line and the `console.log` for post-create complete, add a conditional block: `if (config.runAudit) { await runAudit(config, written); }`. This ensures the audit runs on the actual list of files written to disk (not the full `allFiles` array), and only when `config.runAudit` is true (the flag already exists in `ProjectConfig` and defaults to `true` in `src/defaults.ts`). The audit must never block wizard completion — its graceful degradation is already handled inside `runAudit`.",
            "status": "done",
            "testStrategy": "Integration test in `test/integration/phases.test.ts`: add a test case with `runAudit: false` to verify no `.ai-init-audit.md` is created and post-create still completes normally. Existing tests should continue to pass since `checkClaudeCodeAvailable()` will return false in the test environment (no `claude` binary on PATH).",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:15:04.521Z"
          },
          {
            "id": 4,
            "title": "Write unit tests for audit.ts in test/generators/audit.test.ts",
            "description": "Create `test/generators/audit.test.ts` with comprehensive unit tests for all exported functions in `src/audit.ts`, using vitest's `vi.mock` to mock `commandExists` and `run` from `src/utils.ts`.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create `test/generators/audit.test.ts` following the pattern of `test/generators/commands.test.ts`. Import `describe`, `it`, `expect`, `vi`, `beforeEach`, `afterEach` from vitest. Use `vi.mock('../../src/utils.js', ...)` to mock `commandExists` and `run`. Tests to include: (1) `checkClaudeCodeAvailable()` returns false when `commandExists('claude')` returns false; (2) `checkClaudeCodeAvailable()` returns true when `commandExists` returns true and `run` succeeds; (3) `checkClaudeCodeAvailable()` returns false when `run` throws; (4) `runAudit()` with `commandExists` returning false logs warning and resolves without throwing; (5) `runAudit()` with `run` throwing catches error, logs warning, resolves without throwing, and does NOT write `.ai-init-audit.md`; (6) `runAudit()` happy path with `run` returning audit text — verify `fs.writeFile` is called with the correct path and content beginning with `# AI Init Audit Results`. Use `vi.spyOn(fs, 'writeFile')` or mock `node:fs/promises`. Also verify AUDIT_PROMPT_TEMPLATE substitution by checking the prompt passed to `run` contains the file list.",
            "status": "done",
            "testStrategy": "Run `npm test test/generators/audit.test.ts` — all test cases must pass. Run the full suite `npm test` to ensure no regressions in existing tests.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:15:58.629Z"
          },
          {
            "id": 5,
            "title": "Run quality gate checks and fix any issues",
            "description": "Execute the mandatory pre-completion checklist: format, lint, type-check, build, and full test suite. Fix any failures discovered.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Run the following in order from the project root `/workspaces/ai-dev-setup`: (1) `npm run format` — fix any formatting issues in the new files; (2) `npm run lint` — fix all lint errors in `src/audit.ts` and `test/generators/audit.test.ts`; (3) `npm run typecheck` or `npx tsc --noEmit` — ensure `src/audit.ts` and its integration in `post-create.ts` have no TypeScript errors (verify `runAudit` signature matches `ProjectConfig` from `./types.js` and `run`/`commandExists` imports from `./utils.js`); (4) `npm run build` — verify the project compiles cleanly to `dist/`; (5) `npm test` — run the full test suite including the new `test/generators/audit.test.ts` and the integration test in `test/integration/phases.test.ts`. Confirm `.ai-init-audit.md` is already present in `.gitignore` (it is, at line 31). Fix any issues found and re-run from the failing step.",
            "status": "done",
            "testStrategy": "All five quality gate steps must exit with code 0. The new audit test file must contain at least 6 passing test cases. The integration phase tests must still pass with the new `runAudit` wiring (graceful degradation handles the no-claude-binary case in CI).",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:16:51.618Z"
          }
        ],
        "updatedAt": "2026-02-17T22:16:51.618Z"
      },
      {
        "id": "16",
        "title": "Implement CLI Entry Point with Argument Parsing",
        "description": "Create `src/cli.ts` — the main entry point that wires together the wizard, phases, and audit. Uses `meow` for argument parsing. Handles all subcommands: `ai-init`, `ai-init on-create`, `ai-init post-create`, `ai-init post-start`, `ai-init --non-interactive`.",
        "details": "Create `src/cli.ts`:\n\n```typescript\n#!/usr/bin/env node\nimport meow from 'meow';\nimport path from 'node:path';\nimport { runWizard } from './wizard.js';\nimport { runOnCreate } from './phases/on-create.js';\nimport { runPostCreate } from './phases/post-create.js';\nimport { runPostStart } from './phases/post-start.js';\nimport { runAudit, checkClaudeCodeAvailable, installClaudeCode } from './audit.js';\n\nconst cli = meow(`\n  Usage\n    $ ai-init [command] [options]\n\n  Commands\n    (none)          Interactive setup wizard\n    on-create       Heavy installs — run once during Codespace creation\n    post-create     Project scaffolding — run after Codespace creation\n    post-start      Per-session setup — run on every container start\n\n  Options\n    --non-interactive   Skip prompts, use environment variables\n    --no-audit          Skip the Claude Code audit step\n    --overwrite         Overwrite existing files (default: true)\n    --version           Show version\n    --help              Show help\n\n  Environment Variables\n    SETUP_AI_NONINTERACTIVE=1   Same as --non-interactive\n    SETUP_AI_MCPS               Comma-separated MCP names\n    SETUP_AI_TRACKER            taskmaster | beads | markdown\n    SETUP_AI_ARCH               monolith | 2-tier | 3-tier | microservices | skip\n    SETUP_AI_SKIP_AUDIT=1       Skip audit step\n    SETUP_AI_AGENT_TEAMS=1      Enable agent teams\n`, {\n  importMeta: import.meta,\n  flags: {\n    nonInteractive: { type: 'boolean', default: false },\n    audit: { type: 'boolean', default: true },\n    overwrite: { type: 'boolean', default: true },\n  },\n});\n\nasync function main() {\n  const [command] = cli.input;\n  const projectRoot = process.cwd();\n\n  switch (command) {\n    case 'on-create':\n      await runOnCreate();\n      break;\n\n    case 'post-create': {\n      // In lifecycle mode, use env vars for config\n      process.env.SETUP_AI_NONINTERACTIVE = '1';\n      const config = await runWizard(projectRoot);\n      const written = await runPostCreate(config, cli.flags.overwrite);\n      console.log(`[ai-init] Generated ${written.length} files.`);\n      break;\n    }\n\n    case 'post-start':\n      // Import config from .setup-ai-mcps or use defaults\n      // Run per-session setup\n      break;\n\n    default: {\n      // Interactive wizard (default command)\n      // Step 0: Ensure Claude Code is installed\n      if (!(await checkClaudeCodeAvailable())) {\n        await installClaudeCode().catch(() => {\n          console.warn('[ai-init] Could not install Claude Code — audit will be skipped');\n        });\n      }\n\n      const config = await runWizard(projectRoot);\n      const written = await runPostCreate(config, cli.flags.overwrite);\n      config.generatedFiles = written;\n\n      console.log(`\\n[ai-init] Generated ${written.length} files:`);\n      written.forEach(f => console.log(`  ✓ ${f}`));\n\n      if (config.runAudit && cli.flags.audit) {\n        await runAudit(config, written);\n      }\n\n      console.log('\\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━');\n      console.log('  Setup complete! Next steps:');\n      console.log(`  1. Fill in docs/prd.md with your project requirements`);\n      console.log(`  2. Run 'task-master parse-prd docs/prd.md' to generate tasks`);\n      console.log(`  3. Open Claude Code and run /dev-next to start building`);\n      console.log('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n');\n    }\n  }\n}\n\nmain().catch(err => {\n  console.error('[ai-init] Fatal error:', err.message);\n  process.exit(1);\n});\n```\n\nAdd shebang `#!/usr/bin/env node` at the top. The `bin` field in package.json points to `dist/cli.js`.",
        "testStrategy": "Integration test `test/integration/cli.test.ts` — run the compiled CLI in a temp directory:\n1. `ai-init --non-interactive` in a temp dir creates expected files without prompting\n2. `ai-init on-create` doesn't throw (skips installs if tools already present)\n3. `ai-init --help` prints usage info\n4. `ai-init --version` prints version from package.json\n5. With `SETUP_AI_NONINTERACTIVE=1 SETUP_AI_TRACKER=markdown`, generated CLAUDE.md contains TASKS.md reference",
        "priority": "high",
        "dependencies": [
          "13",
          "14",
          "15"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Install meow dependency and verify package.json bin field",
            "description": "Confirm meow is installed at latest version and the bin field in package.json correctly points to dist/cli.js. Verify meow ESM import works with the project's tsconfig (ESNext modules, NodeNext resolution).",
            "dependencies": [],
            "details": "Run `npm view meow version` to confirm current version. Check package.json `bin` field is `{ \"ai-init\": \"./dist/cli.js\" }`. Verify meow is listed in `dependencies` (not devDependencies) since it's needed at runtime. Check tsconfig.json `moduleResolution` supports named ESM imports from meow. If meow is missing, run `npm install meow@latest`. Verify the import `import meow from 'meow'` compiles without TypeScript errors by running `npx tsc --noEmit`.",
            "status": "pending",
            "testStrategy": "Run `npx tsc --noEmit` — zero errors confirms meow types resolve correctly. Run `npm ls meow` to confirm it appears in the dependency tree.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Refactor src/cli.ts to use meow for argument parsing",
            "description": "Replace the manual `process.argv.slice(2)` parsing in src/cli.ts with a meow-based CLI definition. Define all flags (`--non-interactive`, `--audit`, `--overwrite`) with correct types and defaults. Wire the meow `cli.input[0]` value into the existing switch-case command dispatch.",
            "dependencies": [
              1
            ],
            "details": "Rewrite src/cli.ts to match the target implementation:\n\n1. Add `import meow from 'meow'` at the top (after the shebang).\n2. Define the meow instance with the full help text string covering all commands and environment variables.\n3. Set `importMeta: import.meta` for ESM compatibility.\n4. Define flags: `nonInteractive: { type: 'boolean', default: false }`, `audit: { type: 'boolean', default: true }`, `overwrite: { type: 'boolean', default: true }`.\n5. Replace `args[0]` with `cli.input[0]` and `args.includes('--non-interactive')` with `cli.flags.nonInteractive`.\n6. Remove the manual `--version`, `--help`, and `--non-interactive` branches — meow handles `--version` and `--help` automatically; `--non-interactive` becomes a flag.\n7. In the `post-create` case, pass `cli.flags.overwrite` to `runPostCreate`.\n8. In the default case, conditionally call `runAudit` only if `config.runAudit && cli.flags.audit`.\n9. Add `checkClaudeCodeAvailable` and `installClaudeCode` imports from `./audit.js`.\n10. Keep the `#!/usr/bin/env node` shebang as the very first line.",
            "status": "pending",
            "testStrategy": "Run `npm run build` to compile. Run `node dist/cli.js --help` to confirm meow-formatted help is printed. Run `node dist/cli.js --version` to confirm version is printed. Run `node dist/cli.js --unknown-flag` to confirm meow error handling works.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add file list output and next-steps banner to default command",
            "description": "In the default (interactive wizard) command branch, print the list of generated files with checkmarks and display a formatted next-steps banner after setup completes. Also wire the `--overwrite` flag into the `runPostCreate` call.",
            "dependencies": [
              2
            ],
            "details": "In the `default` switch case of `main()`:\n\n1. After `runPostCreate(config, cli.flags.overwrite)`, capture the returned `written` array.\n2. Set `config.generatedFiles = written`.\n3. Print `\\n[ai-init] Generated ${written.length} files:` followed by each file path prefixed with `  ✓ `.\n4. Add the separator banner:\n```\nconsole.log('\\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━');\nconsole.log('  Setup complete! Next steps:');\nconsole.log('  1. Fill in docs/prd.md with your project requirements');\nconsole.log('  2. Run \\'task-master parse-prd docs/prd.md\\' to generate tasks');\nconsole.log('  3. Open Claude Code and run /dev-next to start building');\nconsole.log('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n');\n```\n5. Also update the `post-create` case to capture and log the written file count: `console.log('[ai-init] Generated ${written.length} files.')`.\n6. Verify `runPostCreate` signature accepts an `overwrite: boolean` second parameter and returns `Promise<string[]>`.",
            "status": "pending",
            "testStrategy": "Run integration test in a temp directory with `SETUP_AI_NONINTERACTIVE=1` — stdout should contain `Generated N files:` and individual file paths with `✓` prefix, followed by the banner. Check `config.generatedFiles` is populated.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add Claude Code availability check to default command flow",
            "description": "In the default (interactive wizard) command branch, call `checkClaudeCodeAvailable()` before running the wizard. If not available, attempt installation via `installClaudeCode()` with graceful error handling.",
            "dependencies": [
              2
            ],
            "details": "In the `default` switch case, before calling `runWizard(projectRoot)`:\n\n1. Add:\n```typescript\nif (!(await checkClaudeCodeAvailable())) {\n  await installClaudeCode().catch(() => {\n    console.warn('[ai-init] Could not install Claude Code — audit will be skipped');\n  });\n}\n```\n2. Ensure `checkClaudeCodeAvailable` and `installClaudeCode` are imported from `'./audit.js'`. Check the current audit.ts exports to confirm these function names exist — if named differently, use the actual export names.\n3. This step must not block the wizard from running even if installation fails (the `.catch()` handles the error gracefully).\n4. The `--non-interactive` flow via `cli.flags.nonInteractive` should also set `process.env.SETUP_AI_NONINTERACTIVE = '1'` before calling `runWizard` — wire this in the default case based on the flag value.",
            "status": "pending",
            "testStrategy": "Mock `checkClaudeCodeAvailable` to return false and `installClaudeCode` to reject — the wizard should still run and the warning should be printed. Mock both to succeed — wizard runs without any warning. Verify audit.ts exports these function names.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write integration tests for the CLI entry point",
            "description": "Create `test/integration/cli.test.ts` with integration tests that compile and invoke the CLI in a temporary directory. Cover: non-interactive mode creates expected files, `on-create` subcommand runs without error, `--help` prints usage info, and `--version` prints the version string.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Create `test/integration/cli.test.ts`:\n\n1. Use `vitest` with `beforeEach`/`afterEach` hooks that create and remove a temp directory (`os.tmpdir() + '/' + crypto.randomUUID()`).\n2. Use Node's `child_process.execFile` (promisified) to invoke `node dist/cli.js` in the temp directory.\n3. Test cases:\n   - `--help`: stdout contains `Usage` and `Commands`, process exits 0.\n   - `--version`: stdout contains `0.` (version string), process exits 0.\n   - `SETUP_AI_NONINTERACTIVE=1 node dist/cli.js`: runs without hanging, exits 0, temp dir contains expected files like `CLAUDE.md` or `.mcp.json`.\n   - `node dist/cli.js on-create`: exits 0 (tools may already be installed — should not throw).\n4. Each test should set a 30-second timeout to accommodate the non-interactive run.\n5. Run `npm run build` before tests to ensure `dist/cli.js` is up to date — add a `beforeAll` hook or note in test that build must be run first.\n6. Run the full quality gate: `npm run format`, `npm run lint`, `npm run typecheck`, `npm run build`, `npm test`.",
            "status": "pending",
            "testStrategy": "All four test cases pass. `npm run typecheck` reports zero errors. `npm run build` compiles without errors. `npm test` exits 0 with all tests green.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-02-17T22:25:28.515Z"
      },
      {
        "id": "17",
        "title": "Create install.sh Bootstrap Script",
        "description": "Create the `install.sh` single-line install script that ensures Node.js ≥ 20 is available (via fnm if needed), clones/pulls the repo, runs `npm ci`, and symlinks `ai-init` to `~/.local/bin/`. This implements F1's single-line install.",
        "details": "Create `install.sh`:\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# ============================================================\n# AI Helper Tools — Bootstrap Installer\n# Usage: curl -fsSL https://raw.githubusercontent.com/.../install.sh | bash\n# ============================================================\n\nAI_HELPER_HOME=\"${AI_HELPER_HOME:-$HOME/.ai-helper-tools}\"\nBIN_DIR=\"${HOME}/.local/bin\"\nREPO_URL=\"https://github.com/potgieterdl/ai-helper-tools.git\"\nNODE_MIN_VERSION=20\n\necho \"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\necho \"  AI Helper Tools — Installer\"\necho \"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\n\n# 1. Check for Node.js ≥ 20, install via fnm if missing\nensure_node() {\n  if command -v node &>/dev/null; then\n    local version\n    version=$(node --version | sed 's/v//' | cut -d. -f1)\n    if [ \"$version\" -ge \"$NODE_MIN_VERSION\" ]; then\n      echo \"✓ Node.js $(node --version) found\"\n      return 0\n    fi\n    echo \"Node.js $(node --version) is too old (need >= ${NODE_MIN_VERSION}). Installing newer version via fnm...\"\n  else\n    echo \"Node.js not found. Installing via fnm...\"\n  fi\n\n  # Install fnm\n  curl -fsSL https://fnm.vercel.app/install | bash\n  export PATH=\"$HOME/.local/share/fnm:$PATH\"\n  eval \"$(fnm env)\"\n  fnm install \"$NODE_MIN_VERSION\"\n  fnm use \"$NODE_MIN_VERSION\"\n  fnm default \"$NODE_MIN_VERSION\"\n  echo \"✓ Node.js $(node --version) installed via fnm\"\n}\n\n# 2. Clone or update the repository\nensure_repo() {\n  if [ -d \"$AI_HELPER_HOME/.git\" ]; then\n    echo \"Updating ai-helper-tools...\"\n    git -C \"$AI_HELPER_HOME\" pull --ff-only\n  else\n    echo \"Cloning ai-helper-tools to $AI_HELPER_HOME...\"\n    git clone \"$REPO_URL\" \"$AI_HELPER_HOME\"\n  fi\n  echo \"✓ Repository ready at $AI_HELPER_HOME\"\n}\n\n# 3. Install dependencies\ninstall_deps() {\n  echo \"Installing dependencies...\"\n  npm ci --prefix \"$AI_HELPER_HOME\" --silent\n  npm run build --prefix \"$AI_HELPER_HOME\" --silent\n  echo \"✓ Dependencies installed\"\n}\n\n# 4. Symlink ai-init to PATH\nsetup_bin() {\n  mkdir -p \"$BIN_DIR\"\n  ln -sf \"$AI_HELPER_HOME/dist/cli.js\" \"$BIN_DIR/ai-init\"\n  chmod +x \"$AI_HELPER_HOME/dist/cli.js\"\n  echo \"✓ ai-init symlinked to $BIN_DIR/ai-init\"\n\n  # Ensure BIN_DIR is in PATH\n  if [[ \":$PATH:\" != *\":$BIN_DIR:\"* ]]; then\n    echo \"\"\n    echo \"Add this to your shell config (~/.bashrc or ~/.zshrc):\"\n    echo \"  export PATH=\\\"$BIN_DIR:\\$PATH\\\"\"\n  fi\n}\n\nensure_node\nensure_repo\ninstall_deps\nsetup_bin\n\necho \"\"\necho \"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\necho \"  Installation complete!\"\necho \"  Run 'ai-init' in any project directory to get started.\"\necho \"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\n```\n\nMake executable: `chmod +x install.sh`\n\nAdd `.gitignore` entries: `dist/`, `node_modules/`, `.ai-init-audit.md`",
        "testStrategy": "Manual test in a clean environment: run `bash install.sh` with Node.js absent (using Docker `node:alpine` with Node removed), verify fnm is installed and Node 20 is available after script runs. Verify `ai-init --version` works after install. Automated: test that `install.sh` is executable and contains `fnm.vercel.app/install`, `npm ci`, and symlink logic.",
        "priority": "high",
        "dependencies": [
          "16"
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": "18",
        "title": "Implement Agent Teams Configuration Generator",
        "description": "Create the agent teams opt-in configuration — updates `~/.claude/settings.json` with `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1` env flag and generates the `agent-teams.md` rule file. Implements F10.",
        "details": "Create `src/generators/agent-teams.ts`:\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport os from 'node:os';\nimport { ProjectConfig, FileDescriptor } from '../types.js';\n\nexport async function configureAgentTeams(\n  config: ProjectConfig\n): Promise<void> {\n  if (!config.agentTeamsEnabled) return;\n\n  // Update ~/.claude/settings.json (user-level setting)\n  const claudeSettingsPath = path.join(os.homedir(), '.claude', 'settings.json');\n  \n  let existing: Record<string, unknown> = {};\n  try {\n    const content = await fs.readFile(claudeSettingsPath, 'utf8');\n    existing = JSON.parse(content);\n  } catch {\n    // File doesn't exist — start fresh\n  }\n\n  const merged = {\n    ...existing,\n    env: {\n      ...((existing.env as Record<string, string>) ?? {}),\n      CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS: '1',\n    },\n  };\n\n  await fs.mkdir(path.dirname(claudeSettingsPath), { recursive: true });\n  await fs.writeFile(claudeSettingsPath, JSON.stringify(merged, null, 2), 'utf8');\n  console.log('[ai-init] Agent teams mode enabled in ~/.claude/settings.json');\n}\n```\n\nThe `agent-teams.md` rule is handled by the rules generator (Task 10) when `agentTeamsEnabled: true`. This function handles only the user-level settings file update, which cannot go through the normal `writeFiles()` mechanism (it's outside the project root).\n\n**Integration in post-create:** Call `configureAgentTeams(config)` at the end of `runPostCreate()` after all file generation.",
        "testStrategy": "Unit test `test/generators/agent-teams.test.ts`:\n1. With `agentTeamsEnabled: false` → `configureAgentTeams()` returns without writing any files\n2. With `agentTeamsEnabled: true` and no existing settings file → creates file with `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS: '1'`\n3. With existing settings file containing other keys → merges without overwriting existing keys\n4. With existing `env` block → merges the new key into existing env block",
        "priority": "low",
        "dependencies": [
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/generators/agent-teams.ts with configureAgentTeams function",
            "description": "Implement the agent-teams generator module that updates ~/.claude/settings.json with the CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1 env flag when agentTeamsEnabled is true.",
            "dependencies": [],
            "details": "Create `src/generators/agent-teams.ts` following the same pattern as `src/generators/hooks.ts`. The function must: (1) early-return when `config.agentTeamsEnabled` is false, (2) read existing `~/.claude/settings.json` if it exists (gracefully handling missing file), (3) merge in `env.CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS: '1'` preserving all existing keys, (4) create the `~/.claude/` directory if needed with `fs.mkdir({ recursive: true })`, (5) write the merged JSON back to `~/.claude/settings.json`, (6) log `[ai-init] Agent teams mode enabled in ~/.claude/settings.json`. Import `fs from 'node:fs/promises'`, `path from 'node:path'`, `os from 'node:os'`, and `type { ProjectConfig } from '../types.js'`. Note: do NOT import `FileDescriptor` since this function has no return value (returns `Promise<void>`).",
            "status": "done",
            "testStrategy": "Covered by unit tests in subtask 2.",
            "parentId": "undefined",
            "updatedAt": "2026-02-18T07:42:42.915Z"
          },
          {
            "id": 2,
            "title": "Write unit tests in test/generators/agent-teams.test.ts",
            "description": "Create a comprehensive unit test file that verifies all behaviour of configureAgentTeams, using a temp directory for the home dir simulation via environment stubbing.",
            "dependencies": [
              1
            ],
            "details": "Create `test/generators/agent-teams.test.ts` following the style of `test/generators/hooks.test.ts`. Use `vitest` (`describe`, `it`, `expect`, `beforeEach`, `afterEach`). In each test, redirect `os.homedir()` by mocking it (or by writing to a real `os.tmpdir()` sub-path). Tests to include: (1) with `agentTeamsEnabled: false` — function returns without writing any files; (2) with `agentTeamsEnabled: true` and no existing settings file — creates `~/.claude/settings.json` containing `env.CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS: '1'`; (3) with an existing settings file that has other keys — merges in the env flag without removing existing keys; (4) with existing settings that already have an `env` object with other vars — merges into the existing env object; (5) written file is valid JSON. Use `makeConfig` helper with `defaultConfig` + overrides pattern consistent with other test files.",
            "status": "done",
            "testStrategy": "Tests themselves are the test strategy; run with `npm test -- agent-teams`.",
            "parentId": "undefined",
            "updatedAt": "2026-02-18T07:43:18.715Z"
          },
          {
            "id": 3,
            "title": "Integrate configureAgentTeams call into runPostCreate in src/phases/post-create.ts",
            "description": "Import and call configureAgentTeams at the end of runPostCreate, after writeFiles, so the user-level settings update happens once all project files are written.",
            "dependencies": [
              1
            ],
            "details": "Edit `src/phases/post-create.ts`. Add import: `import { configureAgentTeams } from '../generators/agent-teams.js';`. After the `config.generatedFiles.push(...written)` line (and before the final `return written`), add: `await configureAgentTeams(config);`. This placement ensures: (a) all project-scoped files are already written, (b) the user-level settings update only runs when `config.agentTeamsEnabled` is true (the guard is inside `configureAgentTeams`). The function is async so the await is required. Do not change any other logic in `runPostCreate`.",
            "status": "done",
            "testStrategy": "Verify integration by running the full test suite (`npm test`) after the change. Specifically, existing post-create phase tests must still pass and not regress.",
            "parentId": "undefined",
            "updatedAt": "2026-02-18T07:43:47.986Z"
          },
          {
            "id": 4,
            "title": "Verify agent-teams.md rule template exists and is used by generateRules",
            "description": "Confirm the agent-teams.md rule template file exists under templates/rules/ and that generateRules already includes it when agentTeamsEnabled is true (Task 10 dependency), documenting that no additional rule generation logic is needed in this task.",
            "dependencies": [],
            "details": "Run `ls templates/rules/` to confirm `agent-teams.md` exists. Then read `src/generators/rules.ts` and verify the conditional block `if (config.agentTeamsEnabled)` reads and emits `.claude/rules/agent-teams.md`. Check that `test/generators/rules.test.ts` already has a passing test for this path (it does — see lines 74-86). If the template file is missing, create a minimal `templates/rules/agent-teams.md` with the agent-teams three-tier agent coordination content described in CLAUDE.md. This subtask is verification/guard-rail — no code change expected unless the template is absent.",
            "status": "done",
            "testStrategy": "Run `npm test -- rules` to confirm the conditional agent-teams rule tests pass. Visually inspect the template file content to ensure it describes the orchestrator/executor/checker roles.",
            "parentId": "undefined",
            "updatedAt": "2026-02-18T07:48:29.701Z"
          },
          {
            "id": 5,
            "title": "Run full quality gate and mark task complete",
            "description": "Execute the mandatory pre-completion checklist: format, lint, type-check, build, and full test suite. Fix any failures discovered.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Run the following commands in order per CLAUDE.md quality gate requirements: (1) `npm run format` — fix any formatting issues in new/changed files; (2) `npm run lint` — fix all lint errors (warnings acceptable); (3) `npm run typecheck` — resolve any TypeScript errors, particularly ensuring `configureAgentTeams` has correct return type annotations and no unused imports; (4) `npm run build` — verify zero compilation errors; (5) `npm test` — run the full Vitest suite and confirm all tests pass including the new `agent-teams.test.ts`. If any step fails, fix the issue and re-run from that step. After all checks pass, log findings via `update_subtask` and set task 18 status to `done`.",
            "status": "done",
            "testStrategy": "All five quality gate commands must exit with code 0. Test suite must show 0 failures across all test files including the new agent-teams tests.",
            "parentId": "undefined",
            "updatedAt": "2026-02-18T07:49:32.452Z"
          }
        ],
        "updatedAt": "2026-02-18T07:49:32.452Z"
      },
      {
        "id": "19",
        "title": "Write Integration Test Suite",
        "description": "Create a comprehensive integration test suite that runs `ai-init` against a temporary directory, verifies all generated files exist with correct content, and cleans up. This dogfoods the project's own integration-first testing philosophy from F9.",
        "details": "Create `test/integration/full-run.test.ts`:\n\n```typescript\nimport { describe, it, expect, beforeEach, afterEach } from 'vitest';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport os from 'node:os';\nimport { execFile } from 'node:child_process';\nimport { promisify } from 'node:util';\n\nconst execFileAsync = promisify(execFile);\nconst CLI_PATH = path.resolve('./dist/cli.js');\n\nasync function runCli(\n  args: string[],\n  cwd: string,\n  env: Record<string, string> = {}\n): Promise<{ stdout: string; stderr: string }> {\n  return execFileAsync('node', [CLI_PATH, ...args], {\n    cwd,\n    env: { ...process.env, ...env },\n  });\n}\n\ndescribe('ai-init integration', () => {\n  let tempDir: string;\n\n  beforeEach(async () => {\n    tempDir = await fs.mkdtemp(path.join(os.tmpdir(), 'ai-init-test-'));\n  });\n\n  afterEach(async () => {\n    await fs.rm(tempDir, { recursive: true, force: true });\n  });\n\n  it('smoke: --non-interactive creates core files', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_MCPS: 'taskmaster',\n      SETUP_AI_TRACKER: 'taskmaster',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    // Verify core files\n    const coreFiles = ['.mcp.json', '.vscode/mcp.json', 'CLAUDE.md', 'CLAUDE_MCP.md'];\n    for (const file of coreFiles) {\n      await expect(fs.access(path.join(tempDir, file))).resolves.toBeUndefined();\n    }\n  });\n\n  it('demo: task master tracker config is accurate', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_TRACKER: 'taskmaster',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    const claudeMd = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n    expect(claudeMd).toContain('task-master next');\n    expect(claudeMd).toContain('@./.taskmaster/CLAUDE.md');\n  });\n\n  it('demo: beads tracker config is accurate', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_TRACKER: 'beads',\n      SETUP_AI_MCPS: 'beads',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    const claudeMd = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n    expect(claudeMd).toContain('bd sync');\n    expect(claudeMd).toContain('beads_ready');\n  });\n\n  it('demo: doc generation creates all template files', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    const docFiles = ['docs/doc_format.md', 'docs/prd.md', 'docs/architecture.md',\n      'docs/testing_strategy.md', 'docs/onboarding.md', 'docs/cuj.md'];\n    for (const file of docFiles) {\n      await expect(fs.access(path.join(tempDir, file))).resolves.toBeUndefined();\n    }\n  });\n\n  it('demo: rules generation includes testing.md with integration-first philosophy', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    const testingRule = await fs.readFile(\n      path.join(tempDir, '.claude/rules/testing.md'),\n      'utf8'\n    );\n    expect(testingRule).toContain('Integration tests');\n    expect(testingRule).toContain('Demo checkpoints');\n    expect(testingRule).toContain('demo:');\n  });\n\n  it('demo: idempotency — running twice produces same result', async () => {\n    const env = { SETUP_AI_NONINTERACTIVE: '1', SETUP_AI_SKIP_AUDIT: '1' };\n    await runCli([], tempDir, env);\n    const firstRun = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n    await runCli([], tempDir, env);\n    const secondRun = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n    expect(firstRun).toBe(secondRun);\n  });\n\n  it('smoke: mcp.json is valid JSON with correct schema', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_MCPS: 'taskmaster,context7',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    const mcpJson = await fs.readFile(path.join(tempDir, '.mcp.json'), 'utf8');\n    const parsed = JSON.parse(mcpJson);\n    expect(parsed).toHaveProperty('mcpServers');\n    expect(parsed.mcpServers).toHaveProperty('taskmaster-ai');\n    expect(parsed.mcpServers).toHaveProperty('context7');\n  });\n});\n```\n\nEach test:\n1. Creates fresh temp dir\n2. Runs CLI with env overrides\n3. Asserts specific outputs\n4. Cleans up in afterEach\n\nThis follows the demo-test naming pattern (`smoke:`, `demo:`) from the testing philosophy.",
        "testStrategy": "These tests ARE the test strategy — they are integration tests that prove the tool works end-to-end. Run with `npm test`. All tests must pass before any task is marked done. Verify test coverage includes at least: MCP generation, CLAUDE.md content, doc scaffolding, rules generation, and idempotency.",
        "priority": "high",
        "dependencies": [
          "16",
          "17"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create test/integration/full-run.test.ts with helper infrastructure",
            "description": "Set up the integration test file skeleton with shared helper functions, temp directory lifecycle hooks, and the runCli() wrapper that invokes dist/cli.js via execFile.",
            "dependencies": [],
            "details": "Create `test/integration/full-run.test.ts`. Import `describe`, `it`, `expect`, `beforeEach`, `afterEach` from vitest; import `fs` from `node:fs/promises`, `path` from `node:path`, `os` from `node:os`, `execFile` from `node:child_process`, `promisify` from `node:util`. Define `CLI_PATH = path.resolve('./dist/cli.js')`. Implement `runCli(args, cwd, env)` that calls `execFileAsync('node', [CLI_PATH, ...args], { cwd, env: { ...process.env, ...env } })` and returns `{ stdout, stderr }`. Add top-level `let tempDir: string` with `beforeEach` creating a temp dir via `fs.mkdtemp(path.join(os.tmpdir(), 'ai-init-test-'))` and `afterEach` cleaning it up via `fs.rm(tempDir, { recursive: true, force: true })`. Wrap all test cases in a single `describe('ai-init integration', ...)` block. Ensure the file is picked up by the existing vitest pattern `test/**/*.test.ts`.",
            "status": "done",
            "testStrategy": "Run `npm test` after creating the file skeleton — vitest should discover the file and pass (tests are stubs at this stage). Confirm `test/integration/full-run.test.ts` appears in the test run output.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:41:45.262Z"
          },
          {
            "id": 2,
            "title": "Add smoke test: non-interactive mode generates core files",
            "description": "Write the first test that runs the CLI in fully non-interactive mode and asserts that the four always-generated core files exist on disk in the temp directory.",
            "dependencies": [
              1
            ],
            "details": "Inside the `describe` block from subtask 1, add:\n```typescript\nit('smoke: --non-interactive creates core files', async () => {\n  await runCli([], tempDir, {\n    SETUP_AI_NONINTERACTIVE: '1',\n    SETUP_AI_MCPS: 'taskmaster',\n    SETUP_AI_TRACKER: 'taskmaster',\n    SETUP_AI_SKIP_AUDIT: '1',\n  });\n  const coreFiles = ['.mcp.json', '.vscode/mcp.json', 'CLAUDE.md', 'CLAUDE_MCP.md'];\n  for (const file of coreFiles) {\n    await expect(fs.access(path.join(tempDir, file))).resolves.toBeUndefined();\n  }\n});\n```\nThis test invokes the default wizard code path (no sub-command) with env overrides. `SETUP_AI_SKIP_AUDIT=1` prevents Claude Code audit from running. Requires `dist/cli.js` to exist (built beforehand). Ensure `npm run build` runs before the test suite to produce the dist artifact; update `package.json` test script to `\"tsc && vitest run\"` or document that `npm run build && npm test` is the required flow.",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. The smoke test should pass and all four core files must be found in the temp directory.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:41:50.589Z"
          },
          {
            "id": 3,
            "title": "Add MCP configuration tests (valid JSON schema, multiple servers)",
            "description": "Write tests that verify .mcp.json and .vscode/mcp.json are valid JSON, contain the expected `mcpServers` property, and include all requested MCP servers when SETUP_AI_MCPS lists multiple values.",
            "dependencies": [
              1
            ],
            "details": "Add two tests:\n1. `it('smoke: mcp.json is valid JSON with correct schema', ...)` — runs CLI with `SETUP_AI_MCPS: 'taskmaster,context7'`, reads `.mcp.json`, calls `JSON.parse()`, asserts `parsed.mcpServers` exists, `parsed.mcpServers['taskmaster-ai']` exists, and `parsed.mcpServers['context7']` exists.\n2. `it('smoke: .vscode/mcp.json mirrors .mcp.json structure', ...)` — reads `.vscode/mcp.json` and verifies it is also valid JSON with `mcpServers` property, confirming both output paths are written.\nFor both tests use env: `{ SETUP_AI_NONINTERACTIVE: '1', SETUP_AI_SKIP_AUDIT: '1', SETUP_AI_MCPS: 'taskmaster,context7' }`. Refer to `src/registry.ts` for canonical MCP server IDs (e.g., `taskmaster-ai`, `context7`).",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. Both MCP tests must pass; JSON.parse must not throw, and all required properties must be present.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:41:55.916Z"
          },
          {
            "id": 4,
            "title": "Add task tracker content tests (taskmaster and beads)",
            "description": "Write demo-labelled tests that verify the generated CLAUDE.md contains tracker-specific content depending on the SETUP_AI_TRACKER env variable.",
            "dependencies": [
              1
            ],
            "details": "Add two tests following the `demo:` naming convention:\n1. `it('demo: task master tracker config is accurate', ...)` — env `{ SETUP_AI_NONINTERACTIVE: '1', SETUP_AI_TRACKER: 'taskmaster', SETUP_AI_SKIP_AUDIT: '1' }`. Reads `CLAUDE.md` as utf8 and asserts it contains `'task-master next'` and `'@./.taskmaster/CLAUDE.md'`.\n2. `it('demo: beads tracker config is accurate', ...)` — env `{ SETUP_AI_NONINTERACTIVE: '1', SETUP_AI_TRACKER: 'beads', SETUP_AI_MCPS: 'beads', SETUP_AI_SKIP_AUDIT: '1' }`. Reads `CLAUDE.md` and asserts it contains `'bd sync'` and `'beads_ready'`.\nThese tests exercise the `src/generators/claude-md.ts` tracker-conditional logic through the full CLI stack.",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. Both tracker tests pass. Inspect actual CLAUDE.md output if assertions fail to identify the exact strings produced by the generators.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:42:01.595Z"
          },
          {
            "id": 5,
            "title": "Add document scaffolding test",
            "description": "Write a demo test that verifies all six standard documentation template files are created under docs/ when the CLI runs with default settings.",
            "dependencies": [
              1
            ],
            "details": "Add:\n```typescript\nit('demo: doc generation creates all template files', async () => {\n  await runCli([], tempDir, {\n    SETUP_AI_NONINTERACTIVE: '1',\n    SETUP_AI_SKIP_AUDIT: '1',\n  });\n  const docFiles = [\n    'docs/doc_format.md',\n    'docs/prd.md',\n    'docs/architecture.md',\n    'docs/testing_strategy.md',\n    'docs/onboarding.md',\n    'docs/cuj.md',\n  ];\n  for (const file of docFiles) {\n    await expect(fs.access(path.join(tempDir, file))).resolves.toBeUndefined();\n  }\n});\n```\nThe default config from `src/defaults.ts` enables `generateDocs: true`, so all docs should be generated. Verify the template files exist in `templates/docs/` to confirm the expected output file names match the generator logic in `src/generators/docs.ts`.",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. The doc generation test passes with all six doc paths accessible. Cross-check with `src/generators/docs.ts` if any doc file is missing.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:42:07.149Z"
          },
          {
            "id": 6,
            "title": "Add rules/skills/hooks generation test with content assertion",
            "description": "Write a demo test verifying that .claude/rules/testing.md is generated and contains the integration-first testing philosophy strings required by F9.",
            "dependencies": [
              1
            ],
            "details": "Add:\n```typescript\nit('demo: rules generation includes testing.md with integration-first philosophy', async () => {\n  await runCli([], tempDir, {\n    SETUP_AI_NONINTERACTIVE: '1',\n    SETUP_AI_SKIP_AUDIT: '1',\n  });\n  const testingRule = await fs.readFile(\n    path.join(tempDir, '.claude/rules/testing.md'),\n    'utf8'\n  );\n  expect(testingRule).toContain('Integration tests');\n  expect(testingRule).toContain('Demo checkpoints');\n  expect(testingRule).toContain('demo:');\n});\n```\nAlso add a companion assertion block for skills and hooks within the same test or as a second test `it('smoke: skills and hooks are generated', ...)` that checks `.claude/skills/testing.md` exists and `.claude/hooks/pre-commit.sh` exists. Verify against `templates/rules/testing.md` for expected string content.",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. Test passes. Read `templates/rules/testing.md` to confirm the strings `Integration tests`, `Demo checkpoints`, and `demo:` are present in the template source.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:42:12.491Z"
          },
          {
            "id": 7,
            "title": "Add idempotency test (running CLI twice yields same output)",
            "description": "Write a demo test that runs the CLI twice against the same temp directory and asserts that CLAUDE.md content is byte-for-byte identical across both runs.",
            "dependencies": [
              1
            ],
            "details": "Add:\n```typescript\nit('demo: idempotency — running twice produces same result', async () => {\n  const env = { SETUP_AI_NONINTERACTIVE: '1', SETUP_AI_SKIP_AUDIT: '1' };\n  await runCli([], tempDir, env);\n  const firstRun = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n  await runCli([], tempDir, env);\n  const secondRun = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n  expect(firstRun).toBe(secondRun);\n});\n```\nThis test validates that the CLI's default `overwrite: true` behaviour produces stable, deterministic output. Also optionally read `.mcp.json` and assert equality across both runs to cover JSON generation idempotency. The test relies on `runPostCreate`'s file-writing logic not appending or generating non-deterministic content (e.g., timestamps).",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. Test passes. If the test reveals non-deterministic output (e.g., a timestamp field), that is a bug in the generator that must be fixed before the test can be marked done.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:42:17.945Z"
          },
          {
            "id": 8,
            "title": "Add CLI lifecycle sub-command tests (post-create, post-start)",
            "description": "Write smoke tests for the `post-create` and `post-start` CLI sub-commands to verify each lifecycle command runs without error and produces expected side effects.",
            "dependencies": [
              1
            ],
            "details": "Add two tests:\n1. `it('smoke: post-create command generates core files', ...)` — calls `runCli(['post-create'], tempDir, { SETUP_AI_SKIP_AUDIT: '1' })` and asserts `.mcp.json` and `CLAUDE.md` exist.\n2. `it('smoke: post-start command runs without error', ...)` — calls `runCli(['post-start'], tempDir, {})` and asserts the promise resolves (no throw). Optionally reads `.env` to confirm env-sync ran if `ANTHROPIC_API_KEY` is set in the test environment.\nFor the `post-create` command, note from `src/cli.ts:62` that it forces `SETUP_AI_NONINTERACTIVE=1` internally, so no env override is needed for non-interactive mode — only `SETUP_AI_SKIP_AUDIT=1` is required. For `post-start`, it uses `defaultConfig(projectRoot)` directly without a wizard, so it always succeeds even with an empty directory.",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. Both lifecycle tests pass. Verify the full test suite (all tests in `test/integration/full-run.test.ts`) passes with `npm test` and that no existing tests in `test/integration/phases.test.ts` are broken.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:42:23.203Z"
          }
        ],
        "updatedAt": "2026-02-17T22:42:23.203Z"
      },
      {
        "id": "20",
        "title": "Configure Linting, Formatting, and Build Pipeline",
        "description": "Set up ESLint, Prettier, and the TypeScript build pipeline. Ensure `npm run lint`, `npm run format`, `npm run typecheck`, and `npm run build` all work correctly. This is the quality gate infrastructure that all other tasks depend on.",
        "details": "Create `.eslintrc.cjs`:\n```javascript\nmodule.exports = {\n  root: true,\n  parser: '@typescript-eslint/parser',\n  plugins: ['@typescript-eslint'],\n  extends: [\n    'eslint:recommended',\n    'plugin:@typescript-eslint/recommended',\n  ],\n  env: { node: true, es2022: true },\n  rules: {\n    '@typescript-eslint/no-unused-vars': ['error', { argsIgnorePattern: '^_' }],\n    '@typescript-eslint/explicit-module-boundary-types': 'off',\n    'no-console': 'off',\n  },\n};\n```\n\nCreate `.prettierrc`:\n```json\n{\n  \"semi\": true,\n  \"singleQuote\": true,\n  \"trailingComma\": \"es5\",\n  \"printWidth\": 100,\n  \"tabWidth\": 2\n}\n```\n\nCreate `.prettierignore`:\n```\ndist/\nnode_modules/\ntemplates/\n```\n\nUpdate `package.json` scripts:\n```json\n{\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"dev\": \"tsx src/cli.ts\",\n    \"test\": \"vitest run\",\n    \"test:watch\": \"vitest\",\n    \"lint\": \"eslint src test --ext .ts --fix\",\n    \"lint:check\": \"eslint src test --ext .ts\",\n    \"typecheck\": \"tsc --noEmit\",\n    \"format\": \"prettier --write 'src/**/*.ts' 'test/**/*.ts'\",\n    \"format:check\": \"prettier --check 'src/**/*.ts' 'test/**/*.ts'\"\n  }\n}\n```\n\nUpdate `.gitignore` to include:\n```\ndist/\nnode_modules/\n.ai-init-audit.md\n*.js.map\n```\n\nVerify the build pipeline: `npm run build` produces `dist/cli.js` with shebang, `npm run lint` passes on all source files, `npm run typecheck` shows zero errors.",
        "testStrategy": "Verify all pipeline commands exit with code 0 on a clean checkout: `npm run format:check`, `npm run lint:check`, `npm run typecheck`, `npm run build`. The CI gate: run all four commands sequentially and fail if any returns non-zero. Also verify that `dist/cli.js` exists after build and the shebang `#!/usr/bin/env node` is present on line 1.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create ESLint configuration file (.eslintrc.cjs)",
            "description": "Create the .eslintrc.cjs file in the project root with TypeScript-aware ESLint rules using @typescript-eslint parser and plugin.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/.eslintrc.cjs` with: `root: true`, parser set to `@typescript-eslint/parser`, plugins array with `@typescript-eslint`, extends from `eslint:recommended` and `plugin:@typescript-eslint/recommended`, env set to `{ node: true, es2022: true }`, and rules: `@typescript-eslint/no-unused-vars` as error with `argsIgnorePattern: '^_'`, `@typescript-eslint/explicit-module-boundary-types` off, `no-console` off. Use `module.exports = { ... }` CommonJS syntax (required because the file uses .cjs extension in an ESM project with `\"type\": \"module\"` in package.json).",
            "status": "done",
            "testStrategy": "Run `npx eslint src --ext .ts` and verify it exits with code 0 and correctly identifies any intentional lint errors introduced as a test.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:54:06.833Z"
          },
          {
            "id": 2,
            "title": "Create Prettier configuration files (.prettierrc and .prettierignore)",
            "description": "Create .prettierrc with formatting rules and .prettierignore to exclude generated/dependency directories from formatting.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/.prettierrc` as JSON with: `semi: true`, `singleQuote: true`, `trailingComma: 'es5'`, `printWidth: 100`, `tabWidth: 2`. Create `/workspaces/ai-dev-setup/.prettierignore` with entries: `dist/`, `node_modules/`, `templates/`. These files ensure consistent code style across the project and prevent Prettier from reformatting generated output files or template files that must remain in their original form.",
            "status": "done",
            "testStrategy": "Run `npx prettier --check 'src/**/*.ts'` and verify it exits cleanly, confirming the config is valid and recognized by Prettier.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:54:36.439Z"
          },
          {
            "id": 3,
            "title": "Update package.json scripts for full quality pipeline",
            "description": "Replace/augment the scripts section in package.json to add lint:check, format:check, and update existing lint/format scripts to match the required pipeline commands.",
            "dependencies": [
              1,
              2
            ],
            "details": "Edit `/workspaces/ai-dev-setup/package.json` scripts section. The current scripts are missing `lint:check` and `format:check` variants needed for CI. Update `lint` to `eslint src test --ext .ts --fix`, add `lint:check` as `eslint src test --ext .ts`, update `format` to `prettier --write 'src/**/*.ts' 'test/**/*.ts'`, add `format:check` as `prettier --check 'src/**/*.ts' 'test/**/*.ts'`. Keep existing `build`, `dev`, `test`, `test:watch`, and `typecheck` scripts unchanged. The `test` directory may not exist yet — the lint scripts should not fail on missing directories (already handled by the current `--no-error-on-unmatched-pattern` flag, which should be preserved or the `test` directory created).",
            "status": "done",
            "testStrategy": "Run `npm run lint:check` and `npm run format:check` and verify both exit with code 0. Run `npm run lint` and `npm run format` and verify they complete without errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:55:19.606Z"
          },
          {
            "id": 4,
            "title": "Update .gitignore with missing entries",
            "description": "Add .ai-init-audit.md and *.js.map to .gitignore since the current file already has dist/ and node_modules/ but is missing these entries.",
            "dependencies": [],
            "details": "Edit `/workspaces/ai-dev-setup/.gitignore` to add `.ai-init-audit.md` (audit files generated by the tool should not be committed to the template repo) and `*.js.map` (source map files produced by `tsc` when `sourceMap` is enabled). The current .gitignore already correctly includes `dist/`, `node_modules/`, `.env`, and `*.tsbuildinfo`. Add these two entries under the Build output section alongside `*.tsbuildinfo`. Do not add `.vscode` — it is already present in the editor section.",
            "status": "done",
            "testStrategy": "Run `git status` after adding an `.ai-init-audit.md` file and verify it does not appear as an untracked file. Verify `git check-ignore -v .ai-init-audit.md` reports a match.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:55:42.534Z"
          },
          {
            "id": 5,
            "title": "Verify full build pipeline executes successfully end-to-end",
            "description": "Run all four quality gate commands sequentially and confirm each exits with code 0: npm run format:check, npm run lint:check, npm run typecheck, npm run build. Verify dist/cli.js is produced with a shebang line.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Execute the complete pipeline in order: (1) `npm run format:check` — verifies all src TypeScript files match Prettier config; (2) `npm run lint:check` — verifies ESLint passes on all src files; (3) `npm run typecheck` — runs `tsc --noEmit` and verifies zero TypeScript errors; (4) `npm run build` — runs `tsc` and produces output in `dist/`. After build, verify `dist/cli.js` exists and its first line is `#!/usr/bin/env node` (shebang). If any step fails, diagnose and fix: common issues include unused imports caught by ESLint, formatting mismatches in existing src files (run `npm run format` to fix), or TypeScript strict-mode errors in existing src files. Fix all issues in the source files to make the pipeline green.",
            "status": "done",
            "testStrategy": "Run each command and check exit codes: `npm run format:check && npm run lint:check && npm run typecheck && npm run build`. Then run `head -1 dist/cli.js` and confirm output is `#!/usr/bin/env node`. The entire sequence must complete with exit code 0.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:56:37.998Z"
          }
        ],
        "updatedAt": "2026-02-17T20:56:37.998Z"
      },
      {
        "id": "21",
        "title": "Write Unit Tests for All Generators",
        "description": "Create comprehensive unit tests for every generator function. Tests use pure function input/output — no temp directories, no mocks, just assert on returned FileDescriptor content. Dogfoods the project's integration-first testing philosophy.",
        "details": "Create the following test files:\n\n**`test/generators/mcp-json.test.ts`** (per Task 7 test strategy)\n\n**`test/generators/claude-md.test.ts`** (per Task 8 test strategy)\n\n**`test/generators/docs.test.ts`** (per Task 9 test strategy)\n\n**`test/generators/rules.test.ts`** (per Task 10 test strategy)\n\n**`test/generators/hooks.test.ts`** (per Task 10 test strategy)\n\n**`test/generators/devcontainer.test.ts`** (per Task 11 test strategy)\n\n**`test/generators/commands.test.ts`** (per Task 12 test strategy)\n\nEach test file follows this pattern:\n```typescript\nimport { describe, it, expect } from 'vitest';\nimport { generateMcpJson } from '../../src/generators/mcp-json.js';\nimport { defaultConfig } from '../../src/defaults.js';\n\ndescribe('generateMcpJson', () => {\n  it('demo: taskmaster config uses correct root key for Claude Code', () => {\n    const config = { ...defaultConfig('/tmp/test'), selectedMcps: ['taskmaster'] };\n    const files = generateMcpJson(config);\n    const mcpFile = files.find(f => f.path === '.mcp.json')!;\n    const parsed = JSON.parse(mcpFile.content);\n    expect(parsed).toHaveProperty('mcpServers');\n    expect(parsed.mcpServers).toHaveProperty('taskmaster-ai');\n  });\n\n  it('demo: vscode config uses correct root key for VS Code', () => {\n    const config = { ...defaultConfig('/tmp/test'), selectedMcps: ['taskmaster'] };\n    const files = generateMcpJson(config);\n    const vsFile = files.find(f => f.path === '.vscode/mcp.json')!;\n    const parsed = JSON.parse(vsFile.content);\n    expect(parsed).toHaveProperty('servers');\n    expect(parsed.servers['taskmaster-ai']).toHaveProperty('cwd');\n  });\n});\n```\n\nKey requirement: Every test uses the `smoke:` or `demo:` naming convention and tests real behavior (JSON parsing, content matching, file counts) — no trivial assertions like `expect(true).toBe(true)`.\n\nTotal test count target: ≥ 40 unit tests across all generators.",
        "testStrategy": "Run `npm test` — all 40+ unit tests must pass. Run `npm run typecheck` — zero type errors in test files. Verify test output shows individual test names matching `smoke:` or `demo:` naming convention. Coverage report (if configured) should show > 80% branch coverage on generator files.",
        "priority": "high",
        "dependencies": [
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "19",
          "20"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Write unit tests for mcp-json and devcontainer generators",
            "description": "Create test/generators/mcp-json.test.ts and test/generators/devcontainer.test.ts with pure function input/output tests covering all config permutations.",
            "dependencies": [],
            "details": "For mcp-json: test that .mcp.json uses 'mcpServers' root key, .vscode/mcp.json uses 'servers' root key with 'cwd' field, all 5 registry servers produce correct entries, empty selectedMcps produces empty objects, VS Code env vars use '${env:VAR}' syntax. For devcontainer: test that output is exactly 1 FileDescriptor at '.devcontainer/devcontainer.json', parsed JSON has onCreateCommand 'ai-init on-create', postCreateCommand 'ai-init post-create', postStartCommand 'ai-init post-start', ANTHROPIC_API_KEY secret always present, MCP-derived secrets filtered to _KEY/_SECRET/_TOKEN suffixes, no duplicate secrets. Use makeConfig() helper for override merging. All test names use 'demo:' or 'smoke:' prefix.",
            "status": "done",
            "testStrategy": "Run npm test -- test/generators/mcp-json.test.ts test/generators/devcontainer.test.ts; verify all tests pass and names follow smoke:/demo: convention.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Write unit tests for claude-md and docs generators",
            "description": "Create test/generators/claude-md.test.ts and test/generators/docs.test.ts covering conditional content generation based on taskTracker, generateDocs, hasApiDocs, and architecture flags.",
            "dependencies": [
              1
            ],
            "details": "For claude-md: test taskmaster variant includes '@./.taskmaster/CLAUDE.md' import and 'task-master next'; beads variant includes 'beads_ready' and 'bd sync'; markdown variant includes 'TASKS.md'; no MCPs → 1 file output; with MCPs → CLAUDE_MCP.md generated; quality gate section always present. For docs: test base file count is 7 (doc_format, prd, architecture, cuj, testing_strategy, onboarding, adr_template); +1 if hasApiDocs; +1 if taskTracker==='markdown' (TASKS.md); PROJECT_NAME placeholder substituted correctly; ARCHITECTURE placeholder substituted; DATE placeholder present in some files. All tests use pure function calls with no temp directories.",
            "status": "done",
            "testStrategy": "Run npm test on both files; verify file count assertions, content markers, and placeholder substitution all pass.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Write unit tests for rules and skills generators",
            "description": "Create test/generators/rules.test.ts and test/generators/skills.test.ts covering conditional rule generation and always-on skills output.",
            "dependencies": [
              1
            ],
            "details": "For rules: test base count is 6 (general, docs, testing, git, security, config); +1 if hasApiDocs (api.md); api.md content includes '@docs/api.md' import when generateDocs=true; +1 if hasDatabase (database.md); +1 if agentTeamsEnabled (agent-teams.md); PROJECT_NAME and TASK_TRACKER placeholders substituted; no leftover '{{...}}' markers. For skills: test output is always exactly 3 files (testing.md, commit.md, task-workflow.md); each has YAML frontmatter with description field; content includes Integration-First section; TASK_TRACKER placeholder substituted with actual tracker value. Use makeConfig() with flag overrides. All test names use 'demo:' or 'smoke:' prefix.",
            "status": "done",
            "testStrategy": "Run npm test on rules.test.ts and skills.test.ts; confirm conditional file counts, YAML frontmatter validity, and no unresolved placeholders.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Write unit tests for hooks and commands generators",
            "description": "Create test/generators/hooks.test.ts and test/generators/commands.test.ts covering the pre-commit shell script, settings.json hooks config, and tracker-specific command files.",
            "dependencies": [
              1
            ],
            "details": "For hooks: test output is exactly 2 files; pre-commit.sh has executable flag set to true; content starts with '#!/usr/bin/env bash' shebang; contains 'set -euo pipefail'; contains ordered steps for format, lint, typecheck, build, test using npm run --if-present; settings.json is valid JSON; settings.json hooks section has PreToolUse matcher for 'Bash(git commit)' pointing to pre-commit.sh path. For commands: test output is always exactly 3 files (dev-next.md, review.md, boot-prompt.txt); taskmaster variant has 'task-master next' in dev-next.md; beads variant has 'bd show' in dev-next.md; markdown variant has 'TASKS.md' reference; review.md contains quality gate references; PROJECT_NAME substituted; no unresolved '{{...}}' placeholders remain.",
            "status": "done",
            "testStrategy": "Run npm test on hooks.test.ts and commands.test.ts; verify executable flag, bash syntax markers, JSON validity, tracker-specific content, and placeholder resolution.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Verify total test count meets ≥40 target and all tests pass",
            "description": "Run the complete test suite, confirm all generator unit tests pass, count meets the ≥40 threshold, naming convention is followed, and typecheck reports zero errors.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Run 'npm test' and verify: all test files in test/generators/ pass with zero failures; total unit tests across mcp-json (target ≥6), claude-md (target ≥6), docs (target ≥6), rules (target ≥6), hooks (target ≥5), devcontainer (target ≥5), commands (target ≥6) sum to at least 40; test names consistently use 'smoke:' or 'demo:' prefix patterns; no trivial assertions like expect(true).toBe(true). Run 'npm run typecheck' and confirm zero TypeScript errors in test files. If any tests fail or type errors exist, fix them before marking task done. Log final test counts and typecheck result in subtask update.",
            "status": "done",
            "testStrategy": "npm test output shows all generator tests passing; npm run typecheck exits 0; grep for 'smoke:' and 'demo:' in test files confirms naming convention across all 7 required test files.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-02-17T22:57:25.352Z"
      },
      {
        "id": "22",
        "title": "Add README and Usage Documentation",
        "description": "Update `README.md` with installation instructions, feature overview, environment variable reference, and examples. The README is the user's first touchpoint — it must make the single-line install immediately obvious.",
        "details": "Update `README.md` with:\n\n**Header section:**\n- One-line description\n- Badges (license, node version)\n- Single-line install command prominently displayed:\n  ```bash\n  curl -fsSL https://raw.githubusercontent.com/potgieterdl/ai-helper-tools/main/install.sh | bash\n  ```\n\n**Usage section:**\n```bash\ncd my-project\nai-init                    # Interactive wizard\nai-init --non-interactive  # Env-var driven\nai-init on-create          # Codespace: heavy installs\nai-init post-create        # Codespace: project scaffolding\nai-init post-start         # Codespace: per-session setup\n```\n\n**What gets generated table:**\n| File/Directory | Purpose |\n|---|---|\n| `CLAUDE.md` | Agent instructions (auto-loaded by Claude Code) |\n| `.mcp.json` | MCP servers for Claude Code CLI |\n| `.vscode/mcp.json` | MCP servers for VS Code / Copilot |\n| `docs/` | Agent-optimized project documentation |\n| `.claude/rules/` | Path-scoped agent rules |\n| `.claude/skills/` | Keyword-activated agent knowledge |\n| `.claude/hooks/` | Pre-commit quality gate |\n| `.claude/commands/` | `/dev-next` and `/review` slash commands |\n| `.devcontainer/devcontainer.json` | Codespace lifecycle hooks |\n\n**Environment variables table** (all SETUP_AI_* vars)\n\n**Task tracker comparison table** (Task Master vs Beads vs Markdown)\n\n**Development section:**\n```bash\ngit clone ...\nnpm ci\nnpm run build\nnpm test\nnpm run dev -- --help\n```\n\nKeep README under 200 lines following the project's own doc_format.md standard.",
        "testStrategy": "Verify README.md contains: install curl command, all subcommand examples, the generated files table, environment variable reference. Verify README.md is under 200 lines. Verify all links in README are relative and valid (no broken anchors). Run `markdownlint README.md` if available.",
        "priority": "medium",
        "dependencies": [
          "17",
          "19"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Verify README content matches all task requirements",
            "description": "Audit the existing README.md against every requirement in task 22: header description, badges, install command, usage section with all subcommands, what-gets-generated table, environment variables table, task tracker comparison table, and development section. Confirm line count is under 200.",
            "dependencies": [],
            "details": "Read /workspaces/ai-dev-setup/README.md in full. Check each required section against the task specification:\n1. One-line description in header\n2. Badges (license, node version) — note: current README is missing badges; add them if absent\n3. Single-line curl install command prominently in Quick Install section\n4. Usage section covers: `ai-init`, `ai-init --non-interactive`, `ai-init on-create`, `ai-init post-create`, `ai-init post-start`\n5. What Gets Generated table lists all 9 entries: CLAUDE.md, .mcp.json, .vscode/mcp.json, docs/, .claude/rules/, .claude/skills/, .claude/hooks/, .claude/commands/, .devcontainer/devcontainer.json\n6. Environment variables table covers all SETUP_AI_* vars\n7. Task tracker comparison table (Task Master vs Beads vs Markdown)\n8. Development section with git clone, npm ci, npm run build, npm test, npm run dev -- --help\n9. Total line count ≤ 200\n\nIf any section is missing or incomplete, add/fix it. If badges are absent, add shield.io badges for license and node version requirements.",
            "status": "pending",
            "testStrategy": "Manually verify each section is present. Run `wc -l README.md` to confirm line count ≤ 200. Grep for the curl install command, each subcommand, and each table header.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add missing badges to README header",
            "description": "The README.md currently lacks license and Node.js version badges. Add shield.io badges to the header section to satisfy the task requirement for badges.",
            "dependencies": [
              1
            ],
            "details": "After the project title and one-line description in README.md, add markdown badge images using shields.io:\n\n```markdown\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n![Node.js: >=20](https://img.shields.io/badge/node-%3E%3D20-brightgreen)\n```\n\nPlace badges between the title/tagline and the Table of Contents. Verify the badge URLs are standard shields.io static badges (no external API calls needed at render time). Keep the total line count under 200 after adding badges. Check package.json for the actual license field to ensure the badge label matches.",
            "status": "pending",
            "testStrategy": "Confirm both badge lines appear in README.md before the Table of Contents. Confirm total line count remains ≤ 200. Check that badge markdown syntax is valid (no broken image syntax).",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Run format, lint, and typecheck quality gate",
            "description": "Execute the project's formatter, linter, and TypeScript type-checker to ensure zero errors before marking the task done.",
            "dependencies": [
              2
            ],
            "details": "Run the following commands in sequence from /workspaces/ai-dev-setup, fixing any issues found:\n\n1. `npm run format` — Prettier formats all files including README.md if configured\n2. `npm run lint` — ESLint with auto-fix; fix any reported errors (warnings acceptable)\n3. `npm run typecheck` — `tsc --noEmit`; fix any type errors\n\nFor each command:\n- If it exits 0, proceed to next step\n- If it exits non-zero, read the error output, fix the issues, and re-run the command\n\nNote: Prettier typically does not format .md files unless configured to do so; if format does not touch README.md, that is acceptable. The primary goal is ensuring the JS/TS codebase is clean.",
            "status": "pending",
            "testStrategy": "All three commands must exit with code 0. No unfixed ESLint errors. No TypeScript compilation errors.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Run build and full test suite",
            "description": "Build the project and run the complete test suite to verify the implementation is solid before closing the task.",
            "dependencies": [
              3
            ],
            "details": "Run from /workspaces/ai-dev-setup:\n\n1. `npm run build` — compile TypeScript to dist/; verify exit code 0 and no errors in stdout/stderr\n2. `npm test` — run the full Vitest suite (392+ tests); verify all tests pass\n\nIf build fails:\n- Read the compiler errors\n- Fix the TypeScript issues\n- Re-run `npm run build`\n\nIf tests fail:\n- Identify which test file and test name failed\n- Read the failing test and the source it tests\n- Fix the implementation or test as appropriate\n- Re-run `npm test`\n\nDo not mark this subtask done until both commands exit 0 with no failures.",
            "status": "pending",
            "testStrategy": "Build exit code 0. All Vitest tests pass (0 failures). Test summary output shows passing count matches expected 392+ tests.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Log completion findings and mark task 22 done",
            "description": "Record the verification results in the task tracker and transition task 22 to done status.",
            "dependencies": [
              4
            ],
            "details": "Using the Task Master MCP or CLI:\n\n1. Call `update_subtask` on task 22 (or the appropriate subtask) to log:\n   - Confirmation that README.md matches all task requirements\n   - Which sections were already present vs what was added (badges)\n   - Line count of final README.md\n   - Results of format, lint, typecheck, build, and test runs\n   - Any issues encountered and how they were resolved\n\n2. Call `set_task_status --id=22 --status=done` to mark the parent task complete\n\nCommand sequence:\n```bash\ntask-master update-subtask --id=22 --prompt=\"README verified: all sections present including badges. Format/lint/typecheck/build/test all passed. Final line count: N lines.\"\ntask-master set-status --id=22 --status=done\n```",
            "status": "pending",
            "testStrategy": "Task 22 status shows 'done' in `task-master list` output. The subtask log contains a summary of all checks performed.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-02-18T08:07:05.947Z"
      },
      {
        "id": "23",
        "title": "Verify if Bash Logic conceptually the same",
        "description": "Ensure the TypeScript CLI conceptually matches what we had in the existing `setup-ai.sh` bash script. We have built a new unitility so it wont match the exact funcionality and this ok.",
        "details": "Audit `setup-ai.sh` against the TypeScript implementation:\n\n**Bash → TypeScript function mapping:**\n| Bash function | TypeScript equivalent |\n|---|---|\n| `select_mcps()` | `wizard.ts` Step 1 + env var handling |\n| `write_mcp_json()` | `generators/mcp-json.ts:generateMcpJson()` |\n| `write_claude_md()` | `generators/claude-md.ts:generateClaudeMd()` |\n| `write_devcontainer_json()` | `generators/devcontainer.ts:generateDevcontainer()` |\n| `phase_on_create()` | `phases/on-create.ts:runOnCreate()` |\n| `phase_post_create()` | `phases/post-create.ts:runPostCreate()` |\n| `phase_post_start()` | `phases/post-start.ts:runPostStart()` |\n| Interactive MCP menu | `wizard.ts` `@inquirer/prompts` checkbox |\n| `.setup-ai-mcps` cache file | Read in `post-start.ts` for session context |\n| Welcome banner + task count | `phases/post-start.ts:runPostStart()` |\n| Shell config injection (TMPDIR) | `phases/on-create.ts` |\n| Pre-commit hook generation | `generators/hooks.ts:generateHooks()` |\n\n**Compatibility verification:**\n1. Existing `.devcontainer/devcontainer.json` calls `setup-ai.sh on-create|post-create|post-start` — update these to `ai-init on-create|post-create|post-start`\n2. Existing `.setup-ai-mcps` file (contains `taskmaster`) — `post-start.ts` should read this for context\n3. CLAUDE.md regeneration — the `<!-- SETUP-AI-MANAGED -->` header is preserved\n4. MCP config parity — verify the TypeScript-generated `.mcp.json` matches the bash-generated one for identical inputs\n\n**Transition path:** \n- Keep `setup-ai.sh` intact during development (it's the reference implementation)\n- Add a deprecation notice to `setup-ai.sh` pointing to `ai-init`\n- Update `.devcontainer/devcontainer.json` to use `ai-init` once tests pass",
        "testStrategy": "Perform end to end testing of the new binary",
        "priority": "medium",
        "dependencies": [
          "16",
          "17",
          "19"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit Bash-to-TypeScript Function Mapping",
            "description": "Systematically compare each bash function in setup-ai.sh against its TypeScript equivalent, documenting gaps and conceptual differences.",
            "dependencies": [],
            "details": "Read setup-ai.sh in full and map each function to its TypeScript equivalent using the provided mapping table. For each pair, verify: (1) select_mcps() vs wizard.ts Step 1 — both support env var SETUP_AI_MCPS, non-interactive fallback, and always-required MCP inclusion; (2) write_mcp_json() vs generators/mcp-json.ts — both produce .mcp.json and .vscode/mcp.json with correct server configs; (3) write_claude_md() vs generators/claude-md.ts — both emit the <!-- SETUP-AI-MANAGED --> header; (4) write_devcontainer_json() vs generators/devcontainer.ts — both synthesize secrets from selected MCP env vars; (5) phase_on_create() vs phases/on-create.ts — both install claude-code and task-master-ai globally; (6) phase_post_create() vs phases/post-create.ts — both call generators then optionally audit; (7) phase_post_start() vs phases/post-start.ts — both sync .env and print welcome banner. Document each gap as either 'acceptable divergence' (new TS has richer functionality) or 'missing behaviour' (bash does something TS does not). Write findings as a markdown checklist in docs/bash-ts-audit.md (or update_subtask notes).",
            "status": "pending",
            "testStrategy": "Manual review with side-by-side diff of bash and TypeScript logic; confirm all 'missing behaviour' items are either implemented or explicitly out-of-scope.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Verify MCP Config Parity Between Bash and TypeScript Output",
            "description": "Run both setup-ai.sh and ai-init with identical MCP selections and diff the resulting .mcp.json files to confirm conceptual equivalence.",
            "dependencies": [
              1
            ],
            "details": "Create a test script or use the integration test harness to: (1) Invoke bash script in a temp directory with SETUP_AI_MCPS=taskmaster,context7 SETUP_AI_NONINTERACTIVE=1 and capture the generated .mcp.json; (2) Invoke ai-init (or run generateMcpJson() directly in a unit test) with the same selections and capture its .mcp.json; (3) Diff the two outputs focusing on: server names (mcpServers keys), command/args/env structure, env var placeholder syntax (${VAR} vs ${localEnv:VAR} etc.), and presence of all selected servers. Note any intentional differences (e.g., TS adds .vscode/mcp.json which bash also does). Document discrepancies. Fix any unintentional gaps in src/generators/mcp-json.ts. Also verify that the existing project's .mcp.json (generated by bash previously) is compatible with what the TS generator would produce for the same inputs.",
            "status": "pending",
            "testStrategy": "Add or update test/generators/mcp-json.test.ts with a parity test that asserts generated output matches known-good bash-generated fixture. Run npm test and confirm it passes.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Update devcontainer.json Lifecycle Commands to Use ai-init",
            "description": "Replace the three bash setup-ai.sh lifecycle calls in .devcontainer/devcontainer.json with equivalent ai-init commands.",
            "dependencies": [
              1,
              2
            ],
            "details": "Edit .devcontainer/devcontainer.json to change: (1) onCreateCommand from 'bash setup-ai.sh on-create' to 'ai-init on-create'; (2) postCreateCommand from 'bash setup-ai.sh post-create' to 'ai-init post-create'; (3) postStartCommand from 'bash setup-ai.sh post-start' to 'ai-init post-start'. Before making changes, confirm that ai-init is installed globally (added to PATH via install.sh symlink). Add a fallback for environments where ai-init may not yet be installed: consider using 'npx --yes . on-create' or keeping a conditional in the command. Verify that the generated devcontainer.json from generateDevcontainer() also uses ai-init commands (not setup-ai.sh) so newly created projects get the correct commands. Update src/generators/devcontainer.ts if it still references setup-ai.sh in its output template. Run the devcontainer generator test to confirm.",
            "status": "pending",
            "testStrategy": "Run test/generators/devcontainer.test.ts and confirm lifecycle commands reference ai-init. Manually inspect updated .devcontainer/devcontainer.json to confirm three commands are updated. Run npm test to ensure no regressions.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add Deprecation Notice to setup-ai.sh",
            "description": "Add a clear deprecation notice at the top of setup-ai.sh directing users to ai-init, while keeping the script fully functional as a reference implementation.",
            "dependencies": [
              3
            ],
            "details": "Edit setup-ai.sh to add a deprecation block near the top (after the shebang and license header but before any execution logic). The notice should: (1) Print a visible warning to stderr when the script is invoked: 'WARNING: setup-ai.sh is deprecated. Please use ai-init instead. See README for migration guide.'; (2) Include a comment block explaining that this file is kept as reference implementation and should not be modified; (3) Point to the TypeScript equivalent location (src/cli.ts) for developers who need to understand the logic; (4) Add a DEPRECATED marker at line 1 as a comment for grep discoverability. The warning should only display if the script is run directly (not when sourced), detected via [ \"${BASH_SOURCE[0]}\" = \"$0\" ]. Keep all existing functionality intact — do not remove or alter any bash functions.",
            "status": "pending",
            "testStrategy": "Run 'bash setup-ai.sh --help' or any valid command and verify the deprecation warning appears on stderr. Confirm the script still functions correctly for all three phases. Check that DEPRECATED comment is visible via grep.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write Integration Test for Bash Parity Verification",
            "description": "Create or update an integration test that validates the TypeScript CLI produces conceptually equivalent output to setup-ai.sh for the core use case.",
            "dependencies": [
              2,
              3
            ],
            "details": "Create test/integration/bash-parity.test.ts (or update test/integration/full-run.test.ts) with the following test cases: (1) 'MCP config parity' — run generateMcpJson() with taskmaster selected, assert the output .mcp.json structure matches the known-good bash-generated fixture in test/fixtures/bash-mcp.json (create this fixture from the current bash-generated .mcp.json); (2) 'CLAUDE.md header preservation' — run generateClaudeMd() and assert the <!-- SETUP-AI-MANAGED --> header is present; (3) 'Devcontainer lifecycle commands' — run generateDevcontainer() and assert onCreateCommand/postCreateCommand/postStartCommand reference ai-init, not setup-ai.sh; (4) 'Phase sequence equivalence' — in a temp directory, run ai-init post-create in non-interactive mode (SETUP_AI_NONINTERACTIVE=1) and assert .mcp.json, CLAUDE.md, and .devcontainer/devcontainer.json are all created; (5) 'Post-start .env sync' — mock Codespace env vars and verify post-start.ts writes correct .env content. Use beforeEach/afterEach to clean up temp directories. All tests should use Vitest and follow the existing test patterns in the codebase.",
            "status": "pending",
            "testStrategy": "Run npm test and confirm all new parity tests pass. Run npm run typecheck to verify no TypeScript errors. Run npm run lint to verify no linting issues.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create ADR Documenting the Bash-to-TypeScript Migration Decision",
            "description": "Write an Architecture Decision Record (ADR) in the docs directory documenting the rationale, trade-offs, and transition path for migrating from setup-ai.sh to the TypeScript CLI.",
            "dependencies": [
              4,
              5
            ],
            "details": "Create docs/adr/001-bash-to-typescript-migration.md using the project's ADR template (templates/docs/adr_template.md). The ADR should cover: (1) Context — why setup-ai.sh existed (single-file Codespaces bootstrap, zero dependencies) and its limitations (hard to test, no type safety, template string management); (2) Decision — build TypeScript CLI as the primary implementation with bash as reference; (3) Consequences — positive (testable, type-safe, generator pattern, pure functions) and negative (requires Node.js build step, install.sh needed); (4) Alternatives Considered — keep bash-only, use Python, use Go; (5) Migration Path — keep setup-ai.sh as reference with deprecation notice, update devcontainer.json to use ai-init, provide install.sh for global installation; (6) Compatibility Notes — document the .setup-ai-mcps file format, <!-- SETUP-AI-MANAGED --> header, and other contracts that both implementations share; (7) Function mapping table (copy from task details). File should follow the doc_format.md standard (TLDR, TOC, sections < 30 lines, tables over prose).",
            "status": "pending",
            "testStrategy": "Verify the ADR file exists at docs/adr/001-bash-to-typescript-migration.md. Confirm it contains the required sections (Context, Decision, Consequences, Migration Path). Run npm test to ensure no regressions from file creation.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-02-18T08:16:49.843Z"
      },
      {
        "id": "24",
        "title": "Remove Boot Prompt Generation (F12)",
        "description": "Remove the `.claude/boot-prompt.txt` generation from the commands generator, delete the boot-prompt template, and update all related tests and documentation. CLAUDE.md and MCP servers now provide all context Claude Code needs at session start, making the boot prompt redundant.",
        "details": "## Overview\n\nF12 removes the boot prompt generation pipeline entirely. CLAUDE.md (auto-loaded by Claude Code) and MCP servers already surface all the context a session needs. Keeping `boot-prompt.txt` creates redundancy and instruction drift risk.\n\n## Files to Modify\n\n### 1. `src/generators/commands.ts`\n\nRemove all boot-prompt logic:\n\n```typescript\n// BEFORE: generateCommands returns 3 FileDescriptors\n// [dev-next.md, review.md, boot-prompt.txt]\n\n// AFTER: generateCommands returns 2 FileDescriptors\n// [dev-next.md, review.md]\n```\n\nSpecific changes:\n- Remove the block that reads `templates/boot-prompt.txt`\n- Remove the `FileDescriptor` entry for `.claude/boot-prompt.txt`\n- Remove `getTrackerNextCommand()` / `getTrackerDoneCommand()` helper calls that were **only** used by the boot-prompt template substitution (keep them if they are still used by `dev-next.md` or `review.md`)\n- Remove unused `TEMPLATES_DIR` constant if it was only used to locate `boot-prompt.txt`\n\nVerify the remaining two files (`dev-next.md`, `review.md`) still render correctly for all three task-tracker variants.\n\n### 2. `templates/boot-prompt.txt`\n\nDelete this file entirely. It is no longer read by any generator.\n\n### 3. `test/generators/commands.test.ts`\n\n- Remove the `boot-prompt.txt` assertions block (project name substitution, tracker-specific content, `/dev-next` and `/review` command references, unresolved placeholder check).\n- Update the \"output always includes N files\" assertion from **3** → **2**.\n- Keep all existing tests for `dev-next.md` and `review.md` unchanged.\n\n### 4. `README.md`\n\n- Remove any mention of `boot-prompt.txt` from the \"Generated files\" table or usage section.\n- If the README references \"session boot prompt\" or \"boot context\", replace with a note that CLAUDE.md provides session context automatically.\n\n## Implementation Notes\n\n- Do **not** remove the tracker-specific helper functions (`getTrackerNextCommand`, `getTrackerDoneCommand`) if they are referenced by `dev-next.md` template rendering — only remove dead code.\n- Confirm `TEMPLATES_DIR` is still needed after removal; if `boot-prompt.txt` was the only file read from `TEMPLATES_DIR` (as opposed to `COMMANDS_DIR`), remove that constant too.\n- After deleting `templates/boot-prompt.txt`, run `git rm templates/boot-prompt.txt` to stage the deletion.\n- The `docs/features/F12-remove-boot-prompt.md` spec file should remain in place as a permanent record of the decision.",
        "testStrategy": "1. **Unit tests** — Run `npm test` targeting `test/generators/commands.test.ts`:\n   - Verify `generateCommands()` returns exactly **2** `FileDescriptor` objects (not 3).\n   - Verify neither descriptor has a path containing `boot-prompt`.\n   - Verify `dev-next.md` still contains tracker-correct commands for all three trackers (`task-master next`, `bd show`, markdown variant).\n   - Verify `review.md` still contains the 5 quality-gate steps and `git diff` reference.\n   - All tests must pass with zero failures.\n\n2. **No dead template file** — Assert `templates/boot-prompt.txt` does **not** exist:\n   ```bash\n   test ! -f templates/boot-prompt.txt && echo \"PASS: boot-prompt.txt deleted\"\n   ```\n\n3. **No unresolved placeholders** — Confirm neither remaining generated file contains `{{…}}` tokens after template fill for any tracker variant.\n\n4. **Full pipeline** — Run the complete quality gate in sequence:\n   ```bash\n   npm run format:check\n   npm run lint\n   npm run typecheck\n   npm run build\n   npm test\n   ```\n   All must exit with code 0.\n\n5. **Integration test** — Run `ai-init --non-interactive` (or the equivalent integration test in `test/integration/full-run.test.ts`) and verify:\n   - `.claude/commands/dev-next.md` is generated.\n   - `.claude/commands/review.md` is generated.\n   - `.claude/boot-prompt.txt` is **not** generated.\n\n6. **README accuracy** — Confirm `README.md` no longer lists `boot-prompt.txt` in the generated files table, and that no broken references to it remain.",
        "status": "done",
        "dependencies": [
          "12",
          "21",
          "22"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-02-18T17:43:32.744Z"
      },
      {
        "id": "25",
        "title": "Enhanced Wizard UI with Colors & Visual Polish (F14)",
        "description": "Upgrade the wizard from plain console.log output to a visually polished CLI experience by adding chalk, ora, boxen, and gradient-string. This covers the header banner, colored step indicators with progress bar, ora spinners for async operations, color-coded output, a boxed summary table, and themed prompt options.",
        "details": "## Overview\n\nThe current wizard (`src/wizard.ts`) uses raw `console.log` with Unicode box-drawing characters and no color. Every async operation (Claude Code check, MCP validation, audit) is silent. This feature upgrades the entire output layer to match the visual quality of tools like `create-next-app` and Vite.\n\n---\n\n## 1. Install Dependencies\n\nAdd to `package.json` as runtime `dependencies` (NOT devDependencies — these ship with the CLI):\n\n```bash\nnpm install chalk@latest ora@latest boxen@latest gradient-string@latest\nnpm install --save-dev @types/gradient-string@latest\n```\n\nAfter install, verify all four packages are ESM-compatible (they are — all are from the Sindre Sorhus ecosystem targeting `\"type\": \"module\"` projects).\n\n---\n\n## 2. Create `src/ui.ts` — Shared UI Helper Module\n\nCreate a new file `src/ui.ts` that exports themed chalk instances and a spinner factory so every file imports from one place:\n\n```typescript\nimport chalk from 'chalk';\nimport ora, { type Ora } from 'ora';\nimport boxen from 'boxen';\nimport gradient from 'gradient-string';\n\n// Themed chalk instances\nexport const c = {\n  success: chalk.green,\n  warning: chalk.yellow,\n  error: chalk.red,\n  info: chalk.cyan,\n  dim: chalk.dim,\n  bold: chalk.bold,\n  step: chalk.cyan.bold,\n  header: chalk.white.bold,\n};\n\n// Spinner factory — creates an ora spinner with the project theme\nexport function spinner(text: string): Ora {\n  return ora({ text, color: 'cyan' });\n}\n\n// Render the gradient header banner using gradient-string + boxen\nexport function renderBanner(version: string): string {\n  const title = gradient(['#00b4d8', '#90e0ef'])('ai-init ' + version);\n  return boxen(`${title}\\n${chalk.dim('Bootstrap your AI dev environment')}`, {\n    padding: 1,\n    margin: { top: 1, bottom: 1, left: 0, right: 0 },\n    borderStyle: 'round',\n    borderColor: 'cyan',\n  });\n}\n\n// Render a step indicator: \"[3/10] 🔧 Step Title\"\nexport function renderStep(current: number, total: number, title: string, emoji = '🔧'): string {\n  const progressPct = Math.round((current / total) * 100);\n  const barLen = 30;\n  const filled = Math.round((progressPct / 100) * barLen);\n  const bar = chalk.cyan('━'.repeat(filled)) + chalk.dim('━'.repeat(barLen - filled));\n  const indicator = chalk.cyan.bold(`[${current}/${total}]`);\n  return `\\n${indicator} ${emoji} ${c.bold(title)}\\n${bar} ${chalk.dim(progressPct + '%')}`;\n}\n\n// Render a boxed summary table of generated files / choices\nexport function renderSummaryBox(rows: Array<{ label: string; value: string; skipped?: boolean }>): string {\n  const lines = rows.map(r =>\n    r.skipped\n      ? `  ${chalk.dim('⊘')} ${chalk.dim(r.label.padEnd(20))} ${chalk.dim(r.value)}`\n      : `  ${chalk.green('✓')} ${chalk.white(r.label.padEnd(20))} ${chalk.dim(r.value)}`\n  );\n  return boxen(lines.join('\\n'), {\n    title: chalk.cyan.bold('Setup Summary'),\n    titleAlignment: 'left',\n    padding: { top: 0, bottom: 0, left: 1, right: 1 },\n    margin: { top: 1, bottom: 1, left: 0, right: 0 },\n    borderStyle: 'round',\n    borderColor: 'cyan',\n  });\n}\n```\n\n---\n\n## 3. Update `src/cli.ts`\n\n- Import `renderBanner` from `./ui.js` and print it at startup (before the `meow` help text is parsed).\n- Replace `printGeneratedFiles` console output with chalk green check marks.\n- Replace `printNextSteps` plain separator lines with a `boxen` call.\n- Set `FORCE_COLOR=1` if stdout is a TTY; set chalk level to 0 in non-interactive mode (ensures CI/piped output stays clean).\n\nKey change in `main()`:\n\n```typescript\n// At top of main(), before switch:\nif (process.stdout.isTTY) {\n  const { version } = JSON.parse(\n    await fs.readFile(new URL('../package.json', import.meta.url), 'utf8')\n  ) as { version: string };\n  console.log(renderBanner(version));\n}\n```\n\n---\n\n## 4. Update `src/wizard.ts`\n\nReplace every `console.log(\"\\n  Step N: ...\")` header with `renderStep(n, 10, title, emoji)`. \n\nAdd a spinner in `stepClaudeBootstrap()`:\n```typescript\nconst spin = spinner('Checking Claude Code installation...');\nspin.start();\nconst hasClaude = await commandExists('claude');\nhasClaude ? spin.succeed(c.success('Claude Code is installed.'))\n           : spin.warn(c.warning('Claude Code not found. Audit step will be skipped.'));\n```\n\nReplace `stepSummary()` with a `renderSummaryBox()` call, passing rows built from the final `config`.\n\nStep emojis: Step 0 🔍, Step 1 🔌, Step 2 📋, Step 3 📄, Step 4 🏗️, Step 5 📡, Step 6 🗄️, Step 7 📁, Step 8 🤖, Step 9 🔎.\n\n---\n\n## 5. Update `src/phases/post-create.ts`\n\nWrap the file-generation loop with an ora spinner:\n```typescript\nconst spin = spinner('Generating project files...');\nspin.start();\n// ... existing writeFiles call ...\nspin.succeed(c.success(`Generated ${written.length} files.`));\n```\n\nPrint each written file with `c.success('✓') + ' ' + c.dim(path)` instead of raw console.log.\n\n---\n\n## 6. Update `src/phases/post-start.ts`\n\nReplace the plain `console.log` box-drawing banner with a `boxen` call. Color-code the task summary:\n- `c.success(done)` for done count\n- `c.warning(inProgress)` for in-progress count  \n- `c.info(pending)` for pending count\n\n---\n\n## 7. Update `src/audit.ts`\n\nAdd an ora spinner during `runAudit()`:\n```typescript\nconst spin = spinner('Running AI-powered audit...');\nspin.start();\n// ... await run('claude', ...) ...\nspin.succeed('Audit complete.');\n```\n\nColor-code audit output lines:\n- Lines starting with `PASS:` → `c.success(line)`\n- Lines starting with `FILL:` → `c.warning(line)`\n- Lines starting with `FIX:` → `c.error(line)`\n\n---\n\n## 8. Test Compatibility\n\nIn test files (`test/`), set chalk's color level to 0 before tests to strip ANSI codes from assertions:\n\n```typescript\n// test/setup.ts (or at top of each test file)\nimport chalk from 'chalk';\nchalk.level = 0; // disable colors in tests\n```\n\nAlternatively pass `--no-color` to the test process via `vitest.config.ts`:\n```typescript\nexport default defineConfig({\n  test: { env: { NO_COLOR: '1' } }\n});\n```\n\n---\n\n## Non-interactive / CI Safety\n\nWhen `SETUP_AI_NONINTERACTIVE=1` or `NO_COLOR` env var is set, or stdout is not a TTY:\n- Skip the gradient banner (chalk.level automatically drops to 0 in non-TTY environments for most chalk v5 builds)\n- Ensure spinners fall back to simple text with no animation (ora handles this automatically when not a TTY)\n- All output must remain human-readable without ANSI codes",
        "testStrategy": "### 1. Manual Visual Test (Primary)\n\nRun the interactive wizard in a real terminal to visually verify each enhancement:\n\n```bash\nnpm run build && node dist/cli.js\n```\n\nVerify:\n- Gradient banner displays at startup (not in CI/piped mode)\n- Step indicators show `[N/10] emoji Title` with a progress bar beneath\n- Spinners animate during Claude Code check (Step 0)\n- Summary box renders using `boxen` round borders at the end\n- Generated files list uses green check marks\n\n### 2. Non-interactive / CI Regression\n\n```bash\nSETUP_AI_NONINTERACTIVE=1 node dist/cli.js\n```\n\n- No gradient banner printed (stdout not a TTY in most CI)\n- No spinner animation output\n- No ANSI escape codes in piped output: `node dist/cli.js | cat` must produce readable plain text\n\n### 3. Unit Tests — ui.ts helpers\n\nCreate `test/ui.test.ts`:\n\n```typescript\nimport { describe, it, expect, beforeAll } from 'vitest';\nimport chalk from 'chalk';\n\nbeforeAll(() => { chalk.level = 0; }); // strip ANSI for assertions\n\ndescribe('renderStep', () => {\n  it('includes step count and title', async () => {\n    const { renderStep } = await import('../src/ui.js');\n    const out = renderStep(3, 10, 'Task Tracker');\n    expect(out).toContain('[3/10]');\n    expect(out).toContain('Task Tracker');\n    expect(out).toContain('30%');\n  });\n});\n\ndescribe('renderSummaryBox', () => {\n  it('marks skipped rows with ⊘ and present rows with ✓', async () => {\n    const { renderSummaryBox } = await import('../src/ui.js');\n    const out = renderSummaryBox([\n      { label: 'CLAUDE.md', value: 'Agent config' },\n      { label: 'Agent teams', value: 'Skipped', skipped: true },\n    ]);\n    expect(out).toContain('✓');\n    expect(out).toContain('⊘');\n  });\n});\n```\n\nRun: `npm test`\n\n### 4. Build & Type-check\n\n```bash\nnpm run typecheck   # zero TS errors (gradient-string types via @types/gradient-string)\nnpm run build       # tsc exits 0, dist/cli.js updated\n```\n\n### 5. Lint\n\n```bash\nnpm run lint        # ESLint passes with no errors on src/ui.ts and modified files\n```\n\n### 6. Audit Output Coloring (Manual)\n\nRun a full wizard with audit enabled and verify:\n- `PASS:` lines print in green\n- `FILL:` lines print in yellow  \n- `FIX:` lines print in red\n- Plain text without color is still readable (test with `| cat`)",
        "status": "done",
        "dependencies": [
          "14",
          "13"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-02-18T17:43:38.996Z"
      },
      {
        "id": "26",
        "title": "Granular Wizard Opt-in for Rules, Hooks & Skills (F13)",
        "description": "Extend the interactive wizard with three new multi-select steps letting users cherry-pick which rules, hook steps, and skills to generate, instead of generating all by default. Adds `selectedRules`, `selectedHookSteps`, and `selectedSkills` fields to `ProjectConfig` with corresponding `SETUP_AI_RULES`, `SETUP_AI_HOOKS`, and `SETUP_AI_SKILLS` environment variables for non-interactive mode.",
        "details": "## Overview\n\nCurrently `generateRules`, `generateSkills`, and `generateHooks` are coarse boolean toggles — you either get everything or nothing. F13 replaces this with granular multi-select pickers in the wizard, while generators filter their output accordingly.\n\n---\n\n## 1. Update `src/types.ts`\n\nAdd three new fields to `ProjectConfig`:\n\n```typescript\nexport interface ProjectConfig {\n  // ... existing fields ...\n  selectedRules: string[];       // e.g. [\"general\",\"testing\",\"git\",\"security\",\"config\"]\n  selectedHookSteps: string[];   // e.g. [\"format\",\"lint\",\"typecheck\",\"build\",\"test\"]\n  selectedSkills: string[];      // e.g. [\"testing\",\"commit\",\"task-workflow\"]\n}\n```\n\n---\n\n## 2. Update `src/defaults.ts`\n\nSet defaults to include everything (opt-out model — users deselect rather than select):\n\n```typescript\nexport function defaultConfig(projectRoot: string): ProjectConfig {\n  return {\n    // ... existing defaults ...\n    selectedRules: [\"general\",\"docs\",\"testing\",\"git\",\"security\",\"config\",\"api\",\"database\"],\n    selectedHookSteps: [\"format\",\"lint\",\"typecheck\",\"build\",\"test\"],\n    selectedSkills: [\"testing\",\"commit\",\"task-workflow\"],\n  };\n}\n```\n\nNote: `api` and `database` remain in defaults but `generateRules` in `rules.ts` already filters them by `hasApiDocs`/`hasDatabase`. The new `selectedRules` check is an additional filter.\n\n---\n\n## 3. Update `src/wizard.ts`\n\nInsert three new wizard steps between current Steps 6 (Database) and 7 (Generation confirm). Renumber subsequent steps accordingly. Follow the existing `fromEnv` + `NON_INTERACTIVE` pattern:\n\n### Step 6b — Rules Picker\n\n```typescript\nasync function stepRulesPicker(config: ProjectConfig): Promise<void> {\n  if (NON_INTERACTIVE) {\n    const envVal = process.env.SETUP_AI_RULES;\n    config.selectedRules = envVal\n      ? envVal.split(',').map(s => s.trim()).filter(Boolean)\n      : defaultConfig(config.projectRoot).selectedRules;\n    return;\n  }\n  if (!config.generateRules) return; // skip if rules disabled entirely\n\n  const choices = [\n    { name: 'general   — Language & coding style', value: 'general', checked: true },\n    { name: 'docs      — Documentation standards', value: 'docs', checked: true },\n    { name: 'testing   — Test-first & quality gate', value: 'testing', checked: true },\n    { name: 'git       — Commit & branch conventions', value: 'git', checked: true },\n    { name: 'security  — OWASP & secrets hygiene', value: 'security', checked: true },\n    { name: 'config    — Config file conventions', value: 'config', checked: true },\n    { name: 'api       — API documentation rules', value: 'api', checked: config.hasApiDocs },\n    { name: 'database  — Database schema & query rules', value: 'database', checked: config.hasDatabase },\n  ];\n\n  config.selectedRules = await checkbox({\n    message: '[6b/10] Which rules do you want to generate? (space to toggle)',\n    choices,\n    validate: (val) => val.length > 0 ? true : 'Select at least one rule.',\n  });\n}\n```\n\n### Step 6c — Hooks Picker\n\n```typescript\nasync function stepHooksPicker(config: ProjectConfig): Promise<void> {\n  if (NON_INTERACTIVE) {\n    const envVal = process.env.SETUP_AI_HOOKS;\n    config.selectedHookSteps = envVal\n      ? envVal.split(',').map(s => s.trim()).filter(Boolean)\n      : defaultConfig(config.projectRoot).selectedHookSteps;\n    return;\n  }\n  if (!config.generateHooks) return; // skip if hooks disabled entirely\n\n  const choices = [\n    { name: 'format    — Run formatter (prettier / black)', value: 'format', checked: true },\n    { name: 'lint      — Run linter (eslint / ruff)', value: 'lint', checked: true },\n    { name: 'typecheck — Run type checker (tsc / mypy)', value: 'typecheck', checked: true },\n    { name: 'build     — Run build command', value: 'build', checked: true },\n    { name: 'test      — Run test suite', value: 'test', checked: true },\n  ];\n\n  config.selectedHookSteps = await checkbox({\n    message: '[6c/10] Which pre-commit quality gate steps should run?',\n    choices,\n    validate: (val) => val.length > 0 ? true : 'Select at least one hook step.',\n  });\n}\n```\n\n### Step 6d — Skills Picker\n\n```typescript\nasync function stepSkillsPicker(config: ProjectConfig): Promise<void> {\n  if (NON_INTERACTIVE) {\n    const envVal = process.env.SETUP_AI_SKILLS;\n    config.selectedSkills = envVal\n      ? envVal.split(',').map(s => s.trim()).filter(Boolean)\n      : defaultConfig(config.projectRoot).selectedSkills;\n    return;\n  }\n  if (!config.generateSkills) return; // skip if skills disabled entirely\n\n  const choices = [\n    { name: 'testing       — Testing workflow skill', value: 'testing', checked: true },\n    { name: 'commit        — Commit message conventions skill', value: 'commit', checked: true },\n    { name: 'task-workflow — Task tracker workflow skill', value: 'task-workflow', checked: true },\n  ];\n\n  config.selectedSkills = await checkbox({\n    message: '[6d/10] Which skills do you want to generate?',\n    choices,\n    validate: (val) => val.length > 0 ? true : 'Select at least one skill.',\n  });\n}\n```\n\nAdd calls to `runWizard()` in the correct position between `stepDatabase` and `stepGenerationConfirm`.\n\n---\n\n## 4. Update `src/generators/rules.ts`\n\nFilter `RULE_FILES` output by `config.selectedRules`. The existing conditional logic for `api` and `database` (checking `hasApiDocs`/`hasDatabase`) remains — `selectedRules` is an additional filter:\n\n```typescript\nexport async function generateRules(config: ProjectConfig): Promise<FileDescriptor[]> {\n  const files: FileDescriptor[] = [];\n  const selected = new Set(config.selectedRules);\n\n  const alwaysRules = ['general','docs','testing','git','security','config'];\n  for (const name of alwaysRules) {\n    if (!selected.has(name)) continue;\n    files.push({ path: `.claude/rules/${name}.md`, content: await readRule(`${name}.md`) });\n  }\n\n  if (config.hasApiDocs && selected.has('api')) {\n    files.push({ path: '.claude/rules/api.md', content: await readRule('api.md') });\n  }\n  if (config.hasDatabase && selected.has('database')) {\n    files.push({ path: '.claude/rules/database.md', content: await readRule('database.md') });\n  }\n  if (config.agentTeamsEnabled && selected.has('agent-teams')) {\n    files.push({ path: '.claude/rules/agent-teams.md', content: await readRule('agent-teams.md') });\n  }\n\n  return files;\n}\n```\n\n---\n\n## 5. Update `src/generators/hooks.ts`\n\nBuild `pre-commit.sh` dynamically from `config.selectedHookSteps` instead of including all steps unconditionally:\n\n```typescript\nconst STEP_SNIPPETS: Record<string, string> = {\n  format:    `if npm run format --if-present 2>/dev/null; then :; fi`,\n  lint:      `npm run lint --if-present || exit 1`,\n  typecheck: `npm run typecheck --if-present || exit 1`,\n  build:     `npm run build --if-present || exit 1`,\n  test:      `npm test --if-present || exit 1`,\n};\n\nexport async function generateHooks(config: ProjectConfig): Promise<FileDescriptor[]> {\n  const selectedSteps = config.selectedHookSteps ?? Object.keys(STEP_SNIPPETS);\n  const stepLines = selectedSteps\n    .filter(s => STEP_SNIPPETS[s])\n    .map(s => STEP_SNIPPETS[s])\n    .join('\\n');\n\n  const script = `#!/usr/bin/env bash\nset -euo pipefail\n\n# Pre-commit quality gate (generated by ai-init)\n# Steps: ${selectedSteps.join(', ')}\n\n${stepLines}\n`;\n  // ... rest of file generation (settings.json merge unchanged)\n}\n```\n\n---\n\n## 6. Update `src/generators/skills.ts`\n\nFilter output by `config.selectedSkills`:\n\n```typescript\nexport async function generateSkills(config: ProjectConfig): Promise<FileDescriptor[]> {\n  const allSkills = ['testing', 'commit', 'task-workflow'];\n  const selected = new Set(config.selectedSkills);\n\n  return Promise.all(\n    allSkills\n      .filter(name => selected.has(name))\n      .map(async name => ({\n        path: `.claude/skills/${name}.md`,\n        content: fillTemplate(await readSkill(`${name}.md`), {\n          TASK_TRACKER: config.taskTracker,\n        }),\n      }))\n  );\n}\n```\n\n---\n\n## 7. README / Docs\n\nUpdate `README.md` environment variables table:\n\n| Variable | Values | Default |\n|---|---|---|\n| `SETUP_AI_RULES` | Comma-separated: `general,docs,testing,git,security,config,api,database` | All |\n| `SETUP_AI_HOOKS` | Comma-separated: `format,lint,typecheck,build,test` | All |\n| `SETUP_AI_SKILLS` | Comma-separated: `testing,commit,task-workflow` | All |\n",
        "testStrategy": "### 1. Unit Tests — `test/generators/rules.test.ts`\n\nAdd test cases for `selectedRules` filtering:\n\n```typescript\nit('only generates selected rules', async () => {\n  const config = { ...defaultConfig('/tmp'), selectedRules: ['general', 'testing'] };\n  const files = await generateRules(config);\n  const paths = files.map(f => f.path);\n  expect(paths).toContain('.claude/rules/general.md');\n  expect(paths).toContain('.claude/rules/testing.md');\n  expect(paths).not.toContain('.claude/rules/docs.md');\n  expect(paths).not.toContain('.claude/rules/git.md');\n});\n\nit('respects hasApiDocs AND selectedRules for api rule', async () => {\n  const config = { ...defaultConfig('/tmp'), hasApiDocs: true, selectedRules: ['general'] };\n  const files = await generateRules(config);\n  expect(files.map(f => f.path)).not.toContain('.claude/rules/api.md');\n});\n\nit('generates api rule when both hasApiDocs and selectedRules include api', async () => {\n  const config = { ...defaultConfig('/tmp'), hasApiDocs: true, selectedRules: ['general', 'api'] };\n  const files = await generateRules(config);\n  expect(files.map(f => f.path)).toContain('.claude/rules/api.md');\n});\n```\n\n### 2. Unit Tests — `test/generators/hooks.test.ts`\n\n```typescript\nit('generates pre-commit.sh with only selected steps', async () => {\n  const config = { ...defaultConfig('/tmp'), selectedHookSteps: ['format', 'lint'] };\n  const files = await generateHooks(config);\n  const script = files.find(f => f.path.includes('pre-commit.sh'))?.content ?? '';\n  expect(script).toContain('npm run format');\n  expect(script).toContain('npm run lint');\n  expect(script).not.toContain('npm run typecheck');\n  expect(script).not.toContain('npm run build');\n  expect(script).not.toContain('npm test');\n});\n\nit('generates a valid bash script with any subset', async () => {\n  const config = { ...defaultConfig('/tmp'), selectedHookSteps: ['test'] };\n  const files = await generateHooks(config);\n  const script = files.find(f => f.path.includes('pre-commit.sh'))?.content ?? '';\n  expect(script).toMatch(/^#!/);\n  expect(script).toContain('npm test');\n});\n```\n\n### 3. Unit Tests — `test/generators/skills.test.ts`\n\n```typescript\nit('only generates selected skills', async () => {\n  const config = { ...defaultConfig('/tmp'), selectedSkills: ['commit'] };\n  const files = await generateSkills(config);\n  const paths = files.map(f => f.path);\n  expect(paths).toContain('.claude/skills/commit.md');\n  expect(paths).not.toContain('.claude/skills/testing.md');\n  expect(paths).not.toContain('.claude/skills/task-workflow.md');\n});\n```\n\n### 4. Integration Test — Wizard Non-Interactive Mode\n\nIn `test/integration/wizard.test.ts`, add cases:\n\n```typescript\nit('reads SETUP_AI_RULES from environment', async () => {\n  process.env.SETUP_AI_NONINTERACTIVE = '1';\n  process.env.SETUP_AI_RULES = 'general,security';\n  const config = await runWizard('/tmp/test-project');\n  expect(config.selectedRules).toEqual(['general', 'security']);\n  delete process.env.SETUP_AI_RULES;\n});\n\nit('reads SETUP_AI_HOOKS from environment', async () => {\n  process.env.SETUP_AI_NONINTERACTIVE = '1';\n  process.env.SETUP_AI_HOOKS = 'lint,test';\n  const config = await runWizard('/tmp/test-project');\n  expect(config.selectedHookSteps).toEqual(['lint', 'test']);\n  delete process.env.SETUP_AI_HOOKS;\n});\n\nit('reads SETUP_AI_SKILLS from environment', async () => {\n  process.env.SETUP_AI_NONINTERACTIVE = '1';\n  process.env.SETUP_AI_SKILLS = 'commit';\n  const config = await runWizard('/tmp/test-project');\n  expect(config.selectedSkills).toEqual(['commit']);\n  delete process.env.SETUP_AI_SKILLS;\n});\n\nit('defaults to all rules/hooks/skills when env vars absent in non-interactive mode', async () => {\n  process.env.SETUP_AI_NONINTERACTIVE = '1';\n  const config = await runWizard('/tmp/test-project');\n  expect(config.selectedRules.length).toBeGreaterThanOrEqual(6);\n  expect(config.selectedHookSteps).toEqual(['format','lint','typecheck','build','test']);\n  expect(config.selectedSkills).toEqual(['testing','commit','task-workflow']);\n});\n```\n\n### 5. Quality Gate\n\nRun the full pre-completion checklist before marking done:\n\n```bash\nnpm run format\nnpm run lint\nnpm run typecheck\nnpm run build\nnpm test\n```\n\nAll must pass with zero errors. Verify `dist/` contains updated generators by inspecting output files.",
        "status": "done",
        "dependencies": [
          "14",
          "10",
          "6"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Add selectedRules, selectedHookSteps, selectedSkills to ProjectConfig and defaults",
            "description": "Extend src/types.ts with three new string array fields on ProjectConfig, and update src/defaults.ts to initialize them with all available values (opt-out model).",
            "dependencies": [],
            "details": "In src/types.ts, add to the ProjectConfig interface: `selectedRules: string[]`, `selectedHookSteps: string[]`, and `selectedSkills: string[]`. In src/defaults.ts, update defaultConfig() to include: `selectedRules: ['general','docs','testing','git','security','config','api','database']`, `selectedHookSteps: ['format','lint','typecheck','build','test']`, `selectedSkills: ['testing','commit','task-workflow']`. These defaults follow the opt-out model where all options are pre-selected and users deselect what they don't want. The existing boolean flags (generateRules, generateSkills, generateHooks) are NOT removed — they remain as coarse toggles that gate the new pickers.",
            "status": "pending",
            "testStrategy": "Run `npm run typecheck` to verify no TypeScript errors. Confirm defaultConfig() returns the three new arrays with expected values.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add stepRulesPicker wizard step in src/wizard.ts",
            "description": "Insert a new wizard step 6b after stepDatabase() that presents a multi-select checkbox for choosing which rule files to generate. Supports NON_INTERACTIVE mode via SETUP_AI_RULES env var.",
            "dependencies": [
              1
            ],
            "details": "In src/wizard.ts, define `async function stepRulesPicker(config: ProjectConfig): Promise<void>`. In NON_INTERACTIVE mode, read SETUP_AI_RULES env var, split by comma, trim whitespace, filter empties; fall back to defaultConfig selectedRules if unset. In interactive mode, skip if `!config.generateRules`. Use the `checkbox` import from @inquirer/prompts with 8 choices (general, docs, testing, git, security, config, api, database). Set `checked: true` for all except `api` (checked: config.hasApiDocs) and `database` (checked: config.hasDatabase). Validate that at least one is selected. Set `config.selectedRules` to the result. Update the `runWizard()` call sequence to call `stepRulesPicker` after `stepDatabase` and before `stepGeneration`. Update the step label message to '[6b/10]'.",
            "status": "pending",
            "testStrategy": "Manual test: run `npm run build && node dist/cli.js` and verify step 6b appears after database step. Test non-interactive: `SETUP_AI_NONINTERACTIVE=1 SETUP_AI_RULES=general,testing node dist/cli.js` and verify only those rules are selected.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add stepHooksPicker wizard step in src/wizard.ts",
            "description": "Insert a new wizard step 6c after stepRulesPicker() that presents a multi-select checkbox for choosing which pre-commit quality gate steps to include. Supports NON_INTERACTIVE mode via SETUP_AI_HOOKS env var.",
            "dependencies": [
              2
            ],
            "details": "In src/wizard.ts, define `async function stepHooksPicker(config: ProjectConfig): Promise<void>`. In NON_INTERACTIVE mode, read SETUP_AI_HOOKS env var, split by comma, trim, filter empties; fall back to defaultConfig selectedHookSteps if unset. In interactive mode, skip if `!config.generateHooks`. Use `checkbox` from @inquirer/prompts with 5 choices: format (prettier/black), lint (eslint/ruff), typecheck (tsc/mypy), build, test. All checked: true by default. Validate at least one selected. Set `config.selectedHookSteps` to result. Add call to `runWizard()` after `stepRulesPicker`. Step label: '[6c/10]'.",
            "status": "pending",
            "testStrategy": "Run wizard interactively and verify step 6c appears. Test env var: `SETUP_AI_HOOKS=format,lint` sets selectedHookSteps correctly in non-interactive mode.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add stepSkillsPicker wizard step in src/wizard.ts",
            "description": "Insert a new wizard step 6d after stepHooksPicker() that presents a multi-select checkbox for choosing which skill files to generate. Supports NON_INTERACTIVE mode via SETUP_AI_SKILLS env var.",
            "dependencies": [
              3
            ],
            "details": "In src/wizard.ts, define `async function stepSkillsPicker(config: ProjectConfig): Promise<void>`. In NON_INTERACTIVE mode, read SETUP_AI_SKILLS env var, split by comma, trim, filter empties; fall back to defaultConfig selectedSkills if unset. In interactive mode, skip if `!config.generateSkills`. Use `checkbox` from @inquirer/prompts with 3 choices: testing, commit, task-workflow. All checked: true by default. Validate at least one selected. Set `config.selectedSkills` to result. Add call to `runWizard()` after `stepHooksPicker`. Step label: '[6d/10]'.",
            "status": "pending",
            "testStrategy": "Run wizard interactively and verify step 6d appears. Test env var: `SETUP_AI_SKILLS=commit` sets selectedSkills correctly in non-interactive mode.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Update src/generators/rules.ts to filter output by selectedRules",
            "description": "Modify generateRules() to check config.selectedRules before including each rule file, while preserving the existing conditional logic for api, database, and agent-teams.",
            "dependencies": [
              1
            ],
            "details": "In src/generators/rules.ts, update generateRules() to build a Set from config.selectedRules (with fallback to all rules if the field is undefined for backward compatibility). For the 6 always-rules (general, docs, testing, git, security, config), add an `if (!selected.has(name)) continue;` guard before pushing each file. For the conditional rules: `api` requires both `config.hasApiDocs && selected.has('api')`, `database` requires both `config.hasDatabase && selected.has('database')`, `agent-teams` requires both `config.agentTeamsEnabled && selected.has('agent-teams')`. The logic is: existing condition AND selection filter — both must pass. Import type assertion may be needed if using `config.selectedRules ?? []`.",
            "status": "pending",
            "testStrategy": "Write tests: (1) with selectedRules=['general','testing'] only those two files are generated, (2) with selectedRules=['api'] and hasApiDocs=true, api.md is generated, (3) with selectedRules=['api'] and hasApiDocs=false, api.md is NOT generated, (4) empty selectedRules or undefined falls back to generating all.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Update src/generators/hooks.ts to dynamically build pre-commit.sh from selectedHookSteps",
            "description": "Replace the static file read of pre-commit.sh template with dynamic script construction using a STEP_SNIPPETS map filtered by config.selectedHookSteps.",
            "dependencies": [
              1
            ],
            "details": "In src/generators/hooks.ts, define a `STEP_SNIPPETS: Record<string, string>` object mapping step names to bash command snippets: format uses `npm run format --if-present 2>/dev/null || true`, lint uses `npm run lint --if-present || exit 1`, typecheck uses `npm run typecheck --if-present || exit 1`, build uses `npm run build --if-present || exit 1`, test uses `npm test --if-present || exit 1`. Update generateHooks() to: (1) read `selectedSteps = config.selectedHookSteps ?? Object.keys(STEP_SNIPPETS)` for backward compat, (2) filter to only known steps, (3) map to snippets and join with newlines, (4) construct the full script string with shebang, set -euo pipefail, comment with step names, and the joined step lines. Do NOT read from the pre-commit.sh template file anymore for hook script content. Keep the buildSettingsJson() call and settings.json output unchanged.",
            "status": "pending",
            "testStrategy": "Write tests: (1) with selectedHookSteps=['format','lint'] the script contains only those two steps and NOT typecheck/build/test, (2) with all steps selected, script contains all 5, (3) verify steps appear in correct order matching the input array order, (4) script always starts with #!/usr/bin/env bash and set -euo pipefail.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Update src/generators/skills.ts to filter output by selectedSkills and update all three generator test files",
            "description": "Modify generateSkills() to only generate skill files listed in config.selectedSkills, then add/update tests in all three generator test files to cover the new selection filtering behavior.",
            "dependencies": [
              5,
              6
            ],
            "details": "In src/generators/skills.ts, update generateSkills() to: build a Set from `config.selectedSkills ?? allSkills` (backward compat), filter the SKILLS array to only those in the set, generate only selected files. The fillTemplate() call and task-workflow TASK_TRACKER placeholder logic remain unchanged. For tests: In test/generators/rules.test.ts, add a new describe block 'selectedRules filtering' with tests: only generates requested rules, api/database/agent-teams respect both conditions. In test/generators/hooks.test.ts, add describe block 'selectedHookSteps filtering' with tests: partial selection produces correct script, order preserved, unknown steps ignored. In test/generators/skills.test.ts, add describe block 'selectedSkills filtering' with tests: only generates selected skill files, single skill selection works, task-workflow placeholder still replaced when selected.",
            "status": "pending",
            "testStrategy": "Run `npm test` and verify all new and existing tests pass. Run `npm run typecheck` to confirm no type errors introduced.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Update README.md with new env vars and run final quality checks",
            "description": "Add SETUP_AI_RULES, SETUP_AI_HOOKS, and SETUP_AI_SKILLS to the README environment variables table, update wizard steps documentation, then run format/lint/typecheck/build/test to verify zero errors.",
            "dependencies": [
              7
            ],
            "details": "In README.md, locate the environment variables table and add three new rows: SETUP_AI_RULES with values 'Comma-separated: general,docs,testing,git,security,config,api,database' and default 'All'; SETUP_AI_HOOKS with values 'Comma-separated: format,lint,typecheck,build,test' and default 'All'; SETUP_AI_SKILLS with values 'Comma-separated: testing,commit,task-workflow' and default 'All'. If README has a wizard steps section, update it to mention the three new sub-steps (6b, 6c, 6d) for granular selection. Run the full quality gate: `npm run format`, `npm run lint`, `npm run typecheck`, `npm run build`, `npm test`. Fix any failures before marking done. Use update_subtask to log findings.",
            "status": "pending",
            "testStrategy": "Verify README renders correctly in markdown preview. All quality gate commands exit with code 0. `npm test` shows all tests passing including the new ones added in subtask 7.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-02-18T17:52:17.474Z"
      },
      {
        "id": "27",
        "title": "Package Manager Detection & Abstraction (F15)",
        "description": "Auto-detect the project's package manager (npm/pnpm/yarn/bun) via lock files and the `packageManager` field in package.json, create a `PackageManager` interface abstracting install/run/exec/global commands, and replace all hardcoded `npm` references in generators and templates with PM-aware values.",
        "details": "## Overview\n\nThis task implements F15 as specified in `docs/features/F15-package-manager-detection.md`. The goal is to make every generated file and lifecycle command PM-aware instead of hardcoding `npm`.\n\n---\n\n## 1. Update `src/types.ts`\n\nAdd the `PackageManager` interface and a `pm` field to `ProjectConfig`:\n\n```typescript\n/** Supported package managers */\nexport type PackageManagerName = \"npm\" | \"pnpm\" | \"yarn\" | \"bun\";\n\nexport interface PackageManager {\n  name: PackageManagerName;\n  install: string;          // e.g. \"npm ci\" | \"pnpm install --frozen-lockfile\"\n  installGlobal: string;    // e.g. \"npm install -g\" | \"pnpm add -g\"\n  run: string;              // e.g. \"npm run\" | \"pnpm\" | \"yarn\" | \"bun run\"\n  exec: string;             // e.g. \"npx\" | \"pnpm dlx\" | \"yarn dlx\" | \"bunx\"\n  lockFile: string;         // e.g. \"package-lock.json\" | \"pnpm-lock.yaml\"\n  runIfPresent: string;     // e.g. \"npm run --if-present\" | \"pnpm run --if-present\"\n}\n\n// In ProjectConfig, add:\npm: PackageManager;\n```\n\n---\n\n## 2. Create `src/pm.ts` (or add to `src/utils.ts`)\n\nAdd a `PACKAGE_MANAGERS` map and `detectPackageManager()` function:\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { PackageManager, PackageManagerName } from './types.js';\n\nexport const PACKAGE_MANAGERS: Record<PackageManagerName, PackageManager> = {\n  npm: {\n    name: 'npm',\n    install: 'npm ci',\n    installGlobal: 'npm install -g',\n    run: 'npm run',\n    exec: 'npx',\n    lockFile: 'package-lock.json',\n    runIfPresent: 'npm run --if-present',\n  },\n  pnpm: {\n    name: 'pnpm',\n    install: 'pnpm install --frozen-lockfile',\n    installGlobal: 'pnpm add -g',\n    run: 'pnpm',\n    exec: 'pnpm dlx',\n    lockFile: 'pnpm-lock.yaml',\n    runIfPresent: 'pnpm run --if-present',\n  },\n  yarn: {\n    name: 'yarn',\n    install: 'yarn install --immutable',\n    installGlobal: 'yarn global add',\n    run: 'yarn',\n    exec: 'yarn dlx',\n    lockFile: 'yarn.lock',\n    runIfPresent: 'yarn run --if-present',\n  },\n  bun: {\n    name: 'bun',\n    install: 'bun install --frozen-lockfile',\n    installGlobal: 'bun add -g',\n    run: 'bun run',\n    exec: 'bunx',\n    lockFile: 'bun.lock',\n    runIfPresent: 'bun run --if-present',\n  },\n};\n\n/**\n * Detect package manager using (in priority order):\n * 1. Lock file presence in projectRoot\n * 2. packageManager field in package.json\n * 3. Defaults to npm\n */\nexport async function detectPackageManager(projectRoot: string): Promise<PackageManager> {\n  // Priority 1: lock files\n  const lockFileMap: Record<string, PackageManagerName> = {\n    'package-lock.json': 'npm',\n    'pnpm-lock.yaml': 'pnpm',\n    'yarn.lock': 'yarn',\n    'bun.lock': 'bun',\n    'bun.lockb': 'bun',\n  };\n  for (const [lockFile, pmName] of Object.entries(lockFileMap)) {\n    try {\n      await fs.access(path.join(projectRoot, lockFile));\n      return PACKAGE_MANAGERS[pmName];\n    } catch { /* not found */ }\n  }\n\n  // Priority 2: packageManager field\n  try {\n    const pkgJson = JSON.parse(\n      await fs.readFile(path.join(projectRoot, 'package.json'), 'utf8')\n    );\n    if (pkgJson.packageManager) {\n      const pmName = pkgJson.packageManager.split('@')[0] as PackageManagerName;\n      if (PACKAGE_MANAGERS[pmName]) return PACKAGE_MANAGERS[pmName];\n    }\n  } catch { /* no package.json */ }\n\n  // Fallback\n  return PACKAGE_MANAGERS.npm;\n}\n```\n\n---\n\n## 3. Update `src/wizard.ts`\n\n- Import `detectPackageManager` from `./pm.js` (or `./utils.js`)\n- At the start of the wizard (after step 0), auto-detect PM from `process.cwd()`\n- Add a wizard step to confirm or override if detection is ambiguous (no lock file found)\n- Add result to the returned `ProjectConfig` as `pm`\n\n---\n\n## 4. Update `src/cli.ts`\n\n- Add `--pm` flag to the meow config: `pm: { type: 'string', shortFlag: 'p' }`\n- If `--pm` flag is provided and is a valid `PackageManagerName`, override the detected PM\n- Pass the resolved `PackageManager` into `ProjectConfig.pm`\n\n---\n\n## 5. Update `src/generators/claude-md.ts`\n\nReplace hardcoded `npm run` in the quality gate section (around line 106–110) with PM-aware commands using `config.pm.run`:\n\n```typescript\n// Before:\n`1. **Format:** \\`npm run format\\``\n\n// After:\n`1. **Format:** \\`${config.pm.run} format\\``\n// Similarly for lint, typecheck, build\n// For test: use config.pm.run + ' test' or config.pm.exec for test runners\n```\n\n---\n\n## 6. Update `src/generators/hooks.ts`\n\nReplace any `npm run --if-present` references with `config.pm.runIfPresent`:\n\n```typescript\n// In the pre-commit hook content/template variables:\nconst vars = {\n  PM_RUN_IF_PRESENT: config.pm.runIfPresent,\n  PM_RUN: config.pm.run,\n};\n```\n\n---\n\n## 7. Update `src/generators/devcontainer.ts`\n\nReplace hardcoded npm references in lifecycle commands if present. Use `config.pm.install` for the postCreate install step.\n\n---\n\n## 8. Update `src/generators/docs.ts`\n\nPass PM-specific template variables to `fillTemplate()` calls:\n\n```typescript\nconst vars = {\n  PROJECT_NAME: config.projectName,\n  ARCHITECTURE: config.architecture,\n  DATE: new Date().toISOString().split('T')[0],\n  TASK_TRACKER: config.taskTracker,\n  PM_NAME: config.pm.name,\n  PM_RUN: config.pm.run,\n  PM_INSTALL: config.pm.install,\n  PM_EXEC: config.pm.exec,\n};\n```\n\n---\n\n## 9. Update `templates/hooks/pre-commit.sh`\n\nReplace all 5 hardcoded `npm run` occurrences with `{{PM_RUN_IF_PRESENT}}`:\n\n```bash\n# Before:\nnpm run --if-present lint\n\n# After:\n{{PM_RUN_IF_PRESENT}} lint\n```\n\n---\n\n## 10. Update `templates/rules/general.md`\n\nReplace `npm` references (lines 17–20) with `{{PM_NAME}}` and `{{PM_INSTALL}}`:\n\n```markdown\n<!-- Before -->\nUse npm for dependency management\n\n<!-- After -->\nUse {{PM_NAME}} for dependency management. Run `{{PM_INSTALL}}` in CI.\n```\n\n---\n\n## 11. Update `templates/docs/onboarding.md`\n\nReplace all `npm install`, `npm run build`, `npm test`, `npm run lint`, `npm run format`, `npm run dev` with PM-aware placeholders:\n\n```markdown\n<!-- Before -->\nnpm install\n\n<!-- After -->\n{{PM_INSTALL}}\n```\n\n---\n\n## 12. Verify `src/phases/on-create.ts`\n\nGlobal installs (`npm install -g`) should remain as `npm install -g` for system-level tools (Claude Code, Task Master) since global tool installation is separate from project-level PM. Document this decision with a comment. Alternatively, use `config.pm.installGlobal` if the spec requires it.\n\n---\n\n## Template Variable Summary\n\nEnsure `fillTemplate()` in `src/utils.ts` supports these new vars across all generators:\n\n| Placeholder | Source |\n|---|---|\n| `{{PM_NAME}}` | `config.pm.name` |\n| `{{PM_RUN}}` | `config.pm.run` |\n| `{{PM_INSTALL}}` | `config.pm.install` |\n| `{{PM_EXEC}}` | `config.pm.exec` |\n| `{{PM_RUN_IF_PRESENT}}` | `config.pm.runIfPresent` |\n| `{{PM_INSTALL_GLOBAL}}` | `config.pm.installGlobal` |\n",
        "testStrategy": "### Unit Tests: `test/pm.test.ts`\n\n1. **Lock file detection:**\n   - Create temp dir with `package-lock.json` → `detectPackageManager()` returns `npm`\n   - Create temp dir with `pnpm-lock.yaml` → returns `pnpm`\n   - Create temp dir with `yarn.lock` → returns `yarn`\n   - Create temp dir with `bun.lock` → returns `bun`\n   - Create temp dir with `bun.lockb` → returns `bun`\n   - Empty temp dir (no lock file, no package.json) → returns `npm` (fallback)\n\n2. **packageManager field detection:**\n   - Temp dir with `package.json` containing `\"packageManager\": \"pnpm@9.15.0\"` and no lock file → returns `pnpm`\n   - Temp dir with `package.json` containing `\"packageManager\": \"yarn@4.0.0\"` → returns `yarn`\n\n3. **Lock file takes priority over packageManager field:**\n   - Temp dir with `package-lock.json` AND `package.json` `\"packageManager\": \"pnpm@9\"` → returns `npm` (lock file wins)\n\n4. **PACKAGE_MANAGERS constant:**\n   - All 4 entries exist (`npm`, `pnpm`, `yarn`, `bun`)\n   - Each has non-empty `install`, `installGlobal`, `run`, `exec`, `lockFile`, `runIfPresent`\n\n### Unit Tests: `test/generators/claude-md.test.ts`\n\n5. With `config.pm = PACKAGE_MANAGERS.pnpm` → generated CLAUDE.md quality gate contains `pnpm format`, `pnpm lint`, `pnpm build`, `pnpm test`\n6. With `config.pm = PACKAGE_MANAGERS.yarn` → quality gate contains `yarn format` etc.\n7. With `config.pm = PACKAGE_MANAGERS.npm` → quality gate contains `npm run format` etc.\n\n### Unit Tests: `test/generators/hooks.test.ts`\n\n8. With `config.pm = PACKAGE_MANAGERS.pnpm` → pre-commit.sh contains `pnpm run --if-present lint` (no `npm` anywhere)\n9. With `config.pm = PACKAGE_MANAGERS.bun` → pre-commit.sh contains `bun run --if-present`\n\n### Unit Tests: `test/generators/docs.test.ts`\n\n10. With `config.pm = PACKAGE_MANAGERS.pnpm` → `onboarding.md` content contains `pnpm install --frozen-lockfile` and `pnpm` (no bare `npm`)\n11. `general.md` rule content substitutes `{{PM_NAME}}` with `pnpm`\n\n### CLI Flag Test\n\n12. Run `ai-init --pm=pnpm --non-interactive` in a temp dir → resulting CLAUDE.md contains `pnpm` commands (integration test or snapshot test)\n13. Run `ai-init --pm=invalid` → CLI exits with an error message listing valid PM names\n\n### Regression Test\n\n14. All existing tests pass with default `npm` PM (no regressions from adding `pm` field to `ProjectConfig`).\n15. Run `npm run typecheck` / `tsc --noEmit` with no errors after adding `pm` to `ProjectConfig` and all generators using it.\n16. Run `npm run lint` — no ESLint errors introduced.\n17. Run `npm run build` — compiles without errors.\n18. Run `npm test` — full Vitest suite green.",
        "status": "pending",
        "dependencies": [
          "3",
          "5",
          "6",
          "7",
          "9",
          "11",
          "12"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": "28",
        "title": "Implement `ai-init update` Incremental Re-configuration Subcommand (F16)",
        "description": "Add an `ai-init update` subcommand that allows non-destructive, diff-based re-configuration after initial setup — enabling users to add/remove MCP servers, switch task trackers, add rules, enable agent teams, and change the package manager without re-running the full wizard or overwriting customizations. Persists wizard choices to `.ai-init.json` on first run and uses backups before destructive changes.",
        "details": "## Overview\n\nThis feature implements F16 as specified in `docs/features/F16-incremental-update-command.md`. It introduces config persistence via `.ai-init.json`, a new `src/update.ts` module with diff-based selective regeneration, and extends `src/cli.ts` with the `update` subcommand.\n\n---\n\n## 1. Update `src/types.ts` — Add `SavedConfig`\n\n```typescript\n/** Persisted wizard state written to .ai-init.json after first run */\nexport interface SavedConfig {\n  version: string;                      // semver of ai-init that wrote it\n  selectedMcps: string[];\n  taskTracker: TaskTracker;\n  architecture: Architecture;\n  selectedRules: string[];              // from F13 — default [] if F13 not yet done\n  pm: string;                          // package manager name, e.g. \"npm\"\n  agentTeamsEnabled: boolean;\n  agentTeamsFile?: string;             // relative path to the agent teams file\n  generatedAt: string;                 // ISO 8601 timestamp\n}\n```\n\n---\n\n## 2. Update `src/utils.ts` — Add Config Persistence & Backup Helpers\n\n```typescript\nconst SAVED_CONFIG_FILENAME = '.ai-init.json';\n\n/** Read the persisted config from <projectRoot>/.ai-init.json, or null if absent. */\nexport async function readSavedConfig(projectRoot: string): Promise<SavedConfig | null> {\n  const filePath = path.join(projectRoot, SAVED_CONFIG_FILENAME);\n  const raw = await readOptional(filePath);\n  if (!raw) return null;\n  try {\n    return JSON.parse(raw) as SavedConfig;\n  } catch {\n    return null;\n  }\n}\n\n/** Write the persisted config to <projectRoot>/.ai-init.json. */\nexport async function writeSavedConfig(projectRoot: string, config: SavedConfig): Promise<void> {\n  const filePath = path.join(projectRoot, SAVED_CONFIG_FILENAME);\n  await fs.writeFile(filePath, JSON.stringify(config, null, 2) + '\\n', 'utf8');\n}\n\n/**\n * Copy a list of relative file paths to .ai-init-backup/<timestamp>/ before destructive changes.\n * Returns the backup directory path.\n */\nexport async function backupFiles(projectRoot: string, relativePaths: string[]): Promise<string> {\n  const ts = new Date().toISOString().replace(/[:.]/g, '-');\n  const backupDir = path.join(projectRoot, '.ai-init-backup', ts);\n  await fs.mkdir(backupDir, { recursive: true });\n  for (const rel of relativePaths) {\n    const src = path.join(projectRoot, rel);\n    const dst = path.join(backupDir, rel);\n    await fs.mkdir(path.dirname(dst), { recursive: true });\n    try {\n      await fs.copyFile(src, dst);\n    } catch { /* file may not exist — skip */ }\n  }\n  return backupDir;\n}\n```\n\n---\n\n## 3. Create `src/update.ts`\n\nThis is the core of F16. Structure:\n\n```typescript\nimport { select, checkbox } from '@inquirer/prompts';\nimport type { ProjectConfig, SavedConfig } from './types.js';\nimport { readSavedConfig, writeSavedConfig, backupFiles } from './utils.js';\nimport { MCP_REGISTRY } from './registry.js';\nimport { runPostCreate } from './phases/post-create.js';\nimport { defaultConfig } from './defaults.js';\n\n/** Merge a SavedConfig back into a ProjectConfig for regeneration. */\nfunction savedToProjectConfig(saved: SavedConfig, projectRoot: string): ProjectConfig {\n  const base = defaultConfig(projectRoot);\n  return {\n    ...base,\n    selectedMcps: saved.selectedMcps,\n    taskTracker: saved.taskTracker,\n    architecture: saved.architecture,\n    agentTeamsEnabled: saved.agentTeamsEnabled,\n    // selectedRules from F13, if present\n    ...(saved.selectedRules?.length ? { selectedRules: saved.selectedRules } : {}),\n  };\n}\n\n/** Compute which generator categories need to re-run based on config diff. */\nfunction computeChangedCategories(prev: SavedConfig, next: SavedConfig): Set<string> {\n  const changed = new Set<string>();\n  const arraysEqual = (a: string[], b: string[]) =>\n    a.length === b.length && a.every((v, i) => v === b[i]);\n\n  if (!arraysEqual([...prev.selectedMcps].sort(), [...next.selectedMcps].sort()))\n    changed.add('mcp');\n  if (prev.taskTracker !== next.taskTracker) changed.add('tracker');\n  if (!arraysEqual([...prev.selectedRules ?? []].sort(), [...next.selectedRules ?? []].sort()))\n    changed.add('rules');\n  if (prev.agentTeamsEnabled !== next.agentTeamsEnabled) changed.add('teams');\n  if (prev.pm !== next.pm) changed.add('pm');\n  return changed;\n}\n\n/** Identify file paths that should be backed up before a destructive change. */\nconst DESTRUCTIVE_FILE_MAP: Record<string, string[]> = {\n  mcp:     ['.mcp.json'],\n  tracker: ['CLAUDE.md', '.taskmaster/config.json'],\n  rules:   ['.claude/rules/'],\n  teams:   ['~/.claude/settings.json'],  // noted but not backed up (outside project)\n};\n\n/** Interactive update dashboard — shows current config and offers changes. */\nexport async function runUpdate(projectRoot: string, cliFlags: Record<string, unknown>): Promise<void> {\n  const saved = await readSavedConfig(projectRoot);\n\n  if (!saved) {\n    console.log('[ai-init] No .ai-init.json found. Run `ai-init` first to generate initial config.');\n    process.exit(1);\n  }\n\n  // --- CLI flag shortcuts (non-interactive update) ---\n  const flags = cliFlags as {\n    addMcp?: string;\n    removeMcp?: string;\n    tracker?: string;\n    addRule?: string;\n    enableTeams?: boolean;\n    pm?: string;\n  };\n\n  let newSaved = { ...saved };\n  let changed = false;\n\n  if (flags.addMcp) {\n    newSaved.selectedMcps = [...new Set([...newSaved.selectedMcps, flags.addMcp])];\n    changed = true;\n  }\n  if (flags.removeMcp) {\n    newSaved.selectedMcps = newSaved.selectedMcps.filter(m => m !== flags.removeMcp);\n    changed = true;\n  }\n  if (flags.tracker) {\n    newSaved.taskTracker = flags.tracker as SavedConfig['taskTracker'];\n    changed = true;\n  }\n  if (flags.addRule) {\n    newSaved.selectedRules = [...new Set([...(newSaved.selectedRules ?? []), flags.addRule])];\n    changed = true;\n  }\n  if (flags.enableTeams) {\n    newSaved.agentTeamsEnabled = true;\n    changed = true;\n  }\n  if (flags.pm) {\n    newSaved.pm = flags.pm;\n    changed = true;\n  }\n\n  // --- Interactive mode (no flags given) ---\n  if (!changed) {\n    // Show current config dashboard\n    console.log('\\nCurrent Configuration:');\n    console.log(`  MCP Servers:     ${saved.selectedMcps.join(', ') || 'none'}`);\n    console.log(`  Task Tracker:    ${saved.taskTracker}`);\n    console.log(`  Architecture:    ${saved.architecture}`);\n    console.log(`  Rules:           ${(saved.selectedRules ?? []).join(', ') || 'all defaults'}`);\n    console.log(`  Package Manager: ${saved.pm ?? 'npm'}`);\n    console.log(`  Agent Teams:     ${saved.agentTeamsEnabled ? 'enabled' : 'disabled'}`);\n    console.log('');\n\n    const action = await select({\n      message: 'What would you like to change?',\n      choices: [\n        { name: 'Add/remove MCP servers', value: 'mcp' },\n        { name: 'Switch task tracker', value: 'tracker' },\n        { name: 'Add/remove rules', value: 'rules' },\n        { name: 'Enable agent teams', value: 'teams' },\n        { name: 'Change package manager', value: 'pm' },\n        { name: 'Exit without changes', value: 'exit' },\n      ],\n    });\n\n    if (action === 'exit') return;\n\n    // Handle each action interactively, mutating newSaved\n    // (similar to wizard step functions but working from saved state)\n    // ... (implementation for each action type)\n    changed = true;\n  }\n\n  // --- Diff, backup destructive changes, and regenerate ---\n  const changedCategories = computeChangedCategories(saved, newSaved);\n  if (changedCategories.size === 0) {\n    console.log('[ai-init update] No changes detected.');\n    return;\n  }\n\n  // Backup files that will be overwritten\n  const toBackup = [...changedCategories]\n    .flatMap(cat => DESTRUCTIVE_FILE_MAP[cat] ?? [])\n    .filter(p => !p.startsWith('~'));  // exclude home-dir paths\n  if (toBackup.length > 0) {\n    const backupDir = await backupFiles(projectRoot, toBackup);\n    console.log(`[ai-init update] Backed up affected files to ${backupDir}`);\n  }\n\n  // Build a ProjectConfig from updated saved state and run selective regeneration\n  const config = savedToProjectConfig(newSaved, projectRoot);\n  await runPostCreate(config, /* overwrite= */ true);\n\n  // Persist the updated config\n  newSaved.generatedAt = new Date().toISOString();\n  await writeSavedConfig(projectRoot, newSaved);\n\n  console.log('[ai-init update] Done. Configuration updated.');\n}\n```\n\n---\n\n## 4. Update `src/cli.ts` — Wire the `update` Subcommand\n\nExtend the `meow` help text to document `update` and its flags:\n\n```\n  update          Incrementally reconfigure after initial setup\n    --add-mcp=<name>      Add an MCP server\n    --remove-mcp=<name>   Remove an MCP server\n    --tracker=<name>      Switch task tracker (taskmaster|beads|markdown)\n    --add-rule=<name>     Add a rule category\n    --enable-teams        Enable agent teams\n    --pm=<name>           Change package manager (npm|pnpm|yarn|bun)\n```\n\nAdd flags to the meow config:\n\n```typescript\nflags: {\n  // existing flags ...\n  addMcp:       { type: 'string' },\n  removeMcp:    { type: 'string' },\n  tracker:      { type: 'string' },\n  addRule:      { type: 'string' },\n  enableTeams:  { type: 'boolean', default: false },\n  pm:           { type: 'string' },\n}\n```\n\nAdd the `update` case to the switch statement in `main()`:\n\n```typescript\ncase 'update': {\n  await runUpdate(projectRoot, cli.flags);\n  break;\n}\n```\n\n---\n\n## 5. Persist `.ai-init.json` on First Run\n\nIn `src/cli.ts` (or `src/phases/post-create.ts`), after `runPostCreate` completes on the initial wizard flow, call `writeSavedConfig()`:\n\n```typescript\n// After runPostCreate in the default wizard case:\nawait writeSavedConfig(projectRoot, {\n  version: cli.pkg.version,\n  selectedMcps: config.selectedMcps,\n  taskTracker: config.taskTracker,\n  architecture: config.architecture,\n  selectedRules: config.selectedRules ?? [],\n  pm: config.pm ?? 'npm',\n  agentTeamsEnabled: config.agentTeamsEnabled,\n  generatedAt: new Date().toISOString(),\n});\n```\n\n---\n\n## 6. Update `.gitignore` (optional but recommended)\n\nAdd `.ai-init-backup/` to `.gitignore` since backups are machine-local noise. Do NOT gitignore `.ai-init.json` — committing it enables team alignment.\n\n---\n\n## 7. Create `test/update.test.ts`\n\nTest scenarios using a temporary directory:\n\n- `readSavedConfig` returns `null` when `.ai-init.json` absent\n- `readSavedConfig` deserializes correctly from a valid JSON file\n- `writeSavedConfig` creates the file with correct content\n- `backupFiles` copies files to `.ai-init-backup/<timestamp>/`\n- `computeChangedCategories` (if exported) returns correct set for MCP diff\n- `runUpdate` with `--add-mcp=context7` adds to `selectedMcps` and regenerates `.mcp.json`\n- `runUpdate` with `--remove-mcp=beads` removes from `selectedMcps` and creates backup\n- `runUpdate` with `--tracker=beads` switches tracker and regenerates `CLAUDE.md`\n- `runUpdate` when no `.ai-init.json` exists exits with an error message\n\n---\n\n## Key Design Constraints\n\n- **Generators are already pure functions** — `src/generators/*.ts` need no changes; `runPostCreate` calls them and they return `FileDescriptor[]`.\n- **Diff scope** — Only MCP, tracker, rules, teams, and pm changes trigger selective regeneration. Architecture and PRD changes require re-running the full wizard.\n- **Smart merge for settings.json** — The hooks generator already merges into `.claude/settings.json`; the update command inherits this behavior automatically by calling `runPostCreate`.\n- **No F13/F15 hard dependency** — `selectedRules` and `pm` fields default gracefully if those features are not yet implemented.",
        "testStrategy": "### 1. Unit Tests — `test/update.test.ts`\n\nUse `node:fs/promises`, `os.tmpdir()`, and Vitest to set up isolated temp directories for each test:\n\n```typescript\nimport { describe, it, expect, beforeEach, afterEach } from 'vitest';\nimport os from 'node:os';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { readSavedConfig, writeSavedConfig, backupFiles } from '../src/utils.js';\n\nlet tmpDir: string;\nbeforeEach(async () => { tmpDir = await fs.mkdtemp(path.join(os.tmpdir(), 'ai-init-')); });\nafterEach(async () => { await fs.rm(tmpDir, { recursive: true, force: true }); });\n```\n\n**Test cases:**\n\n1. `readSavedConfig` returns `null` when `.ai-init.json` is absent.\n2. `writeSavedConfig` creates `.ai-init.json` with correct JSON structure and `generatedAt` field.\n3. Round-trip: write then read returns identical object.\n4. `backupFiles` creates `.ai-init-backup/<timestamp>/` directory and copies specified files.\n5. `backupFiles` silently skips files that don't exist (no throw).\n6. CLI flag `--add-mcp=context7`: after `runUpdate`, `.ai-init.json` `selectedMcps` contains `context7`.\n7. CLI flag `--remove-mcp=taskmaster`: after `runUpdate`, `selectedMcps` no longer contains `taskmaster`, and a backup of `.mcp.json` is created.\n8. CLI flag `--tracker=beads`: after `runUpdate`, `.ai-init.json` `taskTracker` equals `beads`.\n9. `runUpdate` called without existing `.ai-init.json` prints an error and exits with code 1 (use `vi.spyOn(process, 'exit')`).\n10. No-op: when flags result in no change vs saved config, no files are regenerated and `writeSavedConfig` is not called.\n\n### 2. Integration Test — Full Update Cycle\n\n```bash\n# Create a temp project directory, run initial wizard in non-interactive mode,\n# verify .ai-init.json is created, then run update with a flag:\nmkdir /tmp/test-update-project && cd /tmp/test-update-project\nnpm init -y\nSETUP_AI_NONINTERACTIVE=1 SETUP_AI_MCPS=taskmaster node /path/to/dist/cli.js\n# Verify .ai-init.json exists:\ncat .ai-init.json | jq '.selectedMcps'   # should be [\"taskmaster\"]\n# Add a second MCP:\nnode /path/to/dist/cli.js update --add-mcp=context7\ncat .ai-init.json | jq '.selectedMcps'   # should be [\"taskmaster\",\"context7\"]\ncat .mcp.json | jq 'keys'               # should include \"context7\"\n```\n\n### 3. Backup Verification\n\n```bash\nnode dist/cli.js update --remove-mcp=taskmaster\n# .ai-init-backup/<timestamp>/.mcp.json must exist and contain the original .mcp.json content\nls .ai-init-backup/\n```\n\n### 4. Idempotency Check\n\nRun `ai-init update --add-mcp=taskmaster` twice in a row. Verify `selectedMcps` does not contain duplicates.\n\n### 5. Quality Gates (mandatory before marking done)\n\n```bash\nnpm run format     # prettier passes\nnpm run lint       # eslint passes with zero errors\nnpm run typecheck  # tsc --noEmit passes\nnpm run build      # zero errors, dist/cli.js produced\nnpm test           # all tests pass including new update.test.ts\n```",
        "status": "pending",
        "dependencies": [
          "1",
          "14",
          "27"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": "29",
        "title": "Implement `ai-init doctor` Config Health Check & Validation Subcommand (F17)",
        "description": "Add an `ai-init doctor` subcommand that validates the current AI dev environment setup by running categorized health checks across MCP config, CLAUDE.md cross-references, rules/skills integrity, hook executability, task tracker setup, documentation completeness, required globals, and API keys. Outputs color-coded results and exits with code 1 on errors.",
        "details": "## Overview\n\nImplement F17 as specified in `docs/features/F17-doctor-health-check.md`. Creates `src/doctor.ts` as the primary module, updates `src/types.ts` with `CheckResult`/`HealthCheck` interfaces, adds helpers to `src/utils.ts`, adds `getRequiredEnvVars()` to `src/registry.ts`, and wires the `doctor` subcommand into `src/cli.ts`.\n\n---\n\n## 1. Update `src/types.ts` — Add `CheckResult` and `HealthCheck`\n\n```typescript\n/** A single health check result */\nexport interface CheckResult {\n  status: 'pass' | 'warn' | 'error';\n  message: string;\n}\n\n/** A named category of health checks */\nexport interface HealthCheck {\n  category: string;\n  results: CheckResult[];\n}\n```\n\n---\n\n## 2. Update `src/registry.ts` — Add `getRequiredEnvVars()`\n\nExtend the `McpServer` interface in `src/types.ts` to add an optional `requiredEnvVars?: string[]` field. Update each registry entry's `env` keys that contain `${...}` placeholders into explicit `requiredEnvVars` arrays. Add the exported helper:\n\n```typescript\n/** Returns environment variable names required by the given MCP server (by registry name). */\nexport function getRequiredEnvVars(serverName: string): string[] {\n  const server = getMcpByName(serverName);\n  if (!server) return [];\n  // Derive from env values that look like ${VAR_NAME}\n  return Object.values(server.env ?? {})\n    .filter(v => /^\\$\\{.+\\}$/.test(v))\n    .map(v => v.slice(2, -1));\n}\n```\n\nFor `taskmaster`, this should yield `['ANTHROPIC_API_KEY', 'PERPLEXITY_API_KEY']`.\n\n---\n\n## 3. Update `src/utils.ts` — Add Helper Functions\n\nAdd the following helpers after the existing `readOptional` function:\n\n```typescript\n/** Returns true if the file at filePath exists. */\nexport async function fileExists(filePath: string): Promise<boolean> {\n  try {\n    await fs.access(filePath);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/** Returns true if the file at filePath is executable by the current user. */\nexport async function isExecutable(filePath: string): Promise<boolean> {\n  try {\n    await fs.access(filePath, fs.constants.X_OK);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/** Returns true if the string is valid JSON. */\nexport function isValidJson(content: string): boolean {\n  try {\n    JSON.parse(content);\n    return true;\n  } catch {\n    return false;\n  }\n}\n```\n\n---\n\n## 4. Create `src/doctor.ts` — All Health Check Logic\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { fileExists, isExecutable, isValidJson, commandExists, readOptional } from './utils.js';\nimport { MCP_REGISTRY, getRequiredEnvVars } from './registry.js';\nimport type { CheckResult, HealthCheck } from './types.js';\n\n// ANSI colour helpers (degrade gracefully if NO_COLOR is set)\nconst useColor = !process.env.NO_COLOR && process.stdout.isTTY;\nconst GREEN  = useColor ? '\\x1b[32m' : '';\nconst YELLOW = useColor ? '\\x1b[33m' : '';\nconst RED    = useColor ? '\\x1b[31m' : '';\nconst RESET  = useColor ? '\\x1b[0m'  : '';\nconst BOLD   = useColor ? '\\x1b[1m'  : '';\n\nfunction symbol(status: CheckResult['status']): string {\n  switch (status) {\n    case 'pass':  return `${GREEN}✓${RESET}`;\n    case 'warn':  return `${YELLOW}⚠${RESET}`;\n    case 'error': return `${RED}✗${RESET}`;\n  }\n}\n\n// ─── Individual Check Categories ────────────────────────────────────────────\n\n/** 1. MCP Config — valid JSON, server fields, packages installed, API keys set */\nasync function checkMcpConfig(root: string): Promise<HealthCheck> {\n  const results: CheckResult[] = [];\n\n  for (const file of ['.mcp.json', '.vscode/mcp.json']) {\n    const fullPath = path.join(root, file);\n    const content = await readOptional(fullPath);\n    if (content === null) {\n      results.push({ status: 'warn', message: `${file} not found` });\n      continue;\n    }\n    if (!isValidJson(content)) {\n      results.push({ status: 'error', message: `${file} is not valid JSON` });\n    } else {\n      results.push({ status: 'pass', message: `${file} is valid JSON` });\n    }\n  }\n\n  // Check which servers are configured in .mcp.json\n  const mcpContent = await readOptional(path.join(root, '.mcp.json'));\n  if (mcpContent && isValidJson(mcpContent)) {\n    const mcpJson = JSON.parse(mcpContent) as { mcpServers?: Record<string, unknown> };\n    const configuredServers = Object.keys(mcpJson.mcpServers ?? {});\n\n    for (const server of MCP_REGISTRY) {\n      const isConfigured = configuredServers.includes(server.claudeMcpName);\n      if (!isConfigured) continue;\n\n      results.push({ status: 'pass', message: `${server.claudeMcpName} server configured` });\n\n      // Check required env vars\n      for (const envVar of getRequiredEnvVars(server.name)) {\n        const isSet = !!process.env[envVar];\n        results.push(\n          isSet\n            ? { status: 'pass', message: `${envVar} is set` }\n            : { status: 'warn', message: `${envVar} not set in environment` }\n        );\n      }\n    }\n  }\n\n  return { category: 'MCP Configuration', results };\n}\n\n/** 2. Agent Instructions — CLAUDE.md exists, not too large, cross-references resolve */\nasync function checkAgentInstructions(root: string): Promise<HealthCheck> {\n  const results: CheckResult[] = [];\n\n  for (const file of ['CLAUDE.md', 'CLAUDE_MCP.md']) {\n    const fullPath = path.join(root, file);\n    const content = await readOptional(fullPath);\n    if (content === null) {\n      results.push({ status: 'error', message: `${file} not found` });\n      continue;\n    }\n    const lines = content.split('\\n').length;\n    results.push({ status: 'pass', message: `${file} exists (${lines} lines)` });\n    if (lines > 200) {\n      results.push({ status: 'warn', message: `${file} is over 200 lines — consider splitting` });\n    }\n\n    // Check @import cross-references\n    const refs = [...content.matchAll(/@([\\w./\\-]+\\.md)/g)].map(m => m[1]);\n    for (const ref of refs) {\n      const refPath = path.join(root, ref);\n      const exists = await fileExists(refPath);\n      results.push(\n        exists\n          ? { status: 'pass', message: `${file} references ${ref} — found` }\n          : { status: 'error', message: `${file} references ${ref} but file not found` }\n      );\n    }\n  }\n\n  return { category: 'Agent Instructions', results };\n}\n\n/** 3. Rules & Skills — files exist, @import references resolve */\nasync function checkRulesAndSkills(root: string): Promise<HealthCheck> {\n  const results: CheckResult[] = [];\n\n  for (const dir of ['.claude/rules', '.claude/skills']) {\n    const fullDir = path.join(root, dir);\n    try {\n      const files = await fs.readdir(fullDir);\n      const mdFiles = files.filter(f => f.endsWith('.md'));\n      results.push({ status: 'pass', message: `${mdFiles.length} files found in ${dir}/` });\n\n      for (const file of mdFiles) {\n        const content = await fs.readFile(path.join(fullDir, file), 'utf8');\n        const refs = [...content.matchAll(/@([\\w./\\-]+\\.md)/g)].map(m => m[1]);\n        for (const ref of refs) {\n          const refPath = path.join(root, ref);\n          const exists = await fileExists(refPath);\n          if (!exists) {\n            results.push({ status: 'warn', message: `${dir}/${file} imports @${ref} — file missing` });\n          }\n        }\n      }\n    } catch {\n      results.push({ status: 'warn', message: `${dir}/ directory not found` });\n    }\n  }\n\n  return { category: 'Rules & Skills', results };\n}\n\n/** 4. Hooks — scripts exist, are executable, settings.json matchers are valid */\nasync function checkHooks(root: string): Promise<HealthCheck> {\n  const results: CheckResult[] = [];\n\n  const hooksDir = path.join(root, '.claude/hooks');\n  try {\n    const files = await fs.readdir(hooksDir);\n    const shFiles = files.filter(f => f.endsWith('.sh'));\n    if (shFiles.length === 0) {\n      results.push({ status: 'warn', message: 'No hook scripts found in .claude/hooks/' });\n    }\n    for (const file of shFiles) {\n      const fullPath = path.join(hooksDir, file);\n      results.push({ status: 'pass', message: `${file} exists` });\n      const exec = await isExecutable(fullPath);\n      results.push(\n        exec\n          ? { status: 'pass', message: `${file} is executable` }\n          : { status: 'error', message: `${file} is not executable (run: chmod +x .claude/hooks/${file})` }\n      );\n    }\n  } catch {\n    results.push({ status: 'warn', message: '.claude/hooks/ directory not found' });\n  }\n\n  const settingsPath = path.join(root, '.claude/settings.json');\n  const settingsContent = await readOptional(settingsPath);\n  if (settingsContent && isValidJson(settingsContent)) {\n    results.push({ status: 'pass', message: '.claude/settings.json is valid JSON' });\n    const settings = JSON.parse(settingsContent) as { hooks?: unknown };\n    if (settings.hooks) {\n      results.push({ status: 'pass', message: '.claude/settings.json has hooks configured' });\n    }\n  } else if (settingsContent) {\n    results.push({ status: 'error', message: '.claude/settings.json is not valid JSON' });\n  } else {\n    results.push({ status: 'warn', message: '.claude/settings.json not found' });\n  }\n\n  return { category: 'Hooks', results };\n}\n\n/** 5. Task Tracker — configured tracker is set up, task files exist */\nasync function checkTaskTracker(root: string): Promise<HealthCheck> {\n  const results: CheckResult[] = [];\n\n  const tasksJson = path.join(root, '.taskmaster/tasks/tasks.json');\n  const beadsDir  = path.join(root, '.beads');\n  const tasksmd   = path.join(root, 'TASKS.md');\n\n  if (await fileExists(tasksJson)) {\n    const content = await readOptional(tasksJson);\n    results.push({ status: 'pass', message: 'Task Master configured' });\n    results.push({ status: 'pass', message: '.taskmaster/tasks/tasks.json exists' });\n    if (content && isValidJson(content)) {\n      const data = JSON.parse(content) as { tasks?: unknown[] };\n      const count = data.tasks?.length ?? 0;\n      results.push({ status: 'pass', message: `${count} tasks found` });\n    } else {\n      results.push({ status: 'error', message: '.taskmaster/tasks/tasks.json is not valid JSON' });\n    }\n  } else if (await fileExists(beadsDir)) {\n    results.push({ status: 'pass', message: 'Beads task tracker configured' });\n  } else if (await fileExists(tasksmd)) {\n    results.push({ status: 'pass', message: 'TASKS.md task tracker configured' });\n  } else {\n    results.push({ status: 'warn', message: 'No task tracker configured (no .taskmaster/, .beads/, or TASKS.md)' });\n  }\n\n  return { category: 'Task Tracker', results };\n}\n\n/** 6. Documentation — template placeholders are filled, docs/ directory exists */\nasync function checkDocumentation(root: string): Promise<HealthCheck> {\n  const results: CheckResult[] = [];\n  const docsDir = path.join(root, 'docs');\n\n  try {\n    const files = await fs.readdir(docsDir, { recursive: true });\n    const mdFiles = (files as string[]).filter(f => f.endsWith('.md'));\n    results.push({ status: 'pass', message: `docs/ directory exists (${mdFiles.length} markdown files)` });\n\n    for (const file of mdFiles) {\n      const content = await readOptional(path.join(docsDir, file));\n      if (content && /\\{\\{[A-Z_]+\\}\\}/.test(content)) {\n        results.push({ status: 'warn', message: `docs/${file} still has unfilled placeholders ({{...}})` });\n      }\n    }\n  } catch {\n    results.push({ status: 'warn', message: 'docs/ directory not found' });\n  }\n\n  return { category: 'Documentation', results };\n}\n\n/** 7. Dependencies — required npm globals are installed */\nasync function checkDependencies(): Promise<HealthCheck> {\n  const results: CheckResult[] = [];\n  const globals = [\n    { cmd: 'claude',       label: 'Claude Code CLI' },\n    { cmd: 'task-master',  label: 'Task Master CLI' },\n    { cmd: 'npx',          label: 'npx (Node.js)' },\n  ];\n\n  for (const { cmd, label } of globals) {\n    const exists = await commandExists(cmd);\n    results.push(\n      exists\n        ? { status: 'pass',  message: `${label} (${cmd}) is installed` }\n        : { status: 'warn',  message: `${label} (${cmd}) not found on PATH` }\n    );\n  }\n\n  return { category: 'Dependencies', results };\n}\n\n// ─── Runner ─────────────────────────────────────────────────────────────────\n\n/** Run all health checks and return structured results. */\nexport async function runDoctor(projectRoot: string): Promise<HealthCheck[]> {\n  return Promise.all([\n    checkMcpConfig(projectRoot),\n    checkAgentInstructions(projectRoot),\n    checkRulesAndSkills(projectRoot),\n    checkHooks(projectRoot),\n    checkTaskTracker(projectRoot),\n    checkDocumentation(projectRoot),\n    checkDependencies(),\n  ]);\n}\n\n/** Print results to stdout with colour coding. Returns exit code (0 or 1). */\nexport function printDoctorReport(checks: HealthCheck[]): number {\n  let passes = 0, warnings = 0, errors = 0;\n\n  console.log(`\\n${BOLD}AI Dev Environment Health Check${RESET}`);\n  console.log('━'.repeat(32));\n\n  for (const check of checks) {\n    console.log(`\\n${BOLD}${check.category}${RESET}`);\n    for (const result of check.results) {\n      console.log(`  ${symbol(result.status)} ${result.message}`);\n      if (result.status === 'pass')  passes++;\n      if (result.status === 'warn')  warnings++;\n      if (result.status === 'error') errors++;\n    }\n  }\n\n  console.log(`\\n${'━'.repeat(32)}`);\n  console.log(\n    `Summary: ${GREEN}${passes} passed${RESET}, ` +\n    `${YELLOW}${warnings} warnings${RESET}, ` +\n    `${RED}${errors} errors${RESET}`\n  );\n  if (errors > 0) {\n    console.log(\"Run 'ai-init update' to fix configuration issues.\");\n  }\n\n  return errors > 0 ? 1 : 0;\n}\n```\n\n---\n\n## 5. Update `src/cli.ts` — Add `doctor` subcommand\n\nAdd `doctor` to the help text, import `runDoctor` and `printDoctorReport` from `./doctor.js`, and add a new `case 'doctor':` branch in the `switch` block:\n\n```typescript\nimport { runDoctor, printDoctorReport } from './doctor.js';\n\n// In the help string, add under Commands:\n//   doctor          Validate the AI dev environment setup\n\n// In the switch statement:\ncase 'doctor': {\n  const checks = await runDoctor(projectRoot);\n  const exitCode = printDoctorReport(checks);\n  process.exit(exitCode);\n}\n```\n\n---\n\n## Architecture Notes\n\n- `src/doctor.ts` has NO dependency on `ProjectConfig` — it inspects the filesystem directly, making it usable without having run the wizard\n- All checks run with `Promise.all` at the top level (categories in parallel) for speed\n- Colour output degrades gracefully: respects `NO_COLOR` env var and non-TTY streams\n- `printDoctorReport()` is pure (takes `HealthCheck[]`, returns exit code) — easy to unit test\n- The `runDoctor()` / `printDoctorReport()` split allows CI scripts to capture structured results without parsing terminal output",
        "testStrategy": "### Unit Tests — `test/doctor.test.ts`\n\nUse `node:fs/promises` and `os.tmpdir()` to create isolated temp directories per test:\n\n```typescript\nimport { describe, it, expect, beforeEach, afterEach } from 'vitest';\nimport os from 'node:os';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\n```\n\n#### 1. `checkMcpConfig` Tests\n- **Valid JSON** — write a valid `.mcp.json` and `.vscode/mcp.json` → both results are `pass`\n- **Invalid JSON** — write `{broken` to `.mcp.json` → result contains `error` for that file\n- **Missing file** — no `.vscode/mcp.json` → result contains `warn` for that file\n- **API key check** — configure taskmaster in `.mcp.json`, unset `ANTHROPIC_API_KEY` in `process.env` → result contains `warn` for missing key\n\n#### 2. `checkAgentInstructions` Tests\n- **Both present, short** — write `CLAUDE.md` (10 lines) and `CLAUDE_MCP.md` → both `pass`\n- **Missing CLAUDE.md** — no file → `error` result\n- **Over 200 lines** — write `CLAUDE.md` with 201 lines → `warn` about length\n- **Broken @import** — `CLAUDE.md` contains `@docs/missing.md`, file absent → `error` result\n- **Valid @import** — `CLAUDE.md` references `.taskmaster/CLAUDE.md`, file present → `pass`\n\n#### 3. `checkRulesAndSkills` Tests\n- **Rules present, no broken imports** → `pass` with count\n- **Missing .claude/rules/** directory → `warn`\n- **Rule with broken @import** → `warn` for that import\n\n#### 4. `checkHooks` Tests\n- **Hook script exists and executable** → both `pass` results\n- **Hook script not executable** → `error` result (uses real `fs.chmod(path, 0o644)`)\n- **No hooks directory** → `warn`\n- **Invalid settings.json** → `error` for settings\n\n#### 5. `checkTaskTracker` Tests\n- **Valid tasks.json with tasks** → `pass` with task count\n- **Invalid tasks.json JSON** → `error`\n- **Only TASKS.md** → `pass` for markdown tracker\n- **Nothing** → `warn` for no tracker\n\n#### 6. `checkDocumentation` Tests\n- **docs/ with clean files** → `pass` with file count\n- **docs/prd.md with `{{PROJECT_NAME}}`** → `warn` for unfilled placeholders\n- **No docs/** → `warn`\n\n#### 7. `checkDependencies` Tests\n- Mock `commandExists` (or use vi.mock on utils) to return `true` for `npx`, `false` for `claude` → correct mix of `pass`/`warn`\n\n#### 8. `printDoctorReport` Tests\n- **All pass** → returns exit code `0`\n- **One error** → returns exit code `1`\n- **Only warnings** → returns exit code `0`\n- Output contains category headers (check via console spy)\n\n#### 9. Integration — `runDoctor` Smoke Test\n- Create a minimal valid project structure (CLAUDE.md, .mcp.json, TASKS.md) → `runDoctor()` resolves to array of 7 `HealthCheck` objects, no unhandled errors\n\n#### 10. Helper Tests (in `test/utils.test.ts` or alongside doctor tests)\n- `fileExists()` — returns true for existing file, false for missing\n- `isExecutable()` — returns true after `chmod 755`, false after `chmod 644`\n- `isValidJson()` — returns true for `{}`, false for `{broken`\n\n### CLI Integration Test\nRun `node dist/cli.js doctor` in a temp directory → exits with code 0 or 1 (never throws), output contains \"Health Check\" header and \"Summary:\" line.\n\n### Pre-Completion Checklist\n1. `npm run format` — no diff\n2. `npm run lint` — zero errors\n3. `npm run typecheck` — zero errors\n4. `npm run build` — `dist/cli.js` present, zero errors\n5. `npm test` — all tests pass including new `test/doctor.test.ts`",
        "status": "pending",
        "dependencies": [
          "3",
          "7",
          "8",
          "10",
          "15",
          "20"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": "30",
        "title": "Implement Preset/Profile System — Save & Share Wizard Configs (F18)",
        "description": "Create `src/presets.ts` to implement a preset/profile system that lets users save wizard configurations as named JSON files in `~/.ai-dev-setup/presets/`, apply them via `ai-init --preset=<name>`, and manage them with `ai-init presets` subcommands. Ships three built-in presets (`minimal`, `standard`, `full`) and integrates a preset picker as the first wizard step when user presets exist.",
        "details": "## Overview\n\nImplements F18 as specified in `docs/features/F18-preset-profile-system.md`. Introduces `src/presets.ts` as a new module, extends `src/types.ts` with the `Preset` interface, modifies `src/cli.ts` to add `--preset`, `--save-preset` flags and `presets` subcommand, updates `src/wizard.ts` to show a preset picker as step 0, updates `src/defaults.ts` to export built-in presets, and updates `install.sh` to create the presets directory.\n\n---\n\n## 1. Update `src/types.ts` — Add `Preset` Interface\n\n```typescript\n/** A named, saveable wizard configuration */\nexport interface Preset {\n  /** Unique identifier used on CLI (e.g. \"fullstack\", \"minimal\") */\n  name: string;\n  /** Human-readable description shown in the wizard picker */\n  description: string;\n  /** Partial ProjectConfig — fields stored in the preset */\n  config: PresetConfig;\n}\n\n/**\n * The subset of ProjectConfig fields that are saveable in a preset.\n * Does NOT include runtime fields like projectRoot, projectName, generatedFiles.\n */\nexport interface PresetConfig {\n  selectedMcps: string[];\n  taskTracker: TaskTracker;\n  architecture: Architecture;\n  selectedRules?: string[];       // from F13 — may be undefined if not yet implemented\n  selectedSkills?: string[];      // from F13\n  selectedHookSteps?: string[];   // from F13\n  pm?: string;                    // from F15 — may be undefined\n  agentTeamsEnabled: boolean;\n  generateDocs: boolean;\n  generateRules: boolean;\n  generateSkills: boolean;\n  generateHooks: boolean;\n  generateCommands: boolean;\n  hasApiDocs: boolean;\n  hasDatabase: boolean;\n}\n```\n\n---\n\n## 2. Create `src/presets.ts` — Core Preset Module\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport os from 'node:os';\nimport { Preset, PresetConfig, ProjectConfig } from './types.js';\n\nexport const PRESETS_DIR = path.join(os.homedir(), '.ai-dev-setup', 'presets');\n\n// ─── Built-in presets (embedded, not read from disk) ────────────────────────\n\nexport const BUILTIN_PRESETS: Preset[] = [\n  {\n    name: 'minimal',\n    description: 'Quick start — Taskmaster + markdown tracker, minimal rules',\n    config: {\n      selectedMcps: ['taskmaster'],\n      taskTracker: 'markdown',\n      architecture: 'skip',\n      agentTeamsEnabled: false,\n      generateDocs: true,\n      generateRules: true,\n      generateSkills: false,\n      generateHooks: false,\n      generateCommands: true,\n      hasApiDocs: false,\n      hasDatabase: false,\n      selectedRules: ['general', 'git'],\n      selectedSkills: [],\n      selectedHookSteps: [],\n    },\n  },\n  {\n    name: 'standard',\n    description: 'Recommended — Taskmaster + Context7, common rules and skills',\n    config: {\n      selectedMcps: ['taskmaster', 'context7'],\n      taskTracker: 'taskmaster',\n      architecture: 'skip',\n      agentTeamsEnabled: false,\n      generateDocs: true,\n      generateRules: true,\n      generateSkills: true,\n      generateHooks: true,\n      generateCommands: true,\n      hasApiDocs: false,\n      hasDatabase: false,\n      selectedRules: ['general', 'testing', 'git', 'security', 'config'],\n      selectedSkills: ['testing', 'commit', 'task-workflow'],\n      selectedHookSteps: ['format', 'lint', 'typecheck', 'build', 'test'],\n    },\n  },\n  {\n    name: 'full',\n    description: 'Everything enabled — all MCPs, all rules, agent teams',\n    config: {\n      selectedMcps: ['taskmaster', 'context7', 'browsermcp', 'beads', 'sequential-thinking'],\n      taskTracker: 'taskmaster',\n      architecture: 'skip',\n      agentTeamsEnabled: true,\n      generateDocs: true,\n      generateRules: true,\n      generateSkills: true,\n      generateHooks: true,\n      generateCommands: true,\n      hasApiDocs: true,\n      hasDatabase: true,\n      selectedRules: ['general', 'testing', 'git', 'security', 'api', 'database', 'config', 'docs'],\n      selectedSkills: ['testing', 'commit', 'task-workflow'],\n      selectedHookSteps: ['format', 'lint', 'typecheck', 'build', 'test'],\n    },\n  },\n];\n\n// ─── Directory management ────────────────────────────────────────────────────\n\nexport async function ensurePresetsDir(): Promise<void> {\n  await fs.mkdir(PRESETS_DIR, { recursive: true });\n}\n\n// ─── Load / Save ─────────────────────────────────────────────────────────────\n\nexport async function loadPreset(name: string): Promise<Preset | null> {\n  // Check built-ins first\n  const builtin = BUILTIN_PRESETS.find((p) => p.name === name);\n  if (builtin) return builtin;\n\n  // Fall back to user-saved presets on disk\n  const filePath = path.join(PRESETS_DIR, `${name}.json`);\n  try {\n    const raw = await fs.readFile(filePath, 'utf8');\n    return JSON.parse(raw) as Preset;\n  } catch {\n    return null;\n  }\n}\n\nexport async function savePreset(name: string, config: ProjectConfig, description = ''): Promise<void> {\n  await ensurePresetsDir();\n  const preset: Preset = {\n    name,\n    description,\n    config: extractPresetConfig(config),\n  };\n  const filePath = path.join(PRESETS_DIR, `${name}.json`);\n  await fs.writeFile(filePath, JSON.stringify(preset, null, 2), 'utf8');\n}\n\n// ─── List ─────────────────────────────────────────────────────────────────────\n\nexport async function listPresets(): Promise<Preset[]> {\n  const userPresets = await loadUserPresets();\n  // Merge: user presets shadow built-ins with same name\n  const userNames = new Set(userPresets.map((p) => p.name));\n  const builtins = BUILTIN_PRESETS.filter((p) => !userNames.has(p.name));\n  return [...builtins, ...userPresets];\n}\n\nasync function loadUserPresets(): Promise<Preset[]> {\n  try {\n    const files = await fs.readdir(PRESETS_DIR);\n    const presets: Preset[] = [];\n    for (const file of files.filter((f) => f.endsWith('.json'))) {\n      try {\n        const raw = await fs.readFile(path.join(PRESETS_DIR, file), 'utf8');\n        presets.push(JSON.parse(raw) as Preset);\n      } catch {\n        // Skip malformed files\n      }\n    }\n    return presets;\n  } catch {\n    return [];\n  }\n}\n\n// ─── Export / Import ──────────────────────────────────────────────────────────\n\nexport async function exportPreset(name: string): Promise<string> {\n  const preset = await loadPreset(name);\n  if (!preset) throw new Error(`Preset \"${name}\" not found.`);\n  return JSON.stringify(preset, null, 2);\n}\n\nexport async function importPreset(filePath: string): Promise<Preset> {\n  const raw = await fs.readFile(filePath, 'utf8');\n  const preset = JSON.parse(raw) as Preset;\n  if (!preset.name || !preset.config) {\n    throw new Error('Invalid preset file: missing \"name\" or \"config\" fields.');\n  }\n  await ensurePresetsDir();\n  await fs.writeFile(path.join(PRESETS_DIR, `${preset.name}.json`), raw, 'utf8');\n  return preset;\n}\n\n// ─── Apply preset → ProjectConfig ─────────────────────────────────────────────\n\nexport function applyPreset(preset: Preset, base: ProjectConfig): ProjectConfig {\n  return { ...base, ...preset.config };\n}\n\n// ─── Extract saveable fields from ProjectConfig ───────────────────────────────\n\nfunction extractPresetConfig(config: ProjectConfig): PresetConfig {\n  return {\n    selectedMcps: config.selectedMcps,\n    taskTracker: config.taskTracker,\n    architecture: config.architecture,\n    agentTeamsEnabled: config.agentTeamsEnabled,\n    generateDocs: config.generateDocs,\n    generateRules: config.generateRules,\n    generateSkills: config.generateSkills,\n    generateHooks: config.generateHooks,\n    generateCommands: config.generateCommands,\n    hasApiDocs: config.hasApiDocs,\n    hasDatabase: config.hasDatabase,\n    // F13 fields (optional — may not exist yet)\n    selectedRules: (config as any).selectedRules,\n    selectedSkills: (config as any).selectedSkills,\n    selectedHookSteps: (config as any).selectedHookSteps,\n    // F15 field (optional)\n    pm: (config as any).pm,\n  };\n}\n```\n\n---\n\n## 3. Update `src/wizard.ts` — Preset Picker as Step 0\n\nAt the top of `runWizard()`, before the existing Step 0 (Claude Code check), add a preset selection step:\n\n```typescript\nimport { listPresets, applyPreset, BUILTIN_PRESETS } from './presets.js';\n\n// In runWizard():\n// NEW: Preset picker (only shown if user presets exist OR always show builtins)\nconst allPresets = await listPresets();\n\nif (!NON_INTERACTIVE && allPresets.length > 0) {\n  const choices = [\n    ...allPresets.map((p) => ({\n      name: `${p.name.padEnd(20)} ${p.description}`,\n      value: p.name,\n    })),\n    { name: 'Configure manually      Step-by-step wizard', value: '__manual__' },\n  ];\n\n  const presetChoice = await select({\n    message: 'Start from a preset or configure manually?',\n    choices,\n  });\n\n  if (presetChoice !== '__manual__') {\n    const preset = allPresets.find((p) => p.name === presetChoice)!;\n    return applyPreset(preset, { ...config, projectName, projectRoot });\n  }\n}\n// Otherwise fall through to the existing 10-step wizard\n```\n\n---\n\n## 4. Update `src/cli.ts` — New Flags and `presets` Subcommand\n\nAdd to the meow configuration:\n\n```typescript\nconst cli = meow(helpText, {\n  importMeta: import.meta,\n  flags: {\n    // ... existing flags ...\n    preset: { type: 'string', shortFlag: 'p' },\n    savePreset: { type: 'string' },\n  },\n});\n```\n\nAdd handling logic:\n\n```typescript\n// ai-init presets [--list] [--export=name] [--import=file]\nif (cli.input[0] === 'presets') {\n  const { export: exportName, import: importFile } = cli.flags as any;\n  if (exportName) {\n    const json = await exportPreset(exportName);\n    process.stdout.write(json + '\\n');\n  } else if (importFile) {\n    const p = await importPreset(importFile);\n    console.log(`Imported preset \"${p.name}\" to ${PRESETS_DIR}.`);\n  } else {\n    // List\n    const presets = await listPresets();\n    console.log('Available presets:\\n');\n    for (const p of presets) {\n      console.log(`  ${p.name.padEnd(20)} ${p.description}`);\n    }\n  }\n  process.exit(0);\n}\n\n// --preset=name: bypass wizard, apply preset directly\nif (cli.flags.preset) {\n  const preset = await loadPreset(cli.flags.preset);\n  if (!preset) {\n    console.error(`Error: preset \"${cli.flags.preset}\" not found. Run \"ai-init presets\" to list available presets.`);\n    process.exit(1);\n  }\n  const base = defaultConfig(process.cwd());\n  config = applyPreset(preset, { ...base, projectName, projectRoot });\n  // Skip wizard, proceed to file generation\n}\n\n// --save-preset=name: run wizard then save result\n// (After wizard returns config, before file generation)\nif (cli.flags.savePreset) {\n  await savePreset(cli.flags.savePreset, config);\n  console.log(`Preset \"${cli.flags.savePreset}\" saved to ${PRESETS_DIR}.`);\n}\n```\n\n---\n\n## 5. Update `src/defaults.ts` — Re-export Built-in Presets\n\nAdd a re-export so `defaults.ts` remains the single source for default values:\n\n```typescript\nexport { BUILTIN_PRESETS } from './presets.js';\n```\n\n---\n\n## 6. Update `install.sh` — Create Presets Directory\n\nAfter creating `~/.local/bin`, add:\n\n```bash\n# Create presets directory\nPRESETS_DIR=\"${HOME}/.ai-dev-setup/presets\"\nmkdir -p \"${PRESETS_DIR}\"\necho \"${PASS} Created presets directory: ${PRESETS_DIR}\"\n```\n\n---\n\n## Architecture Notes\n\n- **Built-in presets are embedded** in `src/presets.ts` as TypeScript constants — no file I/O needed for defaults.\n- **User presets shadow built-ins** with the same name, allowing customization of `minimal`/`standard`/`full`.\n- **`PresetConfig` is a stable subset** of `ProjectConfig` — excludes runtime fields (`projectRoot`, `projectName`, `generatedFiles`, `prdPath`, `hasPrd`, `runAudit`) that don't make sense to persist.\n- **Optional F13/F15 fields** are handled with `(config as any)` casts until those tasks are complete; once done, add them properly to `ProjectConfig` and `PresetConfig`.\n- **`listPresets()` always returns built-ins** so the wizard always offers a quick-start option even for first-time users.\n",
        "testStrategy": "### Test file: `test/presets.test.ts`\n\nUse `node:fs/promises`, `os.tmpdir()`, and Vitest to create isolated temp directories per test. Override `PRESETS_DIR` by dependency injection or by monkey-patching the module-level constant via a test helper.\n\n#### 1. Built-in Preset Assertions\n\n```typescript\nimport { BUILTIN_PRESETS } from '../src/presets.js';\n\nit('ships exactly 3 built-in presets', () => {\n  expect(BUILTIN_PRESETS).toHaveLength(3);\n  const names = BUILTIN_PRESETS.map((p) => p.name);\n  expect(names).toContain('minimal');\n  expect(names).toContain('standard');\n  expect(names).toContain('full');\n});\n\nit('minimal preset has only taskmaster MCP', () => {\n  const minimal = BUILTIN_PRESETS.find((p) => p.name === 'minimal')!;\n  expect(minimal.config.selectedMcps).toEqual(['taskmaster']);\n});\n\nit('full preset has agentTeamsEnabled true', () => {\n  const full = BUILTIN_PRESETS.find((p) => p.name === 'full')!;\n  expect(full.config.agentTeamsEnabled).toBe(true);\n});\n```\n\n#### 2. Save & Load (User Presets)\n\n```typescript\nit('savePreset writes JSON file to disk', async () => {\n  const dir = await fs.mkdtemp(path.join(os.tmpdir(), 'presets-'));\n  // Point PRESETS_DIR to temp dir via module-level override in test setup\n  await savePreset('my-preset', mockConfig, 'My test preset', dir);\n  const raw = await fs.readFile(path.join(dir, 'my-preset.json'), 'utf8');\n  const parsed = JSON.parse(raw);\n  expect(parsed.name).toBe('my-preset');\n  expect(parsed.config.selectedMcps).toEqual(mockConfig.selectedMcps);\n});\n\nit('loadPreset returns null for unknown preset name', async () => {\n  const result = await loadPreset('nonexistent', '/tmp/empty-presets-dir');\n  expect(result).toBeNull();\n});\n\nit('loadPreset returns built-in without disk access', async () => {\n  const result = await loadPreset('standard');\n  expect(result).not.toBeNull();\n  expect(result!.name).toBe('standard');\n});\n\nit('user preset shadows built-in with same name', async () => {\n  const dir = await fs.mkdtemp(path.join(os.tmpdir(), 'presets-'));\n  const customMinimal = { name: 'minimal', description: 'Custom minimal', config: { ...mockPresetConfig } };\n  await fs.writeFile(path.join(dir, 'minimal.json'), JSON.stringify(customMinimal));\n  const presets = await listPresets(dir);\n  const minimal = presets.find((p) => p.name === 'minimal')!;\n  expect(minimal.description).toBe('Custom minimal');\n});\n```\n\n#### 3. List\n\n```typescript\nit('listPresets includes all 3 built-ins for empty dir', async () => {\n  const dir = await fs.mkdtemp(path.join(os.tmpdir(), 'presets-'));\n  const presets = await listPresets(dir);\n  expect(presets.map((p) => p.name)).toContain('minimal');\n  expect(presets.map((p) => p.name)).toContain('standard');\n  expect(presets.map((p) => p.name)).toContain('full');\n});\n\nit('listPresets includes user presets alongside built-ins', async () => {\n  const dir = await fs.mkdtemp(path.join(os.tmpdir(), 'presets-'));\n  await fs.writeFile(path.join(dir, 'custom.json'), JSON.stringify({ name: 'custom', description: 'x', config: mockPresetConfig }));\n  const presets = await listPresets(dir);\n  const names = presets.map((p) => p.name);\n  expect(names).toContain('custom');\n  expect(names).toContain('standard');\n});\n\nit('listPresets skips malformed JSON files without throwing', async () => {\n  const dir = await fs.mkdtemp(path.join(os.tmpdir(), 'presets-'));\n  await fs.writeFile(path.join(dir, 'bad.json'), 'not json{{{');\n  const presets = await listPresets(dir);\n  expect(presets.every((p) => p.name !== 'bad')).toBe(true);\n});\n```\n\n#### 4. Export / Import\n\n```typescript\nit('exportPreset returns valid JSON string for built-in', async () => {\n  const json = await exportPreset('minimal');\n  const parsed = JSON.parse(json);\n  expect(parsed.name).toBe('minimal');\n});\n\nit('exportPreset throws for unknown preset', async () => {\n  await expect(exportPreset('no-such-preset')).rejects.toThrow('\"no-such-preset\" not found');\n});\n\nit('importPreset writes file to presets dir and returns preset', async () => {\n  const dir = await fs.mkdtemp(path.join(os.tmpdir(), 'presets-'));\n  const srcFile = path.join(dir, 'team-acme.json');\n  const preset = { name: 'team-acme', description: 'Team preset', config: mockPresetConfig };\n  await fs.writeFile(srcFile, JSON.stringify(preset));\n  const imported = await importPreset(srcFile, dir);\n  expect(imported.name).toBe('team-acme');\n  const saved = await fs.readFile(path.join(dir, 'team-acme.json'), 'utf8');\n  expect(JSON.parse(saved).name).toBe('team-acme');\n});\n\nit('importPreset throws on invalid preset file structure', async () => {\n  const dir = await fs.mkdtemp(path.join(os.tmpdir(), 'presets-'));\n  const badFile = path.join(dir, 'bad.json');\n  await fs.writeFile(badFile, JSON.stringify({ description: 'missing name and config' }));\n  await expect(importPreset(badFile, dir)).rejects.toThrow('Invalid preset file');\n});\n```\n\n#### 5. applyPreset\n\n```typescript\nit('applyPreset merges preset config onto base ProjectConfig', () => {\n  const base = defaultConfig('/tmp/proj');\n  const preset = BUILTIN_PRESETS.find((p) => p.name === 'full')!;\n  const result = applyPreset(preset, base);\n  expect(result.agentTeamsEnabled).toBe(true);\n  expect(result.selectedMcps).toContain('browsermcp');\n  // Runtime fields preserved\n  expect(result.projectRoot).toBe('/tmp/proj');\n});\n```\n\n#### 6. CLI Integration (manual verification)\n\n```bash\nnpm run build\n\n# List presets — should show minimal, standard, full\nnode dist/cli.js presets\n\n# Apply built-in preset — should skip wizard, generate files\nnode dist/cli.js --preset=standard\n\n# Save after wizard run (run wizard, save result)\nnode dist/cli.js --save-preset=my-config\n\n# Export preset to stdout\nnode dist/cli.js presets --export=standard\n\n# Import exported preset\nnode dist/cli.js presets --export=standard > /tmp/standard.json\nnode dist/cli.js presets --import=/tmp/standard.json\nnode dist/cli.js presets  # should show standard listed twice (built-in + user)\n```\n\n#### 7. Mandatory Quality Gate\n\n```bash\nnpm run format       # Prettier — zero changes\nnpm run lint         # ESLint — zero errors\nnpm run typecheck    # tsc --noEmit — zero errors\nnpm run build        # tsc — exits 0, dist/ updated\nnpm test             # vitest run — all tests green including new presets.test.ts\n```",
        "status": "pending",
        "dependencies": [
          "14",
          "17"
        ],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": "31",
        "title": "Polyglot Language Support Architecture (F19)",
        "description": "Introduce language and toolchain detection so generated hooks, rules, and docs use the correct commands for Node.js, Python, Go, and Rust projects. Implement a `ToolChain` interface, a detection layer that reads `pyproject.toml`/`go.mod`/`Cargo.toml`, per-language template variants, and update all generators to use toolchain-aware values instead of hardcoded Node.js/npm commands.",
        "details": "## Overview\n\nThis task implements F19 as specified in `docs/features/F19-polyglot-language-support.md`. The current codebase hardcodes Node.js/npm tooling throughout generators and templates. The goal is to make every generated file language-aware.\n\n---\n\n## Phase 1 — Type Definitions (`src/types.ts`)\n\nAdd `Language` and `ToolChain` types, and a `toolchain` field to `ProjectConfig`:\n\n```typescript\nexport type Language = \"node\" | \"python\" | \"go\" | \"rust\" | \"unknown\";\n\nexport interface ToolChain {\n  language: Language;\n  /** Format command, e.g. \"npx prettier --write .\" | \"black .\" | \"gofmt -w .\" | \"cargo fmt\" */\n  format: string;\n  /** Lint command, e.g. \"npx eslint . --fix\" | \"ruff check --fix .\" | \"golangci-lint run\" | \"cargo clippy\" */\n  lint: string;\n  /** Type-check command or empty string for Go/Rust (type safety is compile-time) */\n  typecheck: string;\n  /** Build command, e.g. \"npm run build\" | \"python -m build\" | \"go build ./...\" | \"cargo build\" */\n  build: string;\n  /** Test command, e.g. \"npm test\" | \"pytest\" | \"go test ./...\" | \"cargo test\" */\n  test: string;\n}\n\n// In ProjectConfig, add:\n//   toolchain: ToolChain;\n```\n\n---\n\n## Phase 2 — Detection Layer (`src/utils.ts`)\n\nAdd `detectLanguage(projectRoot: string): Promise<Language>` and `buildToolChain(language: Language, pm?: PackageManager): ToolChain`.\n\n**Detection heuristics (in priority order):**\n1. `Cargo.toml` present → `\"rust\"`\n2. `go.mod` present → `\"go\"`\n3. `pyproject.toml`, `requirements.txt`, or `setup.py` present → `\"python\"`\n4. `package.json` or `tsconfig.json` present → `\"node\"`\n5. Fallback → `\"unknown\"` (treat as `\"node\"` for command generation)\n\n**When multiple indicators exist**, prefer the most specific: `Cargo.toml` + `package.json` → prompt user (or accept via `SETUP_AI_LANGUAGE` env var in non-interactive mode).\n\n```typescript\nimport { existsSync } from 'node:fs';\nimport path from 'node:path';\n\nexport async function detectLanguage(projectRoot: string): Promise<Language> {\n  const has = (f: string) => existsSync(path.join(projectRoot, f));\n  if (has('Cargo.toml')) return 'rust';\n  if (has('go.mod')) return 'go';\n  if (has('pyproject.toml') || has('requirements.txt') || has('setup.py')) return 'python';\n  if (has('package.json') || has('tsconfig.json')) return 'node';\n  return 'unknown';\n}\n\nexport function buildToolChain(language: Language): ToolChain {\n  switch (language) {\n    case 'python':\n      return {\n        language,\n        format: 'black .',\n        lint: 'ruff check --fix .',\n        typecheck: 'mypy .',\n        build: 'python -m build',\n        test: 'pytest',\n      };\n    case 'go':\n      return {\n        language,\n        format: 'gofmt -w .',\n        lint: 'golangci-lint run',\n        typecheck: '',\n        build: 'go build ./...',\n        test: 'go test ./...',\n      };\n    case 'rust':\n      return {\n        language,\n        format: 'cargo fmt',\n        lint: 'cargo clippy',\n        typecheck: '',\n        build: 'cargo build',\n        test: 'cargo test',\n      };\n    case 'node':\n    default:\n      return {\n        language: 'node',\n        format: 'npx prettier --write .',\n        lint: 'npx eslint . --fix',\n        typecheck: 'npx tsc --noEmit',\n        build: 'npm run build',\n        test: 'npm test',\n      };\n  }\n}\n```\n\n---\n\n## Phase 3 — Wizard Step (`src/wizard.ts`)\n\nAdd `stepLanguageDetection()` **early in the wizard sequence** (after step 0, before MCP selection):\n\n```typescript\nasync function stepLanguageDetection(projectRoot: string): Promise<Language> {\n  const detected = await detectLanguage(projectRoot);\n\n  // Non-interactive override\n  const envLang = process.env.SETUP_AI_LANGUAGE as Language | undefined;\n  if (NON_INTERACTIVE || envLang) return envLang ?? detected;\n\n  // If detection is confident, confirm; otherwise prompt\n  const choices: { name: string; value: Language }[] = [\n    { name: 'Node.js / TypeScript', value: 'node' },\n    { name: 'Python', value: 'python' },\n    { name: 'Go', value: 'go' },\n    { name: 'Rust', value: 'rust' },\n    { name: 'Unknown / Other', value: 'unknown' },\n  ];\n\n  return select({\n    message: `Primary language detected as \"${detected}\". Confirm or change:`,\n    default: detected,\n    choices,\n  });\n}\n```\n\nSet `config.toolchain = buildToolChain(language)` in `runWizard()` before returning config.\n\nAlso update `defaultConfig()` in `src/defaults.ts` to include a default `toolchain`:\n\n```typescript\ntoolchain: buildToolChain('node'),\n```\n\n---\n\n## Phase 4 — Per-Language Template Variants (`templates/hooks/`)\n\nCreate language-specific pre-commit hook templates. The existing `templates/hooks/pre-commit.sh` becomes the Node.js variant.\n\n**`templates/hooks/pre-commit.node.sh`** — identical to current `pre-commit.sh` (uses `npm run format`, `npm run lint`, etc.)\n\n**`templates/hooks/pre-commit.python.sh`**:\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\necho \"[pre-commit] Formatting with black...\"\nblack . || exit 1\necho \"[pre-commit] Linting with ruff...\"\nruff check --fix . || exit 1\necho \"[pre-commit] Type checking with mypy...\"\nmypy . || exit 1\necho \"[pre-commit] Running tests...\"\npytest || exit 1\n```\n\n**`templates/hooks/pre-commit.go.sh`**:\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\necho \"[pre-commit] Formatting with gofmt...\"\ngofmt -w . || exit 1\necho \"[pre-commit] Linting with golangci-lint...\"\ngolangci-lint run || exit 1\necho \"[pre-commit] Building...\"\ngo build ./... || exit 1\necho \"[pre-commit] Running tests...\"\ngo test ./... || exit 1\n```\n\n**`templates/hooks/pre-commit.rust.sh`**:\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\necho \"[pre-commit] Formatting with cargo fmt...\"\ncargo fmt || exit 1\necho \"[pre-commit] Linting with clippy...\"\ncargo clippy || exit 1\necho \"[pre-commit] Building...\"\ncargo build || exit 1\necho \"[pre-commit] Running tests...\"\ncargo test || exit 1\n```\n\n---\n\n## Phase 5 — Update Generators\n\n### `src/generators/hooks.ts`\nChange template selection based on `config.toolchain.language`:\n```typescript\nconst templateName = `templates/hooks/pre-commit.${config.toolchain.language}.sh`;\n// Fallback: pre-commit.node.sh if language-specific template is missing\n```\n\n### `src/generators/claude-md.ts`\nUpdate `buildQualityGate()` to use `toolchain` values instead of hardcoded npm commands:\n```typescript\nfunction buildQualityGate(toolchain: ToolChain): string {\n  const steps = [\n    toolchain.format && `1. **Format**: \\`${toolchain.format}\\``,\n    toolchain.lint   && `2. **Lint**: \\`${toolchain.lint}\\``,\n    toolchain.typecheck && `3. **Type-check**: \\`${toolchain.typecheck}\\``,\n    toolchain.build  && `4. **Build**: \\`${toolchain.build}\\``,\n    toolchain.test   && `5. **Test**: \\`${toolchain.test}\\``,\n  ].filter(Boolean);\n  return steps.join('\\n');\n}\n```\n\n### `src/generators/rules.ts`\nPass `{{LANGUAGE}}`, `{{FORMAT_CMD}}`, `{{LINT_CMD}}`, `{{BUILD_CMD}}`, `{{TEST_CMD}}` to `fillTemplate()` for all rule files.\n\n### `src/generators/docs.ts`\nUpdate onboarding template substitutions to use `toolchain.test`, `toolchain.build`, etc.\n\n---\n\n## Phase 6 — Template Variable Substitution\n\nAdd toolchain placeholders to `fillTemplate()` calls throughout all generators:\n\n```typescript\nconst toolchainVars = {\n  LANGUAGE: config.toolchain.language,\n  FORMAT_CMD: config.toolchain.format,\n  LINT_CMD: config.toolchain.lint,\n  TYPECHECK_CMD: config.toolchain.typecheck,\n  BUILD_CMD: config.toolchain.build,\n  TEST_CMD: config.toolchain.test,\n};\n```\n\nUpdate relevant template files (`templates/rules/testing.md`, `templates/rules/general.md`, `templates/docs/onboarding.md`) to use these placeholders where Node.js commands are currently hardcoded.\n\n---\n\n## Files Modified\n\n- `src/types.ts` — Add `Language`, `ToolChain` types; add `toolchain` to `ProjectConfig`\n- `src/utils.ts` — Add `detectLanguage()` and `buildToolChain()`\n- `src/defaults.ts` — Add default `toolchain: buildToolChain('node')`\n- `src/wizard.ts` — Add `stepLanguageDetection()` step\n- `src/generators/hooks.ts` — Language-aware template selection\n- `src/generators/claude-md.ts` — Toolchain-driven quality gate\n- `src/generators/rules.ts` — Pass toolchain template vars\n- `src/generators/docs.ts` — Pass toolchain template vars\n- `templates/hooks/pre-commit.node.sh` — Rename from `pre-commit.sh`\n- `templates/hooks/pre-commit.python.sh` — New\n- `templates/hooks/pre-commit.go.sh` — New\n- `templates/hooks/pre-commit.rust.sh` — New\n- `templates/rules/testing.md` — Replace hardcoded `npm test` with `{{TEST_CMD}}`\n- `templates/rules/general.md` — Replace hardcoded npm commands with placeholders\n- `templates/docs/onboarding.md` — Replace hardcoded npm commands with placeholders\n",
        "testStrategy": "### Unit Tests: `test/utils/language-detection.test.ts`\n\n1. **`detectLanguage()` — detection by file presence:**\n   - Create temp dir with `Cargo.toml` → returns `\"rust\"`\n   - Create temp dir with `go.mod` → returns `\"go\"`\n   - Create temp dir with `pyproject.toml` → returns `\"python\"`\n   - Create temp dir with `requirements.txt` → returns `\"python\"`\n   - Create temp dir with `package.json` → returns `\"node\"`\n   - Create temp dir with `tsconfig.json` → returns `\"node\"`\n   - Empty temp dir → returns `\"unknown\"`\n\n2. **Priority ordering:**\n   - Dir with both `Cargo.toml` and `package.json` → returns `\"rust\"` (Rust takes priority)\n   - Dir with both `go.mod` and `package.json` → returns `\"go\"`\n   - Dir with both `pyproject.toml` and `package.json` → returns `\"python\"`\n\n3. **`buildToolChain()` — correct commands per language:**\n   - `buildToolChain(\"rust\").test === \"cargo test\"`\n   - `buildToolChain(\"go\").build === \"go build ./...\"`\n   - `buildToolChain(\"python\").lint === \"ruff check --fix .\"`\n   - `buildToolChain(\"node\").format === \"npx prettier --write .\"`\n   - `buildToolChain(\"go\").typecheck === \"\"` (empty — Go has no separate typecheck step)\n   - `buildToolChain(\"unknown\")` returns same as `\"node\"`\n\n### Unit Tests: `test/generators/hooks-polyglot.test.ts`\n\n4. **Language-specific hook template selection:**\n   - `generateHooks({ toolchain: buildToolChain(\"python\"), ... })` → generated hook content contains `pytest`\n   - `generateHooks({ toolchain: buildToolChain(\"go\"), ... })` → generated hook content contains `go test`\n   - `generateHooks({ toolchain: buildToolChain(\"rust\"), ... })` → generated hook content contains `cargo test`\n   - `generateHooks({ toolchain: buildToolChain(\"node\"), ... })` → generated hook content contains `npm test`\n\n### Unit Tests: `test/generators/claude-md-polyglot.test.ts`\n\n5. **Quality gate uses toolchain values:**\n   - `generateClaudeMd({ toolchain: buildToolChain(\"python\"), ... })` → CLAUDE.md contains `pytest`, `ruff`, `black`\n   - `generateClaudeMd({ toolchain: buildToolChain(\"go\"), ... })` → CLAUDE.md contains `go test`, `gofmt`\n   - `generateClaudeMd({ toolchain: buildToolChain(\"rust\"), ... })` → CLAUDE.md contains `cargo test`, `cargo fmt`\n   - `generateClaudeMd({ toolchain: buildToolChain(\"node\"), ... })` → CLAUDE.md still contains `npm test` (no regression)\n   - CLAUDE.md for Go does NOT contain empty typecheck line (skips falsy steps)\n\n### Integration Tests: `test/integration/polyglot.test.ts`\n\n6. **End-to-end in temp directories:**\n   - Create temp dir with `go.mod`, run `defaultConfig()` + `detectLanguage()`, assert `toolchain.language === \"go\"`\n   - Run `generateHooks(config)`, write files, assert `.claude/hooks/pre-commit.sh` contains `go test ./...`\n   - Create temp dir with `pyproject.toml`, run full pipeline, assert quality gate in CLAUDE.md references `pytest`\n\n### Type Check & Lint\n\n7. `npm run typecheck` — zero TypeScript errors with new `Language`, `ToolChain` types\n8. `npm run lint` — zero ESLint errors across all modified files\n9. `npm run build` — compiles cleanly with new types in place\n10. Existing tests in `test/generators/*.test.ts` still pass (no regressions — Node.js is still the default)\n\n### Template Validation\n\n11. Verify all four pre-commit template files exist: `pre-commit.node.sh`, `pre-commit.python.sh`, `pre-commit.go.sh`, `pre-commit.rust.sh`\n12. Each template must be non-empty and contain the correct toolchain commands for its language\n13. Verify `templates/rules/testing.md` no longer contains literal `npm test` but uses `{{TEST_CMD}}`\n",
        "status": "pending",
        "dependencies": [
          "4",
          "5",
          "8",
          "10",
          "14",
          "27"
        ],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": "32",
        "title": "F20: AI-Powered Project-Aware Configuration — Native Installer, Auth Gate, and Haiku-Based Codebase Analysis",
        "description": "Replace the deprecated npm Claude installation with the native curl-based installer, add an authentication gate with fallback options, add a new-vs-existing project prompt, and for existing projects use `claude --model haiku` in headless mode with `--json-schema` to generate tailored rules/hooks/CLAUDE.md with detected path globs instead of hardcoded defaults. Implements the four-step pattern: detect deterministically → synthesize with Haiku → validate with Zod → confirm with user preview, with graceful degradation at every step.",
        "details": "## Overview\n\nImplements `docs/features/F20-ai-powered-project-aware-config.md`. This is a large feature touching new files and several existing modules. Follow the exact four-step pattern described in the spec.\n\n---\n\n## 1. Install Zod\n\n```bash\nnpm install zod@latest\n```\n\nZod is not currently in `package.json`. Add it as a runtime dependency — it will be used in `src/analyze.ts` to validate Haiku's JSON output.\n\n---\n\n## 2. Update `src/types.ts`\n\nAdd new interfaces and extend `ProjectConfig`:\n\n```typescript\n/** Raw filesystem signals gathered by src/detect.ts */\nexport interface DetectionResult {\n  hasPackageJson: boolean;\n  hasTsConfig: boolean;\n  hasDockerCompose: boolean;\n  hasPrismaSchema: boolean;\n  hasGraphqlConfig: boolean;\n  directories: string[];    // top-level dirs under src/ (or project root)\n  configFiles: string[];    // detected config files\n  frameworks: string[];     // [\"express\", \"next\", \"fastapi\", ...]\n  orms: string[];           // [\"prisma\", \"drizzle\", \"typeorm\", ...]\n  testFrameworks: string[]; // [\"vitest\", \"jest\", \"pytest\", ...]\n}\n\n/** Structured output from claude --model haiku analysis, validated by Zod */\nexport interface ProjectAnalysis {\n  detectedArchitecture: \"monolith\" | \"2-tier\" | \"3-tier\" | \"microservices\";\n  apiPaths: string[];           // e.g. [\"src/routes/**\", \"src/api/**\"]\n  dbPaths: string[];            // e.g. [\"prisma/**\", \"src/db/**\"]\n  testPaths: string[];          // e.g. [\"src/__tests__/**\", \"test/**\"]\n  architectureGuidance: string; // 2-3 sentences for CLAUDE.md\n  recommendedRules: string[];   // subset of KNOWN_RULES enum\n  hookSteps: string[];          // subset of [\"format\",\"lint\",\"typecheck\",\"build\",\"test\"]\n}\n\n// Add to ProjectConfig:\nisExistingProject: boolean;           // from new-vs-existing wizard prompt\nclaudeAuthenticated: boolean;         // result of auth gate check\nanalysisResult?: ProjectAnalysis;     // populated when haiku analysis succeeds\n```\n\n---\n\n## 3. Create `src/detect.ts` — Deterministic Filesystem Detection\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { DetectionResult } from './types.js';\n\nconst KNOWN_FRAMEWORKS = ['express', 'fastify', 'next', 'nuxt', 'remix', 'nestjs', 'hono', 'fastapi', 'flask', 'django'];\nconst KNOWN_ORMS = ['prisma', 'drizzle', 'typeorm', 'sequelize', 'mongoose', 'sqlalchemy'];\nconst KNOWN_TEST_FRAMEWORKS = ['vitest', 'jest', 'mocha', 'pytest', 'unittest'];\n\nexport async function detectProject(projectRoot: string): Promise<DetectionResult> {\n  const check = (file: string) =>\n    fs.access(path.join(projectRoot, file)).then(() => true, () => false);\n\n  const [hasPackageJson, hasTsConfig, hasDockerCompose, hasPrismaSchema, hasGraphqlConfig] =\n    await Promise.all([\n      check('package.json'),\n      check('tsconfig.json'),\n      check('docker-compose.yml').then(r => r || check('docker-compose.yaml')),\n      check('prisma/schema.prisma'),\n      check('graphql.config.ts').then(r => r || check('graphql.config.js') || check('.graphqlrc')),\n    ]);\n\n  // Scan top-level src/ directories (or project root dirs)\n  const srcPath = path.join(projectRoot, 'src');\n  let directories: string[] = [];\n  try {\n    const entries = await fs.readdir(srcPath, { withFileTypes: true });\n    directories = entries.filter(e => e.isDirectory()).map(e => e.name);\n  } catch {\n    // no src/ — try project root top-level dirs\n    const entries = await fs.readdir(projectRoot, { withFileTypes: true });\n    directories = entries\n      .filter(e => e.isDirectory() && !e.name.startsWith('.') && e.name !== 'node_modules' && e.name !== 'dist')\n      .map(e => e.name);\n  }\n\n  // Collect config files\n  const CONFIG_FILES = ['package.json', 'tsconfig.json', '.eslintrc.js', '.prettierrc',\n    'vite.config.ts', 'vitest.config.ts', 'docker-compose.yml', 'Dockerfile',\n    'prisma/schema.prisma', 'pyproject.toml', 'requirements.txt'];\n  const configFiles = (await Promise.all(CONFIG_FILES.map(async f => ((await check(f)) ? f : null))))\n    .filter(Boolean) as string[];\n\n  // Parse package.json for framework/orm/test deps\n  let frameworks: string[] = [];\n  let orms: string[] = [];\n  let testFrameworks: string[] = [];\n  if (hasPackageJson) {\n    try {\n      const pkgRaw = await fs.readFile(path.join(projectRoot, 'package.json'), 'utf8');\n      const pkg = JSON.parse(pkgRaw);\n      const allDeps = Object.keys({ ...pkg.dependencies, ...pkg.devDependencies });\n      frameworks = KNOWN_FRAMEWORKS.filter(f => allDeps.some(d => d === f || d.includes(f)));\n      orms = KNOWN_ORMS.filter(f => allDeps.some(d => d === f || d.includes(f)));\n      testFrameworks = KNOWN_TEST_FRAMEWORKS.filter(f => allDeps.some(d => d === f || d.includes(f)));\n    } catch { /* ignore */ }\n  }\n\n  return {\n    hasPackageJson, hasTsConfig, hasDockerCompose, hasPrismaSchema, hasGraphqlConfig,\n    directories, configFiles, frameworks, orms, testFrameworks,\n  };\n}\n```\n\n**Design principles:**\n- No LLM calls — pure filesystem/JSON reads\n- Runs in ~50ms even for large projects\n- Uses `Promise.all` for parallel filesystem checks\n- Gracefully handles missing `src/` directory\n\n---\n\n## 4. Create `src/analyze.ts` — Haiku Orchestration + Zod Validation\n\n```typescript\nimport { z } from 'zod';\nimport { run } from './utils.js';\nimport { detectProject } from './detect.js';\nimport { DetectionResult, ProjectAnalysis } from './types.js';\n\nconst KNOWN_RULES = ['general', 'api', 'database', 'testing', 'security', 'git', 'config', 'agent-teams'] as const;\nconst KNOWN_HOOK_STEPS = ['format', 'lint', 'typecheck', 'build', 'test'] as const;\n\nconst AnalysisSchema = z.object({\n  detectedArchitecture: z.enum(['monolith', '2-tier', '3-tier', 'microservices']),\n  apiPaths: z.array(z.string()).max(10),\n  dbPaths: z.array(z.string()).max(10),\n  testPaths: z.array(z.string()).max(10),\n  architectureGuidance: z.string().max(500),\n  recommendedRules: z.array(z.enum(KNOWN_RULES)),\n  hookSteps: z.array(z.enum(KNOWN_HOOK_STEPS)),\n});\n\nconst JSON_SCHEMA = {\n  type: 'object',\n  required: ['detectedArchitecture', 'apiPaths', 'dbPaths', 'testPaths', 'architectureGuidance', 'recommendedRules', 'hookSteps'],\n  properties: {\n    detectedArchitecture: { type: 'string', enum: ['monolith', '2-tier', '3-tier', 'microservices'] },\n    apiPaths: { type: 'array', items: { type: 'string' }, maxItems: 10 },\n    dbPaths: { type: 'array', items: { type: 'string' }, maxItems: 10 },\n    testPaths: { type: 'array', items: { type: 'string' }, maxItems: 10 },\n    architectureGuidance: { type: 'string', maxLength: 500 },\n    recommendedRules: { type: 'array', items: { type: 'string', enum: [...KNOWN_RULES] } },\n    hookSteps: { type: 'array', items: { type: 'string', enum: [...KNOWN_HOOK_STEPS] } },\n  },\n};\n\nasync function callHaiku(detection: DetectionResult, projectRoot: string): Promise<unknown> {\n  const prompt = `Given this project structure: ${JSON.stringify(detection, null, 2)}\\n\\nAnalyze the codebase and return structured configuration for AI-assisted development rules, hooks, and documentation.`;\n  const output = await run('claude', [\n    '--model', 'haiku',\n    '-p', prompt,\n    '--output-format', 'json',\n    '--json-schema', JSON.stringify(JSON_SCHEMA),\n    '--max-turns', '1',\n    '--allowedTools', 'Read',\n  ], projectRoot);\n  return JSON.parse(output);\n}\n\nexport async function analyzeProject(projectRoot: string): Promise<ProjectAnalysis | null> {\n  try {\n    const detection = await detectProject(projectRoot);\n    let raw: unknown;\n    try {\n      raw = await callHaiku(detection, projectRoot);\n    } catch {\n      return null; // Haiku call failed — fallback to defaults\n    }\n\n    // First validation attempt\n    const result = AnalysisSchema.safeParse(raw);\n    if (result.success) return result.data;\n\n    // Retry once with validation errors as context\n    try {\n      const retryPrompt = `Previous response failed validation with errors: ${JSON.stringify(result.error.issues)}\\nProject structure: ${JSON.stringify(detection)}\\nReturn valid JSON only.`;\n      const retryRaw = await run('claude', [\n        '--model', 'haiku',\n        '-p', retryPrompt,\n        '--output-format', 'json',\n        '--json-schema', JSON.stringify(JSON_SCHEMA),\n        '--max-turns', '1',\n        '--allowedTools', 'Read',\n      ], projectRoot);\n      const retryResult = AnalysisSchema.safeParse(JSON.parse(retryRaw));\n      if (retryResult.success) return retryResult.data;\n    } catch { /* ignore retry errors */ }\n\n    return null; // Both attempts failed — fallback to defaults\n  } catch {\n    return null; // detectProject failed — fallback to defaults\n  }\n}\n```\n\n---\n\n## 5. Update `src/audit.ts`\n\nReplace `installClaudeCode()` and add `checkClaudeAuthenticated()`:\n\n```typescript\n// Replace:\nexport async function installClaudeCode(): Promise<void> {\n  await run('npm', ['install', '-g', '@anthropic-ai/claude-code']);\n}\n\n// With:\nexport async function installClaudeCode(): Promise<void> {\n  // Use native installer — npm package is deprecated\n  await run('bash', ['-c', 'curl -fsSL https://claude.ai/install.sh | bash']);\n}\n\n// Add new:\nexport async function checkClaudeAuthenticated(): Promise<boolean> {\n  try {\n    const output = await run('claude', ['auth', 'status', '--text']);\n    return output.includes('Login method');\n  } catch {\n    return false;\n  }\n}\n```\n\n---\n\n## 6. Update `src/wizard.ts`\n\n### Step 0.5 — Auth Gate (after Claude install check)\n\n```typescript\nconst isAuthenticated = await checkClaudeAuthenticated();\nif (!isAuthenticated && claudeAvailable) {\n  const authChoice = await select({\n    message: 'Claude is not authenticated. Choose an option:',\n    choices: [\n      { value: 'apikey', name: 'Enter API key (best for Codespaces/CI)' },\n      { value: 'login', name: 'Run claude auth login (browser OAuth)' },\n      { value: 'skip', name: 'Skip AI features (use defaults)' },\n    ],\n  });\n  if (authChoice === 'apikey') {\n    const apiKey = await input({ message: 'Enter ANTHROPIC_API_KEY:' });\n    process.env['ANTHROPIC_API_KEY'] = apiKey;\n  } else if (authChoice === 'login') {\n    await run('claude', ['auth', 'login']);\n  }\n  // 'skip' falls through — claudeAuthenticated remains false\n}\nconfig.claudeAuthenticated = isAuthenticated || authChoice !== 'skip';\n```\n\n### New-vs-Existing Project Prompt (early in wizard, before architecture)\n\n```typescript\nconst isExistingProject = await confirm({\n  message: 'Is this an existing project (scan codebase to auto-detect structure)?',\n  default: false,\n});\nconfig.isExistingProject = isExistingProject;\n```\n\n### AI Analysis Step (existing projects only, after auth gate)\n\n```typescript\nif (isExistingProject && config.claudeAuthenticated) {\n  console.log('\\n🔍 Scanning project structure...');\n  const analysis = await analyzeProject(config.projectRoot);\n  if (analysis) {\n    // Show preview\n    console.log('\\n🤖 AI detected your project structure:');\n    console.log(`  Architecture:  ${analysis.detectedArchitecture}`);\n    console.log(`  API paths:     ${analysis.apiPaths.join(', ')}`);\n    console.log(`  DB paths:      ${analysis.dbPaths.join(', ')}`);\n    console.log(`  Test paths:    ${analysis.testPaths.join(', ')}`);\n    console.log(`  Rules:         ${analysis.recommendedRules.join(', ')}`);\n    console.log(`  Hook steps:    ${analysis.hookSteps.join(', ')}`);\n    console.log(`\\n  Guidance: \"${analysis.architectureGuidance}\"\\n`);\n\n    const accept = await select({\n      message: 'Accept this configuration?',\n      choices: [\n        { value: 'yes', name: 'Yes — use AI-detected config' },\n        { value: 'no', name: 'No — use manual wizard steps' },\n      ],\n    });\n    if (accept === 'yes') {\n      config.analysisResult = analysis;\n      config.architecture = analysis.detectedArchitecture;\n      config.hasApiDocs = analysis.apiPaths.length > 0;\n      config.hasDatabase = analysis.dbPaths.length > 0;\n    }\n  }\n}\n```\n\n---\n\n## 7. Update `src/generators/rules.ts`\n\nReplace hardcoded glob paths with analysis results:\n\n```typescript\n// In api.md generation:\nconst apiPaths = config.analysisResult?.apiPaths ?? ['src/api/**'];\nconst vars = { ...baseVars, API_PATHS: apiPaths.join(', ') };\n\n// In database.md generation:\nconst dbPaths = config.analysisResult?.dbPaths ?? ['src/db/**'];\nconst vars = { ...baseVars, DB_PATHS: dbPaths.join(', ') };\n\n// Conditionally include only recommendedRules if analysis available:\nconst rulesToGenerate = config.analysisResult?.recommendedRules ?? defaultRules;\n```\n\n---\n\n## 8. Update `src/generators/hooks.ts`\n\nUse `hookSteps` from analysis result when available:\n\n```typescript\nconst hookSteps = config.analysisResult?.hookSteps ?? ['format', 'lint', 'typecheck', 'build', 'test'];\n// Build pre-commit script from hookSteps array\n```\n\n---\n\n## 9. Update `src/generators/claude-md.ts`\n\nInsert architecture guidance section when analysis result is present:\n\n```typescript\nif (config.analysisResult?.architectureGuidance) {\n  sections.push(`## Architecture Notes\\n\\n${config.analysisResult.architectureGuidance}`);\n}\n```\n\n---\n\n## 10. Update `templates/rules/api.md`\n\nReplace hardcoded `src/api/**` with `{{API_PATHS}}`:\n```markdown\n<!-- globs: {{API_PATHS}} -->\n```\n\n## 11. Update `templates/rules/database.md`\n\nReplace hardcoded `src/db/**` with `{{DB_PATHS}}`:\n```markdown\n<!-- globs: {{DB_PATHS}} -->\n```\n\n---\n\n## 12. Update `install.sh`\n\n```bash\n# Replace:\nnpm install -g @anthropic-ai/claude-code\n\n# With:\ncurl -fsSL https://claude.ai/install.sh | bash\n```\n\n---\n\n## 13. Non-Interactive Mode\n\nIn non-interactive mode (`SETUP_AI_NONINTERACTIVE=1`), skip ALL AI analysis:\n```typescript\nif (NON_INTERACTIVE) {\n  config.isExistingProject = false;\n  config.claudeAuthenticated = false;\n  config.analysisResult = undefined;\n}\n```\n\n---\n\n## Architecture Notes\n\n- `src/detect.ts` is pure filesystem — no imports from wizard or generators, testable in isolation\n- `src/analyze.ts` depends only on `detect.ts` and `utils.ts` (for `run()`)\n- All generators degrade gracefully: `config.analysisResult?.apiPaths ?? ['src/api/**']`\n- Zod schema mirrors the `--json-schema` JSON Schema exactly — they must stay in sync\n- The `run()` utility in `utils.ts` already supports capturing stdout; no changes needed there",
        "testStrategy": "### Unit Tests — `test/detect.test.ts`\n\nUse `os.tmpdir()` + random suffix, clean up in `afterEach`:\n\n```typescript\ndescribe('detectProject', () => {\n  it('detects package.json and tsconfig.json', async () => {\n    await fs.writeFile(path.join(tmpDir, 'package.json'), '{\"dependencies\":{\"express\":\"^4\"}}');\n    await fs.writeFile(path.join(tmpDir, 'tsconfig.json'), '{}');\n    const result = await detectProject(tmpDir);\n    expect(result.hasPackageJson).toBe(true);\n    expect(result.hasTsConfig).toBe(true);\n    expect(result.frameworks).toContain('express');\n  });\n\n  it('detects Prisma schema', async () => {\n    await fs.mkdir(path.join(tmpDir, 'prisma'));\n    await fs.writeFile(path.join(tmpDir, 'prisma/schema.prisma'), '');\n    const result = await detectProject(tmpDir);\n    expect(result.hasPrismaSchema).toBe(true);\n    expect(result.orms).toContain('prisma');  // from package.json if present\n  });\n\n  it('lists src/ subdirectories', async () => {\n    await fs.mkdir(path.join(tmpDir, 'src/api'), { recursive: true });\n    await fs.mkdir(path.join(tmpDir, 'src/db'), { recursive: true });\n    const result = await detectProject(tmpDir);\n    expect(result.directories).toContain('api');\n    expect(result.directories).toContain('db');\n  });\n\n  it('falls back to root dirs when no src/', async () => {\n    await fs.mkdir(path.join(tmpDir, 'routes'));\n    const result = await detectProject(tmpDir);\n    expect(result.directories).toContain('routes');\n  });\n\n  it('handles missing package.json gracefully', async () => {\n    const result = await detectProject(tmpDir);\n    expect(result.hasPackageJson).toBe(false);\n    expect(result.frameworks).toEqual([]);\n  });\n});\n```\n\n### Unit Tests — `test/analyze.test.ts`\n\nMock the `run()` utility from `utils.ts`:\n\n```typescript\nimport { vi } from 'vitest';\nimport * as utils from '../src/utils.js';\n\nvi.spyOn(utils, 'run');\n\ndescribe('analyzeProject', () => {\n  it('returns ProjectAnalysis on valid Haiku response', async () => {\n    const validResponse = {\n      detectedArchitecture: '3-tier',\n      apiPaths: ['src/routes/**'],\n      dbPaths: ['prisma/**'],\n      testPaths: ['test/**'],\n      architectureGuidance: 'Express + Prisma + React.',\n      recommendedRules: ['general', 'api', 'database'],\n      hookSteps: ['format', 'lint', 'typecheck'],\n    };\n    vi.mocked(utils.run).mockResolvedValueOnce(JSON.stringify(validResponse));\n    const result = await analyzeProject(tmpDir);\n    expect(result?.detectedArchitecture).toBe('3-tier');\n    expect(result?.apiPaths).toEqual(['src/routes/**']);\n  });\n\n  it('retries once on Zod validation failure, then returns null', async () => {\n    const invalidResponse = { detectedArchitecture: 'unknown', apiPaths: 'not-array' };\n    vi.mocked(utils.run)\n      .mockResolvedValueOnce(JSON.stringify(invalidResponse))  // first call fails Zod\n      .mockResolvedValueOnce(JSON.stringify(invalidResponse)); // retry also fails\n    const result = await analyzeProject(tmpDir);\n    expect(result).toBeNull();\n    expect(utils.run).toHaveBeenCalledTimes(2);\n  });\n\n  it('returns null when claude run() throws (network error)', async () => {\n    vi.mocked(utils.run).mockRejectedValueOnce(new Error('command not found'));\n    const result = await analyzeProject(tmpDir);\n    expect(result).toBeNull();\n  });\n\n  it('returns null when JSON.parse fails', async () => {\n    vi.mocked(utils.run).mockResolvedValueOnce('not valid json');\n    const result = await analyzeProject(tmpDir);\n    expect(result).toBeNull();\n  });\n});\n```\n\n### Unit Tests — `test/auth.test.ts`\n\nMock `run()` and `commandExists()`:\n\n```typescript\ndescribe('checkClaudeAuthenticated', () => {\n  it('returns true when claude auth status includes \"Login method\"', async () => {\n    vi.mocked(utils.run).mockResolvedValueOnce('Login method: OAuth\\nUser: test@example.com');\n    expect(await checkClaudeAuthenticated()).toBe(true);\n  });\n\n  it('returns false when claude auth status throws', async () => {\n    vi.mocked(utils.run).mockRejectedValueOnce(new Error('not authenticated'));\n    expect(await checkClaudeAuthenticated()).toBe(false);\n  });\n\n  it('returns false when output does not include \"Login method\"', async () => {\n    vi.mocked(utils.run).mockResolvedValueOnce('Error: not logged in');\n    expect(await checkClaudeAuthenticated()).toBe(false);\n  });\n});\n\ndescribe('installClaudeCode (native installer)', () => {\n  it('calls bash with native curl installer', async () => {\n    vi.mocked(utils.run).mockResolvedValueOnce('');\n    await installClaudeCode();\n    expect(utils.run).toHaveBeenCalledWith(\n      'bash', ['-c', expect.stringContaining('claude.ai/install.sh')], expect.anything()\n    );\n  });\n});\n```\n\n### Integration Tests — `test/integration/wizard-f20.test.ts`\n\nUse `@inquirer/testing` to automate wizard prompts:\n\n1. With `isExistingProject: false` → `config.analysisResult` is `undefined`; standard arch selection proceeds\n2. With `isExistingProject: true` + `claudeAuthenticated: false` → `analyzeProject` not called; wizard shows manual steps\n3. With `isExistingProject: true` + valid Haiku mock → analysis preview shown; user accepts → `config.architecture` set from analysis\n4. With `isExistingProject: true` + user rejects AI config → falls back to manual architecture selection\n\n### Generator Tests — Updated `test/generators/rules.test.ts`\n\n1. With `config.analysisResult.apiPaths = ['src/server/routes/**']` → api.md contains `src/server/routes/**`\n2. With no `analysisResult` → api.md contains fallback `src/api/**`\n3. With `config.analysisResult.recommendedRules = ['general', 'testing']` → only `general.md` and `testing.md` generated (not `api.md`, `database.md`)\n\n### Generator Tests — Updated `test/generators/claude-md.test.ts`\n\n1. With `analysisResult.architectureGuidance` set → CLAUDE.md contains \"## Architecture Notes\" section with guidance text\n2. With no `analysisResult` → CLAUDE.md does not contain \"## Architecture Notes\"\n\n### Build & Lint Verification\n\nAfter all changes, run:\n1. `npm install` — verifies Zod added successfully\n2. `npm run typecheck` — no TypeScript errors in new files\n3. `npm run lint` — no ESLint errors\n4. `npm run build` — zero build errors, `dist/` updated\n5. `npm test` — all tests pass including new test files\n\n### Manual Smoke Test\n\nIn an existing project with known structure (e.g., this repo itself):\n1. Set `ANTHROPIC_API_KEY` and verify `claude auth status` returns authenticated\n2. Run `npm run dev` (tsx src/cli.ts) and select \"Existing project\"\n3. Verify detection output shows correct `hasPackageJson: true`, `frameworks`, `directories`\n4. Verify Haiku call completes and produces parseable JSON\n5. Verify Zod validation passes\n6. Verify generated rules contain detected paths, not hardcoded `src/api/**`",
        "status": "pending",
        "dependencies": [
          "2",
          "4",
          "8",
          "10",
          "14",
          "15",
          "17"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Install Zod, update src/types.ts with new interfaces, and create src/detect.ts",
            "description": "Add Zod as a runtime dependency, extend ProjectConfig with isExistingProject/claudeAuthenticated/analysisResult fields, add DetectionResult and ProjectAnalysis interfaces to src/types.ts, and create the pure-filesystem src/detect.ts module.",
            "dependencies": [],
            "details": "Run `npm install zod@latest` to add Zod. In src/types.ts add three interfaces: DetectionResult (hasPackageJson, hasTsConfig, hasDockerCompose, hasPrismaSchema, hasGraphqlConfig, directories, configFiles, frameworks, orms, testFrameworks), ProjectAnalysis (detectedArchitecture enum, apiPaths, dbPaths, testPaths, architectureGuidance, recommendedRules, hookSteps), and extend ProjectConfig with isExistingProject: boolean, claudeAuthenticated: boolean, analysisResult?: ProjectAnalysis. Then create src/detect.ts: async detectProject(projectRoot) using Promise.all for parallel fs.access checks, scan src/ or project-root directories, read package.json for framework/orm/test deps detection using KNOWN_FRAMEWORKS/KNOWN_ORMS/KNOWN_TEST_FRAMEWORKS arrays. No LLM calls — pure filesystem. Returns DetectionResult.",
            "status": "pending",
            "testStrategy": "Create test/detect.test.ts using os.tmpdir() fixtures. Test: detects package.json presence, detects express from package.json deps, detects prisma schema, scans src/ directories, gracefully handles missing src/ directory, runs under 200ms for typical projects.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create src/analyze.ts with Haiku orchestration and Zod validation",
            "description": "Implement the analyzeProject() function that calls detectProject(), builds a prompt, invokes `claude --model haiku` in headless mode with --json-schema, validates output with Zod, retries once on failure, and returns null on any error for graceful degradation.",
            "dependencies": [
              1
            ],
            "details": "Create src/analyze.ts. Define KNOWN_RULES and KNOWN_HOOK_STEPS const arrays. Define AnalysisSchema with z.object() mirroring ProjectAnalysis fields exactly. Define JSON_SCHEMA object (matching the Zod schema) to pass as --json-schema argument. Implement callHaiku(detection, projectRoot): builds prompt string with JSON.stringify(detection), calls run('claude', ['--model','haiku','-p',prompt,'--output-format','json','--json-schema',JSON.stringify(JSON_SCHEMA),'--max-turns','1','--allowedTools','Read'], projectRoot), returns JSON.parse(output). Implement analyzeProject(projectRoot): calls detectProject, calls callHaiku, AnalysisSchema.safeParse on result, if invalid builds retry prompt with validation errors, calls callHaiku again, returns null if both attempts fail or any exception occurs. All errors caught, never throws.",
            "status": "pending",
            "testStrategy": "Create test/analyze.test.ts. Mock the run() utility. Test: returns null when detectProject fails, returns null when Haiku output is invalid JSON, returns null when Zod validation fails both attempts, returns ProjectAnalysis on valid first-attempt output, returns ProjectAnalysis on valid retry output, Zod schema and JSON_SCHEMA are consistent (same fields and constraints).",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Update src/audit.ts with native installer and auth gate, update src/wizard.ts with new wizard steps",
            "description": "Replace the deprecated npm Claude install with the native curl-based installer in audit.ts, add checkClaudeAuthenticated(), then wire up the auth gate prompt and new-vs-existing project prompt into wizard.ts. Add non-interactive mode guards. Feed analyzeProject() results into ProjectConfig.",
            "dependencies": [
              2
            ],
            "details": "In src/audit.ts: replace `run('npm', ['install', '-g', '@anthropic-ai/claude-code'])` with `run('bash', ['-c', 'curl -fsSL https://claude.ai/install.sh | bash'])`. Add checkClaudeAuthenticated(): run('claude', ['auth','status','--text']), return output.includes('Login method'), catch → return false. In src/wizard.ts: (a) After Claude install check, if not authenticated and claudeAvailable, show select prompt with apikey/login/skip choices; handle apikey by setting process.env.ANTHROPIC_API_KEY, handle login by running claude auth login; set config.claudeAuthenticated. (b) Early in wizard add confirm prompt: 'Is this an existing project?', set config.isExistingProject. (c) If isExistingProject && claudeAuthenticated: call analyzeProject(config.projectRoot), if result show preview console.log lines (architecture, apiPaths, dbPaths, testPaths, rules, hookSteps, guidance), show select prompt accept/no; on accept set config.analysisResult, config.architecture, config.hasApiDocs, config.hasDatabase. (d) NON_INTERACTIVE guard: skip all three new steps, set isExistingProject=false, claudeAuthenticated=false, analysisResult=undefined.",
            "status": "pending",
            "testStrategy": "Update test/audit.test.ts: verify installClaudeCode uses curl not npm, verify checkClaudeAuthenticated returns true when output contains 'Login method', returns false on error. Update test/wizard.test.ts: verify SETUP_AI_NONINTERACTIVE skips new prompts, verify non-interactive config has claudeAuthenticated=false and isExistingProject=false.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Update generators rules.ts, hooks.ts, claude-md.ts and templates api.md/database.md to use analysis results",
            "description": "Update the three generator files to consume analysisResult from ProjectConfig, replacing hardcoded path globs with detected paths. Update template files to use {{API_PATHS}} and {{DB_PATHS}} placeholders. All updates use nullish coalescing for graceful degradation.",
            "dependencies": [
              1
            ],
            "details": "In src/generators/rules.ts: for api.md generation use `const apiPaths = config.analysisResult?.apiPaths ?? ['src/api/**']` and pass API_PATHS: apiPaths.join(', ') to fillTemplate vars. For database.md generation use `const dbPaths = config.analysisResult?.dbPaths ?? ['src/db/**']` and pass DB_PATHS: dbPaths.join(', '). If analysisResult is present, use `config.analysisResult.recommendedRules` to filter which rule files to generate; otherwise use full default rules list. In src/generators/hooks.ts: extract `const hookSteps = config.analysisResult?.hookSteps ?? ['format','lint','typecheck','build','test']` and build pre-commit script from hookSteps array instead of hardcoded list. In src/generators/claude-md.ts: after building sections array, if config.analysisResult?.architectureGuidance, push `## Architecture Notes\n\n${config.analysisResult.architectureGuidance}`. In templates/rules/api.md: replace hardcoded `src/api/**` and `src/routes/**` path globs with `{{API_PATHS}}`. In templates/rules/database.md: replace hardcoded `src/db/**` path glob with `{{DB_PATHS}}`.",
            "status": "pending",
            "testStrategy": "Update test/generators/rules.test.ts: verify with analysisResult containing apiPaths=['app/routes/**'] that generated api.md contains 'app/routes/**' not 'src/api/**'. Verify with no analysisResult fallback to defaults. Update test/generators/hooks.test.ts: verify hookSteps from analysisResult are used. Update test/generators/claude-md.test.ts: verify Architecture Notes section appears when architectureGuidance is set.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Update install.sh and run full quality gate (format, lint, typecheck, build, tests)",
            "description": "Replace the npm install line in install.sh with the native curl-based Claude installer, then run the full mandatory pre-completion checklist: format, lint, typecheck, build, and test suite. Fix any failures.",
            "dependencies": [
              3,
              4
            ],
            "details": "In install.sh: find the line `npm install -g @anthropic-ai/claude-code` and replace with `curl -fsSL https://claude.ai/install.sh | bash`. Run `npm run format` (or `npx prettier --write .`) to auto-fix formatting across all new and modified files. Run `npm run lint` (or `npx eslint . --fix`) to fix lint errors. Run `npx tsc --noEmit` to verify TypeScript types are correct across the new detect.ts, analyze.ts, updated types.ts, audit.ts, wizard.ts, and all generator files. Run `npm run build` to verify zero compilation errors. Run `npm test` to execute the full Vitest suite including all new tests for detect.ts and analyze.ts. Fix any failures encountered at any step before proceeding. Verify that defaults.ts is updated if needed to provide default values for the three new ProjectConfig fields (isExistingProject, claudeAuthenticated, analysisResult).",
            "status": "pending",
            "testStrategy": "Full test suite must pass with zero failures. TypeScript compilation must succeed with zero errors. Build output dist/cli.js must exist. ESLint must report zero errors. Prettier must report zero files changed after format run. Verify install.sh contains curl line and does not contain deprecated npm install line.",
            "parentId": "undefined"
          }
        ]
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-02-18T17:52:17.474Z",
      "taskCount": 32,
      "completedCount": 26,
      "tags": [
        "master"
      ]
    }
  }
}