{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Initialize TypeScript Project Structure",
        "description": "Set up the TypeScript project foundation including package.json, tsconfig.json, vitest configuration, and directory scaffold. This replaces the existing bash monolith with a maintainable TypeScript CLI.",
        "details": "Create the following files in the project root:\n\n**package.json:**\n```json\n{\n  \"name\": \"ai-helper-tools\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Bootstrap tool for AI-assisted development environments\",\n  \"type\": \"module\",\n  \"bin\": { \"ai-init\": \"./dist/cli.js\" },\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"dev\": \"tsx src/cli.ts\",\n    \"test\": \"vitest run\",\n    \"test:watch\": \"vitest\",\n    \"lint\": \"eslint src test --ext .ts\",\n    \"typecheck\": \"tsc --noEmit\",\n    \"format\": \"prettier --write .\"\n  },\n  \"dependencies\": {\n    \"meow\": \"latest\",\n    \"@inquirer/prompts\": \"latest\"\n  },\n  \"devDependencies\": {\n    \"typescript\": \"latest\",\n    \"vitest\": \"latest\",\n    \"tsx\": \"latest\",\n    \"@types/node\": \"latest\",\n    \"eslint\": \"latest\",\n    \"@typescript-eslint/eslint-plugin\": \"latest\",\n    \"@typescript-eslint/parser\": \"latest\",\n    \"prettier\": \"latest\"\n  }\n}\n```\n\n**tsconfig.json:**\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"bundler\",\n    \"outDir\": \"dist\",\n    \"rootDir\": \"src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"declaration\": true\n  },\n  \"include\": [\"src\"]\n}\n```\n\n**vitest.config.ts:**\n```typescript\nimport { defineConfig } from 'vitest/config';\nexport default defineConfig({\n  test: {\n    globals: true,\n    include: ['test/**/*.test.ts'],\n  },\n});\n```\n\nCreate directory structure:\n```\nsrc/\n  phases/\n  generators/\ntemplates/\n  docs/\n  rules/\n  skills/\n  hooks/\n  commands/\ntest/\n  generators/\n  integration/\n  fixtures/\ndist/  (gitignored)\n```\n\nAdd to .gitignore: `dist/`, `node_modules/`\n\nRun `npm install` after creating package.json.\n\nVerify Node.js >= 20 is available (`node --version`).",
        "testStrategy": "Smoke test: `npm run build` succeeds with zero errors. `npm run typecheck` passes. `npm run test` runs (even with zero tests). Verify `dist/` directory is created after build.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Verify prerequisites and create package.json with dependencies",
            "description": "Check Node.js >= 20 is available, then create package.json with all required dependencies and scripts. Run npm install to fetch packages and generate package-lock.json.",
            "dependencies": [],
            "details": "1. Confirm `node --version` outputs v20+ (v24.11.1 is confirmed available). 2. Create `/workspaces/ai-dev-setup/package.json` with `\"type\": \"module\"`, `\"bin\": { \"ai-init\": \"./dist/cli.js\" }`, scripts (`build`, `dev`, `test`, `test:watch`, `lint`, `typecheck`, `format`), runtime dependencies (`meow@latest`, `@inquirer/prompts@latest`), and devDependencies (`typescript@latest`, `vitest@latest`, `tsx@latest`, `@types/node@latest`, `eslint@latest`, `@typescript-eslint/eslint-plugin@latest`, `@typescript-eslint/parser@latest`, `prettier@latest`). 3. Run `npm install` in the project root to resolve and lock all versions in `package-lock.json`. 4. Verify `node_modules/` is created and key binaries exist (e.g., `npx tsc --version`).",
            "status": "done",
            "testStrategy": "Run `npm list --depth=0` and confirm all 9 devDependencies and 2 runtime dependencies are listed without errors.",
            "updatedAt": "2026-02-17T19:34:56.695Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create tsconfig.json and vitest.config.ts",
            "description": "Add TypeScript compiler configuration targeting ES2022 with ESNext modules and bundler resolution, plus a vitest configuration that picks up tests from the test/ directory.",
            "dependencies": [
              1
            ],
            "details": "1. Create `/workspaces/ai-dev-setup/tsconfig.json` with `compilerOptions`: `target: ES2022`, `module: ESNext`, `moduleResolution: bundler`, `outDir: dist`, `rootDir: src`, `strict: true`, `esModuleInterop: true`, `skipLibCheck: true`, `declaration: true`; `include: [\"src\"]`. 2. Create `/workspaces/ai-dev-setup/vitest.config.ts` importing `defineConfig` from `vitest/config`, exporting config with `test.globals: true` and `test.include: ['test/**/*.test.ts']`. 3. Run `npx tsc --noEmit --project tsconfig.json` (will error on missing src/ but confirms tsconfig is parsed correctly — no syntax errors). 4. Run `npx vitest --version` to confirm vitest CLI resolves.",
            "status": "done",
            "testStrategy": "Run `npx tsc --version` and `npx vitest --version` — both must print version strings without errors. Running `npm run typecheck` may report 'no input files' which is acceptable at this stage (src/ is empty).",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:34:56.708Z"
          },
          {
            "id": 3,
            "title": "Scaffold directory structure under src/, templates/, test/, and dist/",
            "description": "Create all required empty directories so subsequent tasks can place files in the correct locations without needing to create parent directories themselves.",
            "dependencies": [
              1
            ],
            "details": "Create the following directories (use `mkdir -p` for nested paths): `src/phases/`, `src/generators/`, `templates/docs/`, `templates/rules/`, `templates/skills/`, `templates/hooks/`, `templates/commands/`, `test/generators/`, `test/integration/`, `test/fixtures/`. Each directory should contain a `.gitkeep` file so the structure is committed to git. The `dist/` directory will be created by the TypeScript compiler and should NOT contain a `.gitkeep` — it is gitignored.",
            "status": "done",
            "testStrategy": "Run `find src templates test -type d | sort` and verify all 10 directories appear. Confirm `dist/` does not exist yet (it is created only after `npm run build`).",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:34:56.729Z"
          },
          {
            "id": 4,
            "title": "Update .gitignore to exclude dist/ and ensure node_modules/ is covered",
            "description": "Add `dist/` to the existing .gitignore file so compiled output is never committed. Confirm `node_modules/` is already present (it is) so no duplicate entry is needed.",
            "dependencies": [
              1
            ],
            "details": "1. Read the existing `/workspaces/ai-dev-setup/.gitignore`. 2. It already contains `node_modules/` on line 10. 3. Append `dist/` as a new line (and optionally `*.tsbuildinfo` for incremental build cache). 4. Do NOT remove any existing entries — only add the missing `dist/` line. 5. Verify `.vscode` is in .gitignore (it already is on line 18 — no change needed). The final .gitignore should have both `node_modules/` and `dist/` entries.",
            "status": "done",
            "testStrategy": "Run `grep -E '^dist/$' .gitignore` — must output `dist/`. Run `git check-ignore -v dist/` after creating a dummy `dist/` directory to confirm git ignores it.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:34:56.734Z"
          },
          {
            "id": 5,
            "title": "Create a minimal src/cli.ts entry point and verify the full build pipeline",
            "description": "Add a minimal TypeScript entry point so that `npm run build`, `npm run typecheck`, and `npm run test` all succeed with zero errors, satisfying the smoke-test acceptance criteria for this task.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "1. Create `/workspaces/ai-dev-setup/src/cli.ts` with a minimal shebang and main guard: `#!/usr/bin/env node` followed by `console.log('ai-init v0.1.0');`. This gives the TypeScript compiler a valid input file. 2. Run `npm run build` — this executes `tsc` and should produce `dist/cli.js` and `dist/cli.d.ts` with zero errors. 3. Run `npm run typecheck` — must exit 0. 4. Run `npm run test` — vitest will run with no test files matched; this is acceptable (exits 0 with 'No test files found'). 5. Optionally run `node dist/cli.js` to confirm the compiled output executes without runtime errors.",
            "status": "done",
            "testStrategy": "All four commands must succeed: `npm run build` (exit 0, dist/ populated), `npm run typecheck` (exit 0), `npm run test` (exit 0 or 'no tests' warning — not an error), `node dist/cli.js` prints version string.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:36:39.535Z"
          }
        ],
        "updatedAt": "2026-02-17T19:36:39.535Z"
      },
      {
        "id": "2",
        "title": "Define Core Types and ProjectConfig Interface",
        "description": "Create the shared TypeScript types that all generators and phases will use. The central `ProjectConfig` type drives all code generation — it must capture every user choice from the wizard.",
        "details": "Create `src/types.ts`:\n\n```typescript\nexport type TaskTracker = 'taskmaster' | 'beads' | 'markdown';\nexport type Architecture = 'monolith' | '2-tier' | '3-tier' | 'microservices' | 'skip';\n\nexport interface McpServer {\n  name: string;\n  description: string;\n  npmPackage: string;\n  claudeMcpName: string;\n  required: boolean;\n  args?: string[];\n  env?: Record<string, string>;\n}\n\nexport interface FileDescriptor {\n  path: string;\n  content: string;\n  executable?: boolean;\n}\n\nexport interface ProjectConfig {\n  // MCP selections\n  selectedMcps: string[];\n  // Task tracker\n  taskTracker: TaskTracker;\n  // Architecture\n  architecture: Architecture;\n  // PRD\n  prdPath?: string;       // path to existing PRD, or undefined if using template\n  hasPrd: boolean;\n  // Feature flags\n  generateDocs: boolean;\n  generateRules: boolean;\n  generateSkills: boolean;\n  generateHooks: boolean;\n  generateCommands: boolean;\n  agentTeamsEnabled: boolean;\n  runAudit: boolean;\n  // Derived from selections\n  hasApiDocs: boolean;    // whether docs/api.md should be generated\n  hasDatabase: boolean;   // whether database rules should be generated\n  // Project metadata\n  projectName: string;\n  projectRoot: string;\n  // Tracking for audit\n  generatedFiles: string[];\n}\n\nexport interface AuditResult {\n  passes: string[];\n  fills: { file: string; section: string; message: string }[];\n  fixes: { file: string; issue: string; fix: string }[];\n  postSetupChecklist: string[];\n}\n```\n\nCreate `src/defaults.ts`:\n```typescript\nimport { ProjectConfig } from './types.js';\nimport path from 'node:path';\n\nexport function defaultConfig(projectRoot: string): ProjectConfig {\n  return {\n    selectedMcps: ['taskmaster'],\n    taskTracker: 'taskmaster',\n    architecture: 'skip',\n    hasPrd: false,\n    generateDocs: true,\n    generateRules: true,\n    generateSkills: true,\n    generateHooks: true,\n    generateCommands: true,\n    agentTeamsEnabled: false,\n    runAudit: true,\n    hasApiDocs: false,\n    hasDatabase: false,\n    projectName: path.basename(projectRoot),\n    projectRoot,\n    generatedFiles: [],\n  };\n}\n```",
        "testStrategy": "TypeScript type-check (`tsc --noEmit`) must pass with zero errors. Write a unit test `test/generators/types.test.ts` that imports `ProjectConfig` and `FileDescriptor` and verifies default config shape matches expected defaults.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/types.ts with all exported TypeScript type definitions",
            "description": "Create the `src/types.ts` file containing all shared TypeScript types: `TaskTracker`, `Architecture`, `McpServer`, `FileDescriptor`, `ProjectConfig`, and `AuditResult` interfaces as specified in the task details.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/src/types.ts` with strict TypeScript. Export `TaskTracker` as a union type of `'taskmaster' | 'beads' | 'markdown'`. Export `Architecture` as `'monolith' | '2-tier' | '3-tier' | 'microservices' | 'skip'`. Export `McpServer` interface with fields: `name`, `description`, `npmPackage`, `claudeMcpName`, `required`, optional `args?: string[]`, optional `env?: Record<string, string>`. Export `FileDescriptor` interface with `path`, `content`, and optional `executable?: boolean`. Export `ProjectConfig` interface with all fields listed in the task: `selectedMcps`, `taskTracker`, `architecture`, `prdPath?`, `hasPrd`, `generateDocs`, `generateRules`, `generateSkills`, `generateHooks`, `generateCommands`, `agentTeamsEnabled`, `runAudit`, `hasApiDocs`, `hasDatabase`, `projectName`, `projectRoot`, `generatedFiles`. Export `AuditResult` interface with `passes`, `fills` (array of objects with `file`, `section`, `message`), `fixes` (array of objects with `file`, `issue`, `fix`), and `postSetupChecklist`. The file must use ESM syntax compatible with `\"type\": \"module\"` in package.json and `\"module\": \"ESNext\"` in tsconfig.",
            "status": "done",
            "testStrategy": "Run `npm run typecheck` to verify zero TypeScript errors. Import `ProjectConfig` and `FileDescriptor` in a test file to verify type shapes.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:49:36.677Z"
          },
          {
            "id": 2,
            "title": "Create src/defaults.ts with defaultConfig factory function",
            "description": "Create the `src/defaults.ts` file that exports a `defaultConfig(projectRoot: string): ProjectConfig` function returning a fully-populated default configuration object.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/src/defaults.ts`. Import `ProjectConfig` from `'./types.js'` (must use `.js` extension for ESM resolution with `moduleResolution: bundler`). Import `path` from `'node:path'`. Implement and export `defaultConfig(projectRoot: string): ProjectConfig` returning an object with: `selectedMcps: ['taskmaster']`, `taskTracker: 'taskmaster'`, `architecture: 'skip'`, `hasPrd: false`, `generateDocs: true`, `generateRules: true`, `generateSkills: true`, `generateHooks: true`, `generateCommands: true`, `agentTeamsEnabled: false`, `runAudit: true`, `hasApiDocs: false`, `hasDatabase: false`, `projectName: path.basename(projectRoot)`, `projectRoot`, `generatedFiles: []`. Note: `prdPath` is intentionally omitted (optional field, undefined by default). Ensure all required `ProjectConfig` fields are covered so TypeScript does not complain about missing properties.",
            "status": "done",
            "testStrategy": "Call `defaultConfig('/some/path/myproject')` and assert `projectName === 'myproject'`, `taskTracker === 'taskmaster'`, `generatedFiles` is an empty array, and `hasPrd === false`.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:49:48.342Z"
          },
          {
            "id": 3,
            "title": "Verify tsconfig.json covers src/types.ts and src/defaults.ts",
            "description": "Confirm the existing `tsconfig.json` `include` pattern covers the new files and that module resolution settings are compatible with `.js` extension imports in ESM.",
            "dependencies": [
              1,
              2
            ],
            "details": "Read `/workspaces/ai-dev-setup/tsconfig.json`. The current config includes `\"include\": [\"src\"]` and uses `\"moduleResolution\": \"bundler\"` with `\"module\": \"ESNext\"`. Confirm that `src/types.ts` and `src/defaults.ts` are within scope. Verify that using `import { ProjectConfig } from './types.js'` inside `src/defaults.ts` is valid under `moduleResolution: bundler` (it is — bundler mode allows omitting extensions but also accepts explicit `.js`). If any tsconfig issues are found (e.g., `rootDir` mismatch), correct them. Run `npm run typecheck` to confirm zero errors after both files are created.",
            "status": "done",
            "testStrategy": "Run `npx tsc --noEmit` from the project root. Expect exit code 0 with no diagnostic output.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:49:49.047Z"
          },
          {
            "id": 4,
            "title": "Write unit test for types and defaultConfig in test/types.test.ts",
            "description": "Create `test/types.test.ts` using Vitest that imports `ProjectConfig`, `FileDescriptor` from `src/types.ts` and `defaultConfig` from `src/defaults.ts`, then asserts correct shape and values.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create `/workspaces/ai-dev-setup/test/types.test.ts`. Use Vitest (`import { describe, it, expect } from 'vitest'`). Import `defaultConfig` from `'../src/defaults.js'`. Import types `ProjectConfig` and `FileDescriptor` from `'../src/types.js'` (TypeScript imports only — used for type assertions). Write a `describe('defaultConfig', ...)` block with tests: (1) `defaultConfig('/tmp/myproject').projectName` equals `'myproject'`; (2) `defaultConfig('/tmp/myproject').taskTracker` equals `'taskmaster'`; (3) `defaultConfig('/tmp/myproject').generatedFiles` is an empty array; (4) `defaultConfig('/tmp/myproject').hasPrd` is `false`; (5) a `FileDescriptor` shape test that constructs `{ path: 'foo.md', content: 'bar' }` and checks it satisfies the interface (via TypeScript compile check, not runtime assertion). Ensure the test file is not inside `src/` since `tsconfig.json` only includes `src` — Vitest picks up test files from the project root via its own config.",
            "status": "done",
            "testStrategy": "Run `npm test` and verify all assertions in `test/types.test.ts` pass with exit code 0.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:50:10.706Z"
          },
          {
            "id": 5,
            "title": "Run full quality gate: format, lint, typecheck, build, and test",
            "description": "Execute the mandatory pre-completion checklist — format, lint, typecheck, build, and test — fixing any failures before marking the task done.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Run the following commands in order from `/workspaces/ai-dev-setup`: (1) `npm run format` — Prettier formats `src/types.ts`, `src/defaults.ts`, and `test/types.test.ts`. (2) `npm run lint` — ESLint checks `src/**/*.ts` and `test/**/*.ts`; fix any lint errors in the new files. (3) `npm run typecheck` — `tsc --noEmit` must exit with 0 errors. (4) `npm run build` — `tsc` compiles to `dist/`; verify `dist/types.d.ts` and `dist/defaults.d.ts` are emitted. (5) `npm test` — Vitest runs all tests including `test/types.test.ts`; all must pass. If any step fails, fix the issue and re-run from that step. Common issues to watch for: missing `.js` extension on relative imports in ESM, unused type imports triggering lint warnings, or `strict: true` requiring explicit return types.",
            "status": "done",
            "testStrategy": "All five commands (`format`, `lint`, `typecheck`, `build`, `test`) must exit with code 0. Confirm `dist/types.d.ts` exists and exports `ProjectConfig`. Confirm `dist/defaults.js` exports `defaultConfig`.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:50:46.363Z"
          }
        ],
        "updatedAt": "2026-02-17T19:50:46.363Z"
      },
      {
        "id": "3",
        "title": "Implement MCP Registry and Server Definitions",
        "description": "Create `src/registry.ts` with the full MCP server registry including the new beads-mcp entry. This is the single source of truth for all MCP server configuration — drives both `.mcp.json` and `.vscode/mcp.json` generation.",
        "details": "Create `src/registry.ts`:\n\n```typescript\nimport { McpServer } from './types.js';\n\nexport const MCP_REGISTRY: McpServer[] = [\n  {\n    name: 'taskmaster',\n    description: 'Task Master AI — task orchestration, dependency tracking, multi-agent coordination',\n    npmPackage: 'task-master-ai',\n    claudeMcpName: 'taskmaster-ai',\n    required: false,\n    args: ['-y', 'task-master-ai'],\n    env: {\n      TASK_MASTER_TOOLS: 'all',\n      ANTHROPIC_API_KEY: '${ANTHROPIC_API_KEY}',\n      PERPLEXITY_API_KEY: '${PERPLEXITY_API_KEY}',\n    },\n  },\n  {\n    name: 'beads',\n    description: 'Beads — distributed git-backed issue tracking for multi-agent workflows',\n    npmPackage: 'beads-mcp',\n    claudeMcpName: 'beads',\n    required: false,\n    args: ['-y', 'beads-mcp'],\n    env: {},\n  },\n  {\n    name: 'context7',\n    description: 'Context7 — up-to-date library docs and code examples via MCP',\n    npmPackage: '@upstash/context7-mcp',\n    claudeMcpName: 'context7',\n    required: false,\n    args: ['-y', '@upstash/context7-mcp'],\n    env: {},\n  },\n  {\n    name: 'browsermcp',\n    description: 'BrowserMCP — browser automation for testing (navigate, click, screenshots)',\n    npmPackage: '@anthropic-ai/mcp-server-puppeteer',\n    claudeMcpName: 'browsermcp',\n    required: false,\n    args: ['-y', '@anthropic-ai/mcp-server-puppeteer'],\n    env: {},\n  },\n  {\n    name: 'sequential-thinking',\n    description: 'Sequential Thinking — dynamic problem-solving through thought sequences',\n    npmPackage: '@anthropic-ai/mcp-server-sequential-thinking',\n    claudeMcpName: 'sequential-thinking',\n    required: false,\n    args: ['-y', '@anthropic-ai/mcp-server-sequential-thinking'],\n    env: {},\n  },\n];\n\nexport function getMcpByName(name: string): McpServer | undefined {\n  return MCP_REGISTRY.find(s => s.name === name);\n}\n\nexport function getSelectedServers(selectedNames: string[]): McpServer[] {\n  return MCP_REGISTRY.filter(s => selectedNames.includes(s.name));\n}\n```",
        "testStrategy": "Unit test `test/generators/registry.test.ts`: assert registry has 5 entries, beads entry exists with correct npmPackage 'beads-mcp', taskmaster entry has TASK_MASTER_TOOLS env var, `getMcpByName('context7')` returns correct server, `getSelectedServers(['taskmaster', 'beads'])` returns exactly 2 servers.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/registry.ts with MCP_REGISTRY array",
            "description": "Create the main registry file with all 5 MCP server definitions as a typed McpServer[] array, importing from './types.js'.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/src/registry.ts`. Import `McpServer` from `'./types.js'`. Export a const `MCP_REGISTRY: McpServer[]` containing exactly 5 entries: taskmaster (npmPackage: 'task-master-ai', claudeMcpName: 'taskmaster-ai', env with TASK_MASTER_TOOLS, ANTHROPIC_API_KEY, PERPLEXITY_API_KEY using `${...}` template literals), beads (npmPackage: 'beads-mcp', claudeMcpName: 'beads', empty env), context7 (npmPackage: '@upstash/context7-mcp', claudeMcpName: 'context7', empty env), browsermcp (npmPackage: '@anthropic-ai/mcp-server-puppeteer', claudeMcpName: 'browsermcp', empty env), and sequential-thinking (npmPackage: '@anthropic-ai/mcp-server-sequential-thinking', claudeMcpName: 'sequential-thinking', empty env). All entries must have `required: false` and appropriate `args: ['-y', '<npmPackage>']`.",
            "status": "done",
            "testStrategy": null,
            "updatedAt": "2026-02-17T19:53:49.399Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement getMcpByName and getSelectedServers helper functions",
            "description": "Add the two exported lookup/filter functions to src/registry.ts that downstream generators and the wizard will use to query the registry.",
            "dependencies": [
              1
            ],
            "details": "In the same `/workspaces/ai-dev-setup/src/registry.ts` file created in subtask 1, add: (1) `export function getMcpByName(name: string): McpServer | undefined` — returns `MCP_REGISTRY.find(s => s.name === name)`. (2) `export function getSelectedServers(selectedNames: string[]): McpServer[]` — returns `MCP_REGISTRY.filter(s => selectedNames.includes(s.name))`. Both functions must handle edge cases gracefully (unknown name returns undefined, empty array returns []).",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:53:50.108Z"
          },
          {
            "id": 3,
            "title": "Write unit tests for MCP_REGISTRY contents",
            "description": "Create test/registry.test.ts to verify the shape and contents of MCP_REGISTRY match expected values.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create `/workspaces/ai-dev-setup/test/registry.test.ts` using vitest (`describe`/`it`/`expect`). Import `{ MCP_REGISTRY, getMcpByName, getSelectedServers }` from `'../src/registry.js'`. Write tests asserting: (1) `MCP_REGISTRY.length === 5`, (2) beads entry exists with `npmPackage === 'beads-mcp'` and `claudeMcpName === 'beads'`, (3) taskmaster entry has `env.TASK_MASTER_TOOLS === 'all'`, (4) `getMcpByName('context7')` returns the context7 server object, (5) `getMcpByName('nonexistent')` returns `undefined`, (6) `getSelectedServers(['taskmaster', 'beads'])` returns exactly 2 entries, (7) `getSelectedServers([])` returns an empty array. Follow the pattern from existing `test/types.test.ts`.",
            "status": "done",
            "testStrategy": "Run `npm test` and verify all registry test cases pass with zero failures.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:54:16.724Z"
          },
          {
            "id": 4,
            "title": "Run format, lint, typecheck, and build quality gates",
            "description": "Execute the full pre-completion checklist: prettier format, eslint, tsc typecheck, and tsc build — fixing any issues found.",
            "dependencies": [
              3
            ],
            "details": "Run these commands sequentially from `/workspaces/ai-dev-setup`: (1) `npm run format` — auto-fixes formatting in src/registry.ts and test/registry.test.ts. (2) `npm run lint` — fix any eslint errors reported in the new files. (3) `npm run typecheck` — ensure `tsc --noEmit` passes with zero errors; the McpServer import path must use `.js` extension per ESM rules. (4) `npm run build` — ensure `tsc` compiles successfully and `dist/registry.js` is emitted. Fix any type errors: check that all McpServer fields (name, description, npmPackage, claudeMcpName, required, args, env) satisfy the interface defined in `src/types.ts`.",
            "status": "done",
            "testStrategy": "All four commands must exit with code 0 before proceeding.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:54:53.270Z"
          },
          {
            "id": 5,
            "title": "Run full test suite and verify registry tests pass",
            "description": "Execute npm test to confirm the new registry tests pass alongside existing types tests, with zero failures.",
            "dependencies": [
              4
            ],
            "details": "Run `npm test` from `/workspaces/ai-dev-setup`. Verify: (1) All tests in `test/types.test.ts` still pass (no regressions), (2) All 7+ tests in `test/registry.test.ts` pass, (3) Overall test suite exits with code 0. If any test fails, diagnose the failure — common causes include wrong import paths (must use `.js` extension for ESM), incorrect env value strings (the `${ANTHROPIC_API_KEY}` must be a literal string, not an interpolated value), or a mismatch between expected registry length and actual entries. Fix and re-run until clean.",
            "status": "done",
            "testStrategy": "npm test must report 0 failing tests across all test files.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T19:54:53.872Z"
          }
        ],
        "updatedAt": "2026-02-17T19:54:53.872Z"
      },
      {
        "id": "4",
        "title": "Implement File I/O Utilities",
        "description": "Create `src/utils.ts` with shared helpers for file writing, shell execution, and path manipulation. The key constraint is that generators never touch the filesystem — all I/O goes through `writeFiles()`.",
        "details": "Create `src/utils.ts`:\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { execFile } from 'node:child_process';\nimport { promisify } from 'node:util';\nimport { FileDescriptor } from './types.js';\n\nconst execFileAsync = promisify(execFile);\n\n/**\n * Write all file descriptors to disk. Creates parent directories as needed.\n * Skips writing if the file already exists and overwrite is false.\n * Returns the list of paths actually written.\n */\nexport async function writeFiles(\n  files: FileDescriptor[],\n  root: string,\n  overwrite = true\n): Promise<string[]> {\n  const written: string[] = [];\n  for (const file of files) {\n    const fullPath = path.resolve(root, file.path);\n    await fs.mkdir(path.dirname(fullPath), { recursive: true });\n    if (!overwrite) {\n      try {\n        await fs.access(fullPath);\n        continue; // skip existing file\n      } catch {\n        // file doesn't exist, proceed\n      }\n    }\n    await fs.writeFile(fullPath, file.content, 'utf8');\n    if (file.executable) {\n      await fs.chmod(fullPath, 0o755);\n    }\n    written.push(file.path);\n  }\n  return written;\n}\n\n/**\n * Run a shell command and return stdout. Throws on non-zero exit.\n */\nexport async function run(\n  cmd: string,\n  args: string[],\n  cwd?: string\n): Promise<string> {\n  const { stdout } = await execFileAsync(cmd, args, { cwd });\n  return stdout.trim();\n}\n\n/**\n * Check if a command is available on PATH.\n */\nexport async function commandExists(cmd: string): Promise<boolean> {\n  try {\n    await execFileAsync('which', [cmd]);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Replace {{PLACEHOLDER}} markers in a template string.\n */\nexport function fillTemplate(\n  template: string,\n  vars: Record<string, string>\n): string {\n  return template.replace(/\\{\\{(\\w+)\\}\\}/g, (_, key) => vars[key] ?? `{{${key}}}`);\n}\n\n/**\n * Read a file relative to project root, return null if missing.\n */\nexport async function readOptional(\n  filePath: string\n): Promise<string | null> {\n  try {\n    return await fs.readFile(filePath, 'utf8');\n  } catch {\n    return null;\n  }\n}\n```",
        "testStrategy": "Unit test `test/generators/utils.test.ts` using a temp directory (use `os.tmpdir()` + random suffix, clean up in afterEach): test `writeFiles` creates nested directories and files, test `fillTemplate` replaces all placeholders, test `writeFiles` with `overwrite=false` skips existing files, test `commandExists('node')` returns true.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-17T20:25:50.805Z"
      },
      {
        "id": "5",
        "title": "Create Document Template Files",
        "description": "Create all template files in `templates/` directory. These are plain markdown files with `{{PLACEHOLDER}}` markers. They are the content foundation for F2 (Document Scaffolding). No templating engine — just string replacement via `fillTemplate()`.",
        "details": "Create the following template files:\n\n**`templates/docs/doc_format.md`** — the meta-standard all docs follow:\n- TLDR section (1-3 sentences)\n- TOC with `#section` anchors\n- Sections < 30 lines\n- Tables over prose for structured data\n- Cross-references as relative links\n- Max ~500 lines; split into sub-docs if larger\n\n**`templates/docs/prd.md`** — PRD template with:\n- Problem / Solution / Features / Phases sections\n- Feature template includes: Business outcome, Demo test (single command proving feature works), Acceptance criteria\n- `{{PROJECT_NAME}}` placeholder\n\n**`templates/docs/architecture.md`** — Architecture overview:\n- TLDR, tier overview (adapts to chosen architecture: monolith/2-tier/3-tier/microservices)\n- Component map, links to detail docs\n- `{{ARCHITECTURE}}` and `{{PROJECT_NAME}}` placeholders\n\n**`templates/docs/api.md`** — API surface:\n- Table format: Endpoint | Method | Description | ADR ref | Source file\n- Auth section\n- Error shapes section\n- `{{PROJECT_NAME}}` placeholder\n\n**`templates/docs/cuj.md`** — Critical user journeys:\n- Step-by-step flows agents should understand\n- One section per major user journey\n\n**`templates/docs/testing_strategy.md`** — Testing philosophy:\n- Integration-first principle\n- Demo-test definition\n- Mock justification rules\n- Quality gate steps\n\n**`templates/docs/onboarding.md`** — Quick-start guide:\n- Project context (1 para)\n- Key commands table\n- Where to find things (map of important files/dirs)\n- First steps for a new developer or agent\n\n**`templates/docs/adr_template.md`** — ADR format:\n```markdown\n# ADR-{{NUMBER}}: {{TITLE}}\n- **Status:** Proposed\n- **Context:** Why this decision was needed\n- **Decision:** What was decided\n- **Consequences:** Trade-offs accepted\n```\n\n**`templates/docs/tasks_simple.md`** — Simple task tracker:\n```markdown\n# Task Tracker\n## Summary\n| # | Task | Status | Depends |\n|---|------|--------|---------|\n| 1 | Example task | [ ] | — |\n\n## Tasks\n### Task 1: Example task\n- **Status:** Pending\n- **Depends on:** —\n- **Success:** App runs without errors\n- **Demo command:** `npm start`\n```\n\nAll templates should follow doc_format.md standard themselves.",
        "testStrategy": "Test that all template files exist via `fs.access()`. Test that `fillTemplate` correctly substitutes `{{PROJECT_NAME}}` and `{{ARCHITECTURE}}` in the architecture template. Test that prd.md template contains the 'Demo test' field placeholder. Test that no template exceeds 500 lines.",
        "priority": "high",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create doc_format.md meta-standard template",
            "description": "Create `templates/docs/doc_format.md` defining the documentation standard that all other templates must follow.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/doc_format.md` with: TLDR section (1-3 sentences), TOC using `#section` anchors, rules for sections < 30 lines, preference for tables over prose, cross-references as relative links, and a max ~500 lines guideline with sub-doc splitting instructions. This file itself must follow its own standard. Remove the existing `.gitkeep` from `templates/docs/` if present after creating the first file.",
            "status": "done",
            "testStrategy": "Check that the file exists via `fs.access()`. Verify it contains 'TLDR', 'TOC', '500 lines', and 'tables' sections. Verify line count is under 500.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:04.441Z"
          },
          {
            "id": 2,
            "title": "Create prd.md PRD template",
            "description": "Create `templates/docs/prd.md` containing the PRD template with Problem/Solution/Features/Phases sections and a `{{PROJECT_NAME}}` placeholder.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/prd.md`. Must include: a TLDR at the top, a TOC, sections for Problem, Solution, Features, and Phases. Each feature entry should have sub-fields: Business outcome, Demo test (single-command proof), and Acceptance criteria. Insert `{{PROJECT_NAME}}` placeholder in the header. Follow doc_format.md standards (sections < 30 lines, tables where applicable).",
            "status": "done",
            "testStrategy": "Verify file exists. Verify `{{PROJECT_NAME}}` placeholder is present. Verify 'Demo test' field appears in the features section. Verify file is under 500 lines.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:12.402Z"
          },
          {
            "id": 3,
            "title": "Create architecture.md architecture overview template",
            "description": "Create `templates/docs/architecture.md` with TLDR, tier overview, component map, and `{{ARCHITECTURE}}` and `{{PROJECT_NAME}}` placeholders.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/architecture.md`. Must include: TLDR section, a tier overview that acknowledges monolith/2-tier/3-tier/microservices options (using `{{ARCHITECTURE}}` placeholder), a component map section, and links to detail docs. Insert both `{{PROJECT_NAME}}` and `{{ARCHITECTURE}}` placeholders. The `fillTemplate()` function in `src/utils.ts` will substitute these at scaffold time. Follow doc_format.md standards.",
            "status": "done",
            "testStrategy": "Verify file exists. Verify both `{{ARCHITECTURE}}` and `{{PROJECT_NAME}}` placeholders are present. Verify `fillTemplate()` correctly substitutes them. Verify file is under 500 lines.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:18.797Z"
          },
          {
            "id": 4,
            "title": "Create api.md and cuj.md template files",
            "description": "Create `templates/docs/api.md` (API surface in table format with auth and error sections) and `templates/docs/cuj.md` (critical user journeys with step-by-step flows).",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/api.md` with: TLDR, a table with columns Endpoint | Method | Description | ADR ref | Source file, an Auth section, an Error shapes section, and `{{PROJECT_NAME}}` placeholder. Create `/workspaces/ai-dev-setup/templates/docs/cuj.md` with: TLDR, instructions on step-by-step user flow documentation, and one example journey section showing the expected format. Both must follow doc_format.md standards.",
            "status": "done",
            "testStrategy": "Verify both files exist. Verify api.md contains the 5-column table header and `{{PROJECT_NAME}}`. Verify cuj.md contains a sample journey section with steps.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:25.134Z"
          },
          {
            "id": 5,
            "title": "Create testing_strategy.md and onboarding.md template files",
            "description": "Create `templates/docs/testing_strategy.md` (testing philosophy with integration-first, demo-test definition, mock rules, quality gate) and `templates/docs/onboarding.md` (quick-start guide for new devs/agents).",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/testing_strategy.md` with: TLDR, Integration-first principle section, Demo-test definition, Mock justification rules (when mocks are acceptable), and Quality gate steps listing all mandatory pre-completion checks. Create `/workspaces/ai-dev-setup/templates/docs/onboarding.md` with: TLDR, Project context paragraph, Key commands table (Command | Description), a 'Where to find things' section mapping important files/dirs, and a First steps checklist. Both follow doc_format.md standards.",
            "status": "done",
            "testStrategy": "Verify both files exist. Verify testing_strategy.md contains 'Integration', 'Demo', 'Mock', and 'Quality gate' sections. Verify onboarding.md contains a commands table and a file map section.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:31.274Z"
          },
          {
            "id": 6,
            "title": "Create adr_template.md ADR format template",
            "description": "Create `templates/docs/adr_template.md` with the standard ADR structure including `{{NUMBER}}` and `{{TITLE}}` placeholders.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/adr_template.md` with exactly the structure: `# ADR-{{NUMBER}}: {{TITLE}}`, followed by fields: **Status:** Proposed, **Context:** (why decision was needed), **Decision:** (what was decided), **Consequences:** (trade-offs accepted). Keep the template concise — it is a reusable scaffold for individual ADR documents. Follow doc_format.md layout conventions.",
            "status": "done",
            "testStrategy": "Verify file exists. Verify `{{NUMBER}}` and `{{TITLE}}` placeholders are present. Verify all four required fields (Status, Context, Decision, Consequences) appear.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:37.422Z"
          },
          {
            "id": 7,
            "title": "Create tasks_simple.md simple task tracker template",
            "description": "Create `templates/docs/tasks_simple.md` providing a markdown-based task tracker with a summary table and individual task sections.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/templates/docs/tasks_simple.md` with exactly the structure: `# Task Tracker` heading, a `## Summary` section containing a table with columns # | Task | Status | Depends, and a `## Tasks` section with a `### Task 1: Example task` entry showing fields Status, Depends on, Success criteria, and Demo command. The Status column uses `[ ]` / `[x]` checkbox notation. Follow doc_format.md standards.",
            "status": "done",
            "testStrategy": "Verify file exists. Verify summary table with correct columns is present. Verify task entry contains 'Demo command' field. Verify `[ ]` checkbox notation is used.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:31:43.428Z"
          },
          {
            "id": 8,
            "title": "Write tests verifying all template files exist and meet constraints",
            "description": "Add a Vitest test file `test/templates.test.ts` that verifies all 9 template files exist, contain required placeholders, and none exceed 500 lines.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Create `/workspaces/ai-dev-setup/test/templates.test.ts`. Tests must: (1) use `fs.access()` to verify each of the 9 template files in `templates/docs/` exists; (2) read each file and assert line count <= 500; (3) assert prd.md contains 'Demo test'; (4) assert architecture.md contains `{{ARCHITECTURE}}` and `{{PROJECT_NAME}}`; (5) import `fillTemplate` from `src/utils.ts` and verify it substitutes `{{PROJECT_NAME}}` and `{{ARCHITECTURE}}` correctly using architecture.md content; (6) assert adr_template.md contains `{{NUMBER}}` and `{{TITLE}}`. Run `npm run format && npm run lint && npm run typecheck && npm run build && npm test` to pass quality gate before marking done.",
            "status": "done",
            "testStrategy": "Run `npm test` — all assertions in `test/templates.test.ts` must pass with zero failures. Run `npm run typecheck` to confirm no TypeScript errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:32:45.636Z"
          }
        ],
        "updatedAt": "2026-02-17T20:32:45.636Z"
      },
      {
        "id": "6",
        "title": "Create Rules, Skills, Hooks, and Commands Template Files",
        "description": "Create all template files for `.claude/rules/`, `.claude/skills/`, `.claude/hooks/`, and `.claude/commands/` directories. These implement F3 (Rules, Skills & Hooks) and F8 (Custom Claude Commands) template content.",
        "details": "**Rules templates (`templates/rules/`):**\n\n`general.md` — global scope: language version, package manager, coding style placeholders\n\n`docs.md` — scope: `docs/**`: how to read/update docs, follow doc_format.md standard\n\n`testing.md` — scope: `**/*.test.*`, `**/*.spec.*`:\n```markdown\n---\npaths:\n  - \"**/*.test.*\"\n  - \"**/*.spec.*\"\n---\n# Testing Rules\n## Default: Integration tests\n- Write tests that exercise real code paths. Use actual database connections, real HTTP requests, real file I/O.\n- Only mock external 3rd-party services. Add a comment: `// Mock: <service> — no local instance available`\n- If mocking more than 2 dependencies, reconsider: the test may be testing the wrong layer.\n## Demo checkpoints\n- Each feature task must produce at least one integration test demonstrating the feature end-to-end.\n- Name: `it('demo: user can sign up and access protected route')`\n## Smoke tests\n- Setup tasks get minimal smoke tests: app starts, health check passes, key dependencies connect.\n- Mark: `it('smoke: ...')`\n## Quality gate\n1. Format → 2. Lint → 3. Type-check → 4. Build → 5. ALL tests pass\n- Never delete a test to make the suite pass.\n```\n\n`git.md` — global scope: branch naming `feat/<task-id>-<desc>`, commit format `<task-id>: <what> — <value>`, one feature branch at a time\n\n`security.md` — scope: `src/auth/**`, `src/middleware/**`, `**/*secret*`: input validation, no credential logging, OWASP basics\n\n`api.md` — scope: `src/api/**`, `src/routes/**`: RESTful conventions, standard error shapes, input validation, references `@docs/api.md`\n\n`database.md` — scope: `src/db/**`, `src/models/**`, `**/migrations/**`: parameterized queries, migration discipline, no raw SQL\n\n`config.md` — scope: `**/*.config.*`, `**/.env*`: never hardcode secrets, use env vars, document in .env.example\n\n`agent-teams.md` — global scope: when to use teams, when not to, coordination rules\n\n**Skills templates (`templates/skills/`):**\n\n`testing.md` — activates on: test, coverage, demo — integration-first philosophy, demo-test patterns\n\n`commit.md` — activates on: commit, push, branch — full commit workflow: quality gate → format → lint → type-check → build → test → commit\n\n`task-workflow.md` — activates on: next task, pick up, start working — how to pick, implement, verify, and close a task\n\n**Hooks (`templates/hooks/`):**\n\n`pre-commit.sh`:\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\necho \"Running quality gate before commit...\"\nnpm run format --if-present 2>/dev/null || true\nnpm run lint --if-present || { echo \"BLOCK: Lint errors found.\"; exit 1; }\nnpm run typecheck --if-present || { echo \"BLOCK: Type errors found.\"; exit 1; }\nnpm run build --if-present || { echo \"BLOCK: Build failed.\"; exit 1; }\nnpm test --if-present || { echo \"BLOCK: Tests failing.\"; exit 1; }\necho \"Quality gate passed.\"\n```\n\n**Commands (`templates/commands/`):**\n\n`dev-next.md` — `/dev-next` command:\n1. Read docs/prd.md, docs/architecture.md, docs/adr/ for context\n2. Check dependency chain and last git commit\n3. Get next task from tracker\n4. Implement following .claude/rules/\n5. Commit: `<task-id>: <change> — <value>`\n6. Report and ask to continue\n\n`review.md` — `/review` command:\n1. Run `git diff`\n2. Check each changed file against applicable .claude/rules/\n3. Verify integration tests exist\n4. Run full quality gate\n5. Report: ready/needs-fixing\n\n**Boot prompt (`templates/boot-prompt.txt`):** Session startup instructions referencing project docs and chosen task tracker (uses `{{TASK_TRACKER}}` placeholder).",
        "testStrategy": "Test that all template files exist. Test `testing.md` rule contains 'Integration tests', 'Demo checkpoints', and 'Quality gate' sections. Test `pre-commit.sh` contains `--if-present` flag and correct exit codes. Test `dev-next.md` references `docs/prd.md` and `docs/adr/`. Test `git.md` contains the branch naming pattern `feat/<task-id>`.",
        "priority": "high",
        "dependencies": [
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create rules template files: general.md, docs.md, and git.md",
            "description": "Create three foundational rules template files in templates/rules/ covering global coding standards, documentation conventions, and git workflow rules.",
            "dependencies": [],
            "details": "Create templates/rules/general.md with frontmatter (global scope, no paths filter), language version placeholder ({{LANGUAGE_VERSION}}), package manager placeholder ({{PACKAGE_MANAGER}}), and coding style placeholders. Create templates/rules/docs.md with paths frontmatter scoped to docs/**, rules for reading/updating docs referencing doc_format.md standard. Create templates/rules/git.md with global scope, branch naming pattern feat/<task-id>-<desc>, commit format <task-id>: <what> — <value>, and one-feature-branch-at-a-time rule. All files follow the doc_format.md standard (TLDR, short sections). Use --- YAML frontmatter for path scoping where applicable.",
            "status": "pending",
            "testStrategy": "Verify all three files exist under templates/rules/. Check general.md contains {{LANGUAGE_VERSION}} and {{PACKAGE_MANAGER}} placeholders. Check docs.md has a paths frontmatter with docs/**. Check git.md contains 'feat/<task-id>' branch naming pattern.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create rules template files: testing.md, security.md, api.md, database.md, config.md",
            "description": "Create the remaining five rules template files covering testing philosophy, security requirements, API conventions, database safety, and configuration management.",
            "dependencies": [
              1
            ],
            "details": "Create templates/rules/testing.md with YAML frontmatter paths: ['**/*.test.*', '**/*.spec.*'], sections: Default Integration tests (real code paths, mock only external 3rd-party with comment), Demo checkpoints (feature task → at least one integration test, naming 'demo: ...'), Smoke tests ('smoke: ...' prefix), and Quality gate sequence (Format → Lint → Type-check → Build → ALL tests pass, never delete a test). Create templates/rules/security.md scoped to src/auth/**, src/middleware/**, **/*secret*: input validation, no credential logging, OWASP basics. Create templates/rules/api.md scoped to src/api/**, src/routes/**: RESTful conventions, standard error shapes, input validation, reference @docs/api.md. Create templates/rules/database.md scoped to src/db/**, src/models/**, **/migrations/**: parameterized queries, migration discipline, no raw SQL. Create templates/rules/config.md scoped to **/*.config.*, **/.env*: never hardcode secrets, use env vars, document in .env.example.",
            "status": "pending",
            "testStrategy": "Verify all five files exist. Check testing.md contains 'Integration tests', 'Demo checkpoints', and 'Quality gate' sections. Check security.md has path scope for src/auth/**. Check api.md references @docs/api.md. Check database.md contains 'parameterized queries'. Check config.md contains '.env.example'.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create rules template file: agent-teams.md",
            "description": "Create the agent-teams.md rules template file defining when and how to use multi-agent team coordination in Claude Code.",
            "dependencies": [
              1
            ],
            "details": "Create templates/rules/agent-teams.md with global scope (no path filter). Content should include: when to use agent teams (large parallel tasks, independent sub-problems), when NOT to use teams (sequential dependencies, small tasks, shared mutable state), coordination rules (orchestrator pattern, task hand-off format, status reporting conventions), and reference to the three-tier system (task-orchestrator, task-executor, task-checker). Follow doc_format.md: TLDR at top, TOC, sections under 30 lines, tables preferred for structured data.",
            "status": "pending",
            "testStrategy": "Verify templates/rules/agent-teams.md exists. Check it contains sections for 'when to use' and 'when not to use' teams. Check it references the orchestrator/executor/checker pattern.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create skills template files: testing.md, commit.md, and task-workflow.md",
            "description": "Create three skills template files in templates/skills/ that define reusable Claude Code skill activations for testing, committing, and task workflow.",
            "dependencies": [],
            "details": "Create templates/skills/testing.md: frontmatter with activateOn: [test, coverage, demo], content covering integration-first philosophy, how to write demo tests (naming: 'demo: user can ...'), how to write smoke tests, coverage expectations. Create templates/skills/commit.md: frontmatter with activateOn: [commit, push, branch], full commit workflow steps: 1) run quality gate (format → lint → typecheck → build → test), 2) stage changes, 3) write commit message in format '<task-id>: <what> — <value>', 4) push to feature branch. Create templates/skills/task-workflow.md: frontmatter with activateOn: [next task, pick up, start working], steps: pick next available task from tracker, read task details, explore affected code, implement following .claude/rules/, self-verify against test strategy, mark done.",
            "status": "pending",
            "testStrategy": "Verify all three files exist under templates/skills/. Check testing.md has activateOn including 'demo'. Check commit.md includes the quality gate steps sequence. Check task-workflow.md references .claude/rules/ and task tracker.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create hooks template file: pre-commit.sh and boot-prompt.txt",
            "description": "Create the pre-commit.sh hook template and boot-prompt.txt session startup file in their respective template directories.",
            "dependencies": [],
            "details": "Create templates/hooks/pre-commit.sh as an executable bash script with shebang #!/usr/bin/env bash and set -euo pipefail. Script content: echo 'Running quality gate before commit...', then run each step with --if-present: npm run format (|| true, non-blocking), npm run lint (exit 1 on failure with BLOCK message), npm run typecheck (exit 1 on failure), npm run build (exit 1 on failure), npm test (exit 1 on failure). End with echo 'Quality gate passed.'. The FileDescriptor for this file must set executable: true so writeFiles() calls chmod 755. Create templates/boot-prompt.txt (or .md) with session startup instructions: read docs/prd.md, docs/architecture.md, docs/adr/ for context, identify current task from {{TASK_TRACKER}}, apply .claude/rules/, check last git commit, report ready status.",
            "status": "pending",
            "testStrategy": "Verify templates/hooks/pre-commit.sh exists and is executable (chmod 755). Check it contains --if-present flag for all npm run commands. Check it uses correct exit 1 with BLOCK messages. Verify boot-prompt.txt exists and contains {{TASK_TRACKER}} placeholder.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create commands template files: dev-next.md and review.md",
            "description": "Create two custom Claude Code slash command template files in templates/commands/ for the /dev-next and /review workflow commands.",
            "dependencies": [],
            "details": "Create templates/commands/dev-next.md defining the /dev-next command: step 1 read docs/prd.md, docs/architecture.md, and docs/adr/ for context; step 2 check dependency chain and last git commit; step 3 get next task from tracker (references {{TASK_TRACKER}}); step 4 implement following .claude/rules/; step 5 commit using format '<task-id>: <change> — <value>'; step 6 report summary and ask user to continue. Create templates/commands/review.md defining the /review command: step 1 run git diff; step 2 check each changed file against applicable .claude/rules/; step 3 verify integration tests exist for changed functionality; step 4 run full quality gate (format → lint → typecheck → build → test); step 5 report result as either 'ready to merge' or 'needs-fixing' with specific issues listed.",
            "status": "pending",
            "testStrategy": "Verify both files exist under templates/commands/. Check dev-next.md references docs/prd.md and docs/adr/. Check dev-next.md contains the commit format pattern '<task-id>'. Check review.md references .claude/rules/ and includes quality gate steps. Check review.md produces a ready/needs-fixing report format.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-02-17T20:50:49.340Z"
      },
      {
        "id": "7",
        "title": "Implement MCP Configuration Generator",
        "description": "Create `src/generators/mcp-json.ts` — a pure function that takes `ProjectConfig` and returns `FileDescriptor[]` for both `.mcp.json` (Claude Code) and `.vscode/mcp.json` (VS Code/Copilot). These two files use different JSON schemas.",
        "details": "Create `src/generators/mcp-json.ts`:\n\n```typescript\nimport { ProjectConfig, FileDescriptor } from '../types.js';\nimport { getSelectedServers } from '../registry.js';\n\ninterface McpServerConfig {\n  command: string;\n  args: string[];\n  env?: Record<string, string>;\n}\n\nfunction buildClaudeCodeMcpConfig(\n  config: ProjectConfig\n): Record<string, McpServerConfig> {\n  const servers = getSelectedServers(config.selectedMcps);\n  const result: Record<string, McpServerConfig> = {};\n  for (const server of servers) {\n    result[server.claudeMcpName] = {\n      command: 'npx',\n      args: server.args ?? ['-y', server.npmPackage],\n      ...(server.env && Object.keys(server.env).length > 0\n        ? { env: server.env }\n        : {}),\n    };\n  }\n  return result;\n}\n\nfunction buildVscodeMcpConfig(\n  config: ProjectConfig\n): Record<string, unknown> {\n  const servers = getSelectedServers(config.selectedMcps);\n  const result: Record<string, unknown> = {};\n  for (const server of servers) {\n    const envBlock: Record<string, string> = {};\n    for (const [key] of Object.entries(server.env ?? {})) {\n      envBlock[key] = `\\${env:${key}}`;\n    }\n    result[server.claudeMcpName] = {\n      command: 'npx',\n      args: server.args ?? ['-y', server.npmPackage],\n      cwd: '${workspaceFolder}',\n      ...(Object.keys(envBlock).length > 0 ? { env: envBlock } : {}),\n      envFile: '${workspaceFolder}/.env',\n      type: 'stdio',\n    };\n  }\n  return result;\n}\n\nexport function generateMcpJson(config: ProjectConfig): FileDescriptor[] {\n  return [\n    {\n      path: '.mcp.json',\n      content: JSON.stringify(\n        { mcpServers: buildClaudeCodeMcpConfig(config) },\n        null,\n        2\n      ),\n    },\n    {\n      path: '.vscode/mcp.json',\n      content: JSON.stringify(\n        { servers: buildVscodeMcpConfig(config) },\n        null,\n        2\n      ),\n    },\n  ];\n}\n```\n\nKey design constraints:\n- `.mcp.json` root key: `\"mcpServers\"`\n- `.vscode/mcp.json` root key: `\"servers\"`, adds `cwd`, `envFile`, `type: \"stdio\"`, env vars use `${env:VAR}` syntax\n- Both use `npx` as command\n- Pure function — no filesystem access",
        "testStrategy": "Unit test `test/generators/mcp-json.test.ts`:\n1. With `selectedMcps: ['taskmaster']` → `.mcp.json` has `mcpServers.taskmaster-ai`, `.vscode/mcp.json` has `servers.taskmaster-ai` with `cwd: '${workspaceFolder}'`\n2. With `selectedMcps: ['taskmaster', 'beads']` → both configs have 2 entries\n3. `.mcp.json` does NOT have `cwd` field\n4. `.vscode/mcp.json` env values use `${env:ANTHROPIC_API_KEY}` format\n5. Both outputs are valid JSON (JSON.parse succeeds)\n6. Generator returns exactly 2 FileDescriptors",
        "priority": "high",
        "dependencies": [
          "3",
          "4"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-17T21:08:12.934Z"
      },
      {
        "id": "8",
        "title": "Implement CLAUDE.md Generator",
        "description": "Create `src/generators/claude-md.ts` — generates `CLAUDE.md` and `CLAUDE_MCP.md` tailored to the user's chosen task tracker and selected MCPs. The generated CLAUDE.md must reference the actual docs and rules that were generated.",
        "details": "Create `src/generators/claude-md.ts`:\n\n```typescript\nimport { ProjectConfig, FileDescriptor } from '../types.js';\nimport { getSelectedServers } from '../registry.js';\n\nfunction buildTaskTrackerInstructions(config: ProjectConfig): string {\n  switch (config.taskTracker) {\n    case 'taskmaster':\n      return `## Task Tracker: Task Master AI\n\n**Import Task Master's workflow commands:**\n@./.taskmaster/CLAUDE.md\n\n- \\`task-master next\\` — Get next task\n- \\`task-master show <id>\\` — View task details  \n- \\`task-master set-status --id=<id> --status=done\\` — Mark complete\n- \\`task-master update-subtask --id=<id> --prompt=\"...\"\\` — Log progress`;\n\n    case 'beads':\n      return `## Task Tracker: Beads\n\n**Beads tools:** beads_ready, beads_create, beads_show, beads_update, beads_close, beads_dep_add, beads_dep_tree, beads_sync\n\n- \\`bd show\\` — View current tasks\n- \\`bd next\\` — Get next task\n- Reference tasks in commits: \\`bd-<hash>: <change> — <value>\\`\n- Run \\`bd sync\\` before push`;\n\n    case 'markdown':\n      return `## Task Tracker: Simple Markdown\n\n- Edit \\`TASKS.md\\` directly\n- Mark tasks with \\`[x]\\` when done\n- Add demo command for each task before marking done`;\n  }\n}\n\nfunction buildMcpSection(config: ProjectConfig): string {\n  const servers = getSelectedServers(config.selectedMcps);\n  if (servers.length === 0) return '';\n  const lines = servers.map(s => `- **${s.claudeMcpName}**: ${s.description}`);\n  return `## MCP Servers\\n\\n${lines.join('\\n')}`;\n}\n\nexport function generateClaudeMd(config: ProjectConfig): FileDescriptor[] {\n  const docImports = config.generateDocs\n    ? `\n## Project Documentation\n@docs/doc_format.md\n@docs/prd.md\n@docs/architecture.md\n@docs/testing_strategy.md\n@docs/onboarding.md\n`.trim()\n    : '';\n\n  const rulesRef = config.generateRules\n    ? `\\n## Agent Rules\\nPath-scoped rules in \\`.claude/rules/\\` auto-load based on the file being edited.`\n    : '';\n\n  const claudeMdContent = `<!-- SETUP-AI-MANAGED — regenerated by ai-init -->\n\n# Project Instructions for Claude Code\n\n${docImports}\n\n${buildTaskTrackerInstructions(config)}\n\n${buildMcpSection(config)}\n\n${rulesRef}\n\n## Quality Gate\n\nBefore marking any task done:\n1. Format: \\`npm run format\\`\n2. Lint: \\`npm run lint\\`  \n3. Type-check: \\`npm run typecheck\\`\n4. Build: \\`npm run build\\`\n5. Test: \\`npm test\\`\n`.trim();\n\n  const files: FileDescriptor[] = [\n    { path: 'CLAUDE.md', content: claudeMdContent },\n  ];\n\n  // Generate CLAUDE_MCP.md with MCP tool docs\n  const servers = getSelectedServers(config.selectedMcps);\n  if (servers.length > 0) {\n    const mcpDocs = servers\n      .map(s => `## ${s.claudeMcpName}\\n\\n${s.description}\\n\\n**Package:** \\`${s.npmPackage}\\``)\n      .join('\\n\\n---\\n\\n');\n    files.push({\n      path: 'CLAUDE_MCP.md',\n      content: `# MCP Servers Available\\n\\n${mcpDocs}\\n`,\n    });\n  }\n\n  return files;\n}\n```\n\nThe CLAUDE.md must use `@import` syntax for doc references so Claude Code auto-loads them. Tracker-specific instructions must be accurate — agents rely on these to know which commands to use.",
        "testStrategy": "Unit test `test/generators/claude-md.test.ts`:\n1. With `taskTracker: 'taskmaster'` → output contains `@./.taskmaster/CLAUDE.md` and `task-master next`\n2. With `taskTracker: 'beads'` → output contains `beads_ready` and `bd sync`\n3. With `taskTracker: 'markdown'` → output contains `TASKS.md`\n4. With `generateDocs: true` → CLAUDE.md contains `@docs/prd.md`\n5. With `selectedMcps: ['taskmaster', 'context7']` → CLAUDE_MCP.md lists both servers\n6. With `selectedMcps: []` → only CLAUDE.md returned (no CLAUDE_MCP.md)\n7. CLAUDE.md contains the Quality Gate section",
        "priority": "high",
        "dependencies": [
          "3",
          "4",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/generators/claude-md.ts with core generator function",
            "description": "Implement the main generateClaudeMd function and helper functions in src/generators/claude-md.ts following the pattern established by mcp-json.ts.",
            "dependencies": [],
            "details": "Create /workspaces/ai-dev-setup/src/generators/claude-md.ts with:\n1. Import ProjectConfig, FileDescriptor from '../types.js' and getSelectedServers from '../registry.js'\n2. Implement buildTaskTrackerInstructions(config: ProjectConfig): string with switch on config.taskTracker for 'taskmaster' (includes @./.taskmaster/CLAUDE.md import and task-master commands), 'beads' (includes beads_ready, bd sync, commit format), and 'markdown' (TASKS.md editing with [x] markers)\n3. Implement buildMcpSection(config: ProjectConfig): string that calls getSelectedServers and formats each server's claudeMcpName and description\n4. Implement exported generateClaudeMd(config: ProjectConfig): FileDescriptor[] that:\n   - Conditionally builds docImports block (if config.generateDocs) with @docs/doc_format.md, @docs/prd.md, @docs/architecture.md, @docs/testing_strategy.md, @docs/onboarding.md\n   - Conditionally builds rulesRef block (if config.generateRules) mentioning .claude/rules/ path-scoped rules\n   - Assembles CLAUDE.md string with managed header comment, doc imports, task tracker section, MCP section, rules ref, and Quality Gate (format, lint, typecheck, build, test steps)\n   - Returns FileDescriptor array with CLAUDE.md always included\n   - Adds CLAUDE_MCP.md only when config.selectedMcps is non-empty, using getSelectedServers to build per-server docs with claudeMcpName, description, and npmPackage",
            "status": "done",
            "testStrategy": "Manual inspection of output for each tracker type before writing automated tests",
            "updatedAt": "2026-02-17T21:12:39.898Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add accurate beads tracker instructions with correct command syntax",
            "description": "Ensure the beads task tracker branch in buildTaskTrackerInstructions contains accurate beads MCP tool names and bd CLI commands as documented in the beads MCP server entry in registry.ts.",
            "dependencies": [
              1
            ],
            "details": "Read /workspaces/ai-dev-setup/src/registry.ts to find the exact beads server description and any documented commands. Read the beads entry in MCP_REGISTRY to get claudeMcpName, description, and any tool listings. Update the 'beads' case in buildTaskTrackerInstructions to include:\n- All beads MCP tool names: beads_ready, beads_create, beads_show, beads_update, beads_close, beads_dep_add, beads_dep_tree, beads_sync\n- CLI equivalents: bd show, bd next, bd sync\n- Commit message format: bd-<hash>: <change> — <value>\n- Instruction to run bd sync before push\nAlso verify the 'taskmaster' case references @./.taskmaster/CLAUDE.md with exact @ import syntax Claude Code recognizes, and the 'markdown' case references TASKS.md with [x] completion format and demo command requirement.",
            "status": "done",
            "testStrategy": "Check each tracker branch output against known command documentation",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:12:46.368Z"
          },
          {
            "id": 3,
            "title": "Write unit tests in test/generators/claude-md.test.ts",
            "description": "Create comprehensive unit tests covering all 7 required scenarios from the task specification, following the patterns in test/generators/mcp-json.test.ts.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create /workspaces/ai-dev-setup/test/generators/claude-md.test.ts:\n1. Import generateClaudeMd from '../../src/generators/claude-md.js' and defaultConfig from '../../src/defaults.js'\n2. Test 1: taskTracker 'taskmaster' → CLAUDE.md content contains '@./.taskmaster/CLAUDE.md' and 'task-master next'\n3. Test 2: taskTracker 'beads' → CLAUDE.md content contains 'beads_ready' and 'bd sync'\n4. Test 3: taskTracker 'markdown' → CLAUDE.md content contains 'TASKS.md'\n5. Test 4: generateDocs true → CLAUDE.md content contains '@docs/prd.md'\n6. Test 5: selectedMcps ['taskmaster', 'context7'] → result has 2 files, CLAUDE_MCP.md content lists both server names\n7. Test 6: selectedMcps [] → result has exactly 1 file (only CLAUDE.md, no CLAUDE_MCP.md)\n8. Test 7: CLAUDE.md always contains a Quality Gate section with 'npm run format' and 'npm run lint'\n9. Additional tests: generateRules true → content contains '.claude/rules/', generateDocs false → content does NOT contain '@docs/prd.md'\nUse defaultConfig() with spread overrides for each test case as done in mcp-json.test.ts.",
            "status": "done",
            "testStrategy": "Run npm test after creating the file to verify all tests pass",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:13:37.381Z"
          },
          {
            "id": 4,
            "title": "Run format, lint, typecheck, and build to verify implementation",
            "description": "Execute the full quality gate pipeline to ensure the new generator file and test file have no errors before marking the task done.",
            "dependencies": [
              3
            ],
            "details": "Run the following commands in sequence from /workspaces/ai-dev-setup:\n1. npm run format — auto-format src/generators/claude-md.ts and test/generators/claude-md.test.ts with Prettier\n2. npm run lint — run ESLint and fix any reported errors (not just warnings)\n3. npm run typecheck — run tsc --noEmit to catch any TypeScript type errors; common issues include missing return types, wrong import paths (.js vs .ts), or ProjectConfig field mismatches\n4. npm run build — compile the project; ensure no compilation errors in the new file\nIf any step fails, fix the issue in the relevant source file and re-run from that step. Pay particular attention to: import paths using .js extension (ESM requirement), correct ProjectConfig field names matching src/types.ts, and no implicit any types.",
            "status": "done",
            "testStrategy": "All four commands must exit with code 0",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:14:24.350Z"
          },
          {
            "id": 5,
            "title": "Run test suite and fix any failing tests",
            "description": "Execute npm test to run all unit tests including the new claude-md.test.ts, verify all 7+ new tests pass along with existing tests.",
            "dependencies": [
              4
            ],
            "details": "Run 'npm test' from /workspaces/ai-dev-setup and verify:\n1. All existing tests (mcp-json, utils, templates) continue to pass — no regressions\n2. All 7+ new tests in test/generators/claude-md.test.ts pass\n3. If any test fails, identify the mismatch: either fix the generator logic in src/generators/claude-md.ts or fix incorrect test assertions\nCommon failure modes to watch for:\n- Content assertion using wrong substring (check exact casing and spacing in output)\n- CLAUDE_MCP.md not being generated when selectedMcps is non-empty (check getSelectedServers return value)\n- @ import syntax not matching exactly what Claude Code expects\n- Quality Gate section missing or incomplete\nAfter all tests pass, use update_subtask to log that implementation is complete and all checks passed, then set task 8 status to done.",
            "status": "done",
            "testStrategy": "npm test must exit with code 0 and show all new tests as passing",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:14:24.361Z"
          }
        ],
        "updatedAt": "2026-02-17T21:14:24.361Z"
      },
      {
        "id": "9",
        "title": "Implement Document Scaffolding Generator",
        "description": "Create `src/generators/docs.ts` — reads templates from `templates/docs/` and returns `FileDescriptor[]` for all project documentation files. Applies `fillTemplate()` with project-specific values. Implements F2.",
        "details": "Create `src/generators/docs.ts`:\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { ProjectConfig, FileDescriptor } from '../types.js';\nimport { fillTemplate } from '../utils.js';\n\nconst TEMPLATES_DIR = new URL('../../templates/docs', import.meta.url).pathname;\n\nasync function readTemplate(name: string): Promise<string> {\n  return fs.readFile(path.join(TEMPLATES_DIR, name), 'utf8');\n}\n\nexport async function generateDocs(config: ProjectConfig): Promise<FileDescriptor[]> {\n  const vars: Record<string, string> = {\n    PROJECT_NAME: config.projectName,\n    ARCHITECTURE: config.architecture,\n    YEAR: new Date().getFullYear().toString(),\n    TASK_TRACKER: config.taskTracker,\n  };\n\n  const files: FileDescriptor[] = [];\n\n  // Core docs always generated\n  const coreDocs = [\n    { template: 'doc_format.md', output: 'docs/doc_format.md' },\n    { template: 'prd.md', output: 'docs/prd.md' },\n    { template: 'architecture.md', output: 'docs/architecture.md' },\n    { template: 'cuj.md', output: 'docs/cuj.md' },\n    { template: 'testing_strategy.md', output: 'docs/testing_strategy.md' },\n    { template: 'onboarding.md', output: 'docs/onboarding.md' },\n  ];\n\n  for (const { template, output } of coreDocs) {\n    const content = await readTemplate(template);\n    files.push({ path: output, content: fillTemplate(content, vars) });\n  }\n\n  // API docs — only if hasApiDocs\n  if (config.hasApiDocs) {\n    const content = await readTemplate('api.md');\n    files.push({ path: 'docs/api.md', content: fillTemplate(content, vars) });\n  }\n\n  // ADR directory with example ADR and template\n  const adrTemplate = await readTemplate('adr_template.md');\n  files.push({\n    path: 'docs/adr/adr_template.md',\n    content: fillTemplate(adrTemplate, { ...vars, NUMBER: 'NNN', TITLE: 'Decision Title' }),\n  });\n\n  // Task tracker file — only for simple markdown\n  if (config.taskTracker === 'markdown') {\n    const content = await readTemplate('tasks_simple.md');\n    files.push({\n      path: 'TASKS.md',\n      content: fillTemplate(content, vars),\n    });\n  }\n\n  return files;\n}\n```\n\nThe generator must be async because it reads template files. Note: `import.meta.url` requires `\"type\": \"module\"` in package.json and ESM output from TypeScript.",
        "testStrategy": "Unit test `test/generators/docs.test.ts`:\n1. With `generateDocs: true, hasApiDocs: false` → output contains `docs/doc_format.md`, `docs/prd.md`, `docs/architecture.md` but NOT `docs/api.md`\n2. With `hasApiDocs: true` → output contains `docs/api.md`\n3. With `taskTracker: 'markdown'` → output contains `TASKS.md`\n4. With `taskTracker: 'taskmaster'` → output does NOT contain `TASKS.md`\n5. `{{PROJECT_NAME}}` is replaced with config.projectName in all generated content\n6. ADR template file is always included",
        "priority": "high",
        "dependencies": [
          "4",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/generators/docs.ts with core template reading infrastructure",
            "description": "Create the docs generator file with the TEMPLATES_DIR constant using import.meta.url and the private readTemplate helper function. Set up the module structure and imports.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/src/generators/docs.ts` with the following structure:\n\n1. Import `fs` from `node:fs/promises`, `path` from `node:path`, `ProjectConfig` and `FileDescriptor` from `../types.js`, and `fillTemplate` from `../utils.js`.\n2. Define `TEMPLATES_DIR` as `new URL('../../templates/docs', import.meta.url).pathname` — this resolves to the `templates/docs/` directory relative to the compiled output.\n3. Implement `async function readTemplate(name: string): Promise<string>` that calls `fs.readFile(path.join(TEMPLATES_DIR, name), 'utf8')`.\n4. Export an empty stub `generateDocs` function returning `Promise<FileDescriptor[]>` for now.\n\nVerify `package.json` already has `\"type\": \"module\"` and that the TypeScript config emits ESM (check `tsconfig.json` for `\"module\": \"NodeNext\"` or similar). The templates already exist at `templates/docs/` with these files: `doc_format.md`, `prd.md`, `architecture.md`, `cuj.md`, `testing_strategy.md`, `onboarding.md`, `api.md`, `adr_template.md`, `tasks_simple.md`.",
            "status": "done",
            "testStrategy": "Manually verify the file compiles with `npx tsc --noEmit`. Confirm `TEMPLATES_DIR` resolves to the correct absolute path by checking the URL construction logic.",
            "updatedAt": "2026-02-17T21:18:55.429Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement core docs generation loop in generateDocs",
            "description": "Implement the main generateDocs export function: build the vars record from ProjectConfig, iterate over the six core doc templates, read each, apply fillTemplate, and return FileDescriptor[].",
            "dependencies": [
              1
            ],
            "details": "Fill in the `generateDocs(config: ProjectConfig): Promise<FileDescriptor[]>` function body:\n\n1. Build the vars record:\n```typescript\nconst vars: Record<string, string> = {\n  PROJECT_NAME: config.projectName,\n  ARCHITECTURE: config.architecture,\n  YEAR: new Date().getFullYear().toString(),\n  TASK_TRACKER: config.taskTracker,\n};\n```\n2. Define the six core docs array:\n```typescript\nconst coreDocs = [\n  { template: 'doc_format.md', output: 'docs/doc_format.md' },\n  { template: 'prd.md', output: 'docs/prd.md' },\n  { template: 'architecture.md', output: 'docs/architecture.md' },\n  { template: 'cuj.md', output: 'docs/cuj.md' },\n  { template: 'testing_strategy.md', output: 'docs/testing_strategy.md' },\n  { template: 'onboarding.md', output: 'docs/onboarding.md' },\n];\n```\n3. Loop over coreDocs: for each, call `readTemplate(template)`, then `fillTemplate(content, vars)`, then push `{ path: output, content }` to the files array.\n4. Return the files array (without optional entries — those come next).\n\nNote: `fillTemplate` replaces `{{PLACEHOLDER}}` markers. Inspect `templates/docs/prd.md` and others to confirm which placeholder variables they use, and ensure all are covered by the vars record.",
            "status": "done",
            "testStrategy": "Write a quick smoke test: call generateDocs with a minimal ProjectConfig and assert the result contains exactly 6 FileDescriptors with paths starting with 'docs/'.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:20:02.233Z"
          },
          {
            "id": 3,
            "title": "Add conditional API docs, ADR template, and markdown task tracker entries",
            "description": "Extend generateDocs to conditionally push docs/api.md when hasApiDocs is true, always push the ADR template to docs/adr/adr_template.md, and push TASKS.md when taskTracker is 'markdown'.",
            "dependencies": [
              2
            ],
            "details": "After the core docs loop, add three conditional blocks:\n\n1. **API docs** (conditional on `config.hasApiDocs`):\n```typescript\nif (config.hasApiDocs) {\n  const content = await readTemplate('api.md');\n  files.push({ path: 'docs/api.md', content: fillTemplate(content, vars) });\n}\n```\n\n2. **ADR template** (always included — gives a starting template for Architecture Decision Records):\n```typescript\nconst adrTemplate = await readTemplate('adr_template.md');\nfiles.push({\n  path: 'docs/adr/adr_template.md',\n  content: fillTemplate(adrTemplate, { ...vars, NUMBER: 'NNN', TITLE: 'Decision Title' }),\n});\n```\nNote: `adr_template.md` contains `{{NUMBER}}` and `{{TITLE}}` placeholders in addition to standard vars — these must be merged into the vars record for the fillTemplate call.\n\n3. **Markdown task tracker** (conditional on `config.taskTracker === 'markdown'`):\n```typescript\nif (config.taskTracker === 'markdown') {\n  const content = await readTemplate('tasks_simple.md');\n  files.push({ path: 'TASKS.md', content: fillTemplate(content, vars) });\n}\n```\n\n`tasks_simple.md` uses `{{PROJECT_NAME}}` which is already in vars.",
            "status": "done",
            "testStrategy": "Test the three conditional branches: (1) hasApiDocs=false → no api.md; (2) hasApiDocs=true → api.md present; (3) taskTracker='markdown' → TASKS.md present; (4) adr_template.md always present with 'NNN' placeholder resolved.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:20:02.241Z"
          },
          {
            "id": 4,
            "title": "Create test/generators/docs.test.ts with full unit test suite",
            "description": "Create the unit test file for the docs generator, covering all documented test cases: core docs presence, API docs conditionality, ADR template, and markdown task tracker file.",
            "dependencies": [
              3
            ],
            "details": "Create `/workspaces/ai-dev-setup/test/generators/docs.test.ts` following the same pattern as `test/generators/claude-md.test.ts`:\n\n```typescript\nimport { describe, it, expect } from 'vitest';\nimport { generateDocs } from '../../src/generators/docs.js';\nimport { defaultConfig } from '../../src/defaults.js';\nimport type { ProjectConfig } from '../../src/types.js';\n\nfunction makeConfig(overrides: Partial<ProjectConfig> = {}): ProjectConfig {\n  return { ...defaultConfig('/tmp/test-project'), ...overrides };\n}\n\ndescribe('generateDocs', () => {\n  it('returns core 6 docs when generateDocs=true, hasApiDocs=false', async () => {\n    const result = await generateDocs(makeConfig({ generateDocs: true, hasApiDocs: false }));\n    const paths = result.map(f => f.path);\n    expect(paths).toContain('docs/doc_format.md');\n    expect(paths).toContain('docs/prd.md');\n    expect(paths).toContain('docs/architecture.md');\n    expect(paths).not.toContain('docs/api.md');\n  });\n\n  it('includes docs/api.md when hasApiDocs=true', async () => {\n    const result = await generateDocs(makeConfig({ hasApiDocs: true }));\n    expect(result.map(f => f.path)).toContain('docs/api.md');\n  });\n\n  it('always includes docs/adr/adr_template.md', async () => {\n    const result = await generateDocs(makeConfig());\n    expect(result.map(f => f.path)).toContain('docs/adr/adr_template.md');\n  });\n\n  it('includes TASKS.md when taskTracker is markdown', async () => {\n    const result = await generateDocs(makeConfig({ taskTracker: 'markdown' }));\n    expect(result.map(f => f.path)).toContain('TASKS.md');\n  });\n\n  it('does NOT include TASKS.md when taskTracker is not markdown', async () => {\n    const result = await generateDocs(makeConfig({ taskTracker: 'taskmaster' }));\n    expect(result.map(f => f.path)).not.toContain('TASKS.md');\n  });\n\n  it('applies PROJECT_NAME substitution in template content', async () => {\n    const result = await generateDocs(makeConfig({ projectName: 'my-cool-app' }));\n    const tasksFile = result.find(f => f.path === 'TASKS.md');\n    // tasks_simple.md uses {{PROJECT_NAME}}\n    // This test only runs if taskTracker='markdown'; adjust makeConfig accordingly\n  });\n});\n```\n\nAlso verify the test works with the actual template files on disk (no mocking needed since it's a unit test that reads real files).",
            "status": "done",
            "testStrategy": "Run `npm test -- test/generators/docs.test.ts` and confirm all tests pass. Verify real template files are read from disk without errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:20:02.246Z"
          },
          {
            "id": 5,
            "title": "Run quality gate checks and fix any issues",
            "description": "Run format, lint, type-check, build, and full test suite. Fix any failures before marking the task done.",
            "dependencies": [
              4
            ],
            "details": "Execute the full quality gate sequence as required by project standards:\n\n1. `npm run format` — run Prettier to format all files\n2. `npm run lint` — run ESLint and fix all errors\n3. `npm run typecheck` — run `tsc --noEmit` and fix any type errors\n4. `npm run build` — compile TypeScript to the output directory and verify zero errors\n5. `npm test` — run the full Vitest suite including the new `test/generators/docs.test.ts`\n\nCommon issues to watch for:\n- ESM import paths must use `.js` extension (e.g., `from '../types.js'`)\n- `import.meta.url` requires the file to be compiled as ESM; verify tsconfig `module` and `moduleResolution` settings\n- Template file path resolution: in tests, the TEMPLATES_DIR will be computed relative to the compiled JS output location, so verify dist/ output structure has access to templates/ (may require tsconfig `include` or npm `files` config to copy templates)\n- If templates are not copied to dist/, the test will fail with ENOENT — in that case, add a build step to copy templates or use a path relative to the project root\n\nLog results using `update_subtask` and set status to done only after all checks pass.",
            "status": "done",
            "testStrategy": "All 5 quality gate steps must exit with code 0. The full test suite must pass with no failures. Zero TypeScript errors. Zero ESLint errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:20:02.254Z"
          }
        ],
        "updatedAt": "2026-02-17T21:20:02.254Z"
      },
      {
        "id": "10",
        "title": "Implement Rules, Skills, and Hooks Generators",
        "description": "Create `src/generators/rules.ts`, `src/generators/skills.ts`, and `src/generators/hooks.ts` — pure functions generating the `.claude/` subdirectory files that implement F3. Rules are path-scoped and conditionally generated based on project config.",
        "details": "**`src/generators/rules.ts`:**\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { ProjectConfig, FileDescriptor } from '../types.js';\nimport { fillTemplate } from '../utils.js';\n\nconst RULES_DIR = new URL('../../templates/rules', import.meta.url).pathname;\n\nasync function readRule(name: string): Promise<string> {\n  return fs.readFile(path.join(RULES_DIR, name), 'utf8');\n}\n\nexport async function generateRules(config: ProjectConfig): Promise<FileDescriptor[]> {\n  const vars = { PROJECT_NAME: config.projectName, TASK_TRACKER: config.taskTracker };\n  const files: FileDescriptor[] = [];\n\n  // Always generated\n  const alwaysRules = ['general.md', 'docs.md', 'testing.md', 'git.md', 'security.md', 'config.md'];\n  for (const name of alwaysRules) {\n    files.push({\n      path: `.claude/rules/${name}`,\n      content: fillTemplate(await readRule(name), vars),\n    });\n  }\n\n  // Conditional — API rules only if api docs selected\n  if (config.hasApiDocs) {\n    const content = await readRule('api.md');\n    const withImport = config.generateDocs\n      ? content + '\\n\\n@docs/api.md'\n      : content;\n    files.push({ path: '.claude/rules/api.md', content: fillTemplate(withImport, vars) });\n  }\n\n  // Conditional — database rules only if project has DB\n  if (config.hasDatabase) {\n    files.push({\n      path: '.claude/rules/database.md',\n      content: fillTemplate(await readRule('database.md'), vars),\n    });\n  }\n\n  // Conditional — agent teams rule only if opted in\n  if (config.agentTeamsEnabled) {\n    files.push({\n      path: '.claude/rules/agent-teams.md',\n      content: fillTemplate(await readRule('agent-teams.md'), vars),\n    });\n  }\n\n  return files;\n}\n```\n\n**`src/generators/skills.ts`:**\n```typescript\nexport async function generateSkills(config: ProjectConfig): Promise<FileDescriptor[]> {\n  // Read skill templates and return FileDescriptors\n  // All three skills always generated: testing.md, commit.md, task-workflow.md\n  // task-workflow.md gets TASK_TRACKER substituted\n}\n```\n\n**`src/generators/hooks.ts`:**\n```typescript\nexport async function generateHooks(config: ProjectConfig): Promise<FileDescriptor[]> {\n  // Generate .claude/hooks/pre-commit.sh (executable)\n  // Generate .claude/settings.json entry for PreToolUse hook\n  return [\n    {\n      path: '.claude/hooks/pre-commit.sh',\n      content: preCommitContent,\n      executable: true,\n    },\n  ];\n}\n```\n\n`.claude/settings.json` update: add the hook matcher. Read existing settings.json if present, merge the hooks entry.",
        "testStrategy": "Unit tests `test/generators/rules.test.ts`, `skills.test.ts`, `hooks.test.ts`:\n1. Rules: With `hasApiDocs: false` → no `api.md` rule generated\n2. Rules: With `hasApiDocs: true, generateDocs: true` → `api.md` rule contains `@docs/api.md`\n3. Rules: With `agentTeamsEnabled: true` → `agent-teams.md` rule generated\n4. Rules: Always includes `testing.md` with 'Integration tests' and 'Demo checkpoints' sections\n5. Skills: All 3 skill files generated, `task-workflow.md` has tracker-specific content\n6. Hooks: `pre-commit.sh` is marked executable, contains `--if-present` flags\n7. Hooks: Pre-commit content has all 5 quality gate steps in order",
        "priority": "high",
        "dependencies": [
          "4",
          "6"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-17T21:28:51.186Z"
      },
      {
        "id": "11",
        "title": "Implement Devcontainer Generator",
        "description": "Create `src/generators/devcontainer.ts` — generates `.devcontainer/devcontainer.json` with lifecycle hooks calling `ai-init` phases. This enables automatic setup when a Codespace is created or rebuilt.",
        "details": "Create `src/generators/devcontainer.ts`:\n\n```typescript\nimport { ProjectConfig, FileDescriptor } from '../types.js';\n\ninterface DevcontainerConfig {\n  name: string;\n  image: string;\n  features: Record<string, unknown>;\n  onCreateCommand: string;\n  postCreateCommand: string;\n  postStartCommand: string;\n  customizations: {\n    vscode: {\n      extensions: string[];\n    };\n  };\n  secrets: Record<string, { description: string; documentationUrl: string }>;\n  containerEnv: Record<string, string>;\n  remoteEnv: Record<string, string>;\n}\n\nexport function generateDevcontainer(config: ProjectConfig): FileDescriptor[] {\n  const devcontainer: DevcontainerConfig = {\n    name: config.projectName,\n    image: 'mcr.microsoft.com/devcontainers/universal:2',\n    features: {\n      'ghcr.io/devcontainers/features/node:1': { version: '20' },\n    },\n    onCreateCommand: 'ai-init on-create',\n    postCreateCommand: 'ai-init post-create',\n    postStartCommand: 'ai-init post-start',\n    customizations: {\n      vscode: {\n        extensions: [\n          'GitHub.copilot',\n          'GitHub.copilot-chat',\n        ],\n      },\n    },\n    secrets: {\n      ANTHROPIC_API_KEY: {\n        description: 'Anthropic API key for Claude Code',\n        documentationUrl: 'https://console.anthropic.com/settings/keys',\n      },\n    },\n    containerEnv: { ANTHROPIC_API_KEY: '${localEnv:ANTHROPIC_API_KEY}' },\n    remoteEnv: { ANTHROPIC_API_KEY: '${localEnv:ANTHROPIC_API_KEY}' },\n  };\n\n  return [\n    {\n      path: '.devcontainer/devcontainer.json',\n      content: JSON.stringify(devcontainer, null, 2),\n    },\n  ];\n}\n```\n\nThis is a pure function (sync) since it doesn't read templates — it builds the config object directly. The lifecycle commands use `ai-init` (the symlinked CLI binary) rather than `setup-ai.sh`, enabling the TypeScript tool to replace the bash bootstrap entirely.",
        "testStrategy": "Unit test `test/generators/devcontainer.test.ts`:\n1. Output contains exactly one FileDescriptor with path `.devcontainer/devcontainer.json`\n2. Parsed JSON has `onCreateCommand: 'ai-init on-create'`\n3. Parsed JSON has `postCreateCommand: 'ai-init post-create'`\n4. Parsed JSON has `postStartCommand: 'ai-init post-start'`\n5. JSON is valid (JSON.parse succeeds without throwing)\n6. `config.projectName` appears as the `name` field",
        "priority": "medium",
        "dependencies": [
          "2",
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/generators/devcontainer.ts with core generator function",
            "description": "Implement the pure synchronous `generateDevcontainer` function that builds the `.devcontainer/devcontainer.json` FileDescriptor with lifecycle hooks, VS Code extensions, secrets, and env vars.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/src/generators/devcontainer.ts` following the same module structure as `mcp-json.ts` (pure function, no fs calls). Define the `DevcontainerConfig` interface locally. The function signature must be `export function generateDevcontainer(config: ProjectConfig): FileDescriptor[]`. Hard-code the base config: `image: 'mcr.microsoft.com/devcontainers/universal:2'`, `features: { 'ghcr.io/devcontainers/features/node:1': { version: '20' } }`, lifecycle commands `onCreateCommand: 'ai-init on-create'`, `postCreateCommand: 'ai-init post-create'`, `postStartCommand: 'ai-init post-start'`. Always include `GitHub.copilot` and `GitHub.copilot-chat` in `customizations.vscode.extensions`. Use `config.projectName` for the `name` field. Return exactly one `FileDescriptor` with `path: '.devcontainer/devcontainer.json'` and `content: JSON.stringify(devcontainer, null, 2)`.",
            "status": "done",
            "testStrategy": null,
            "updatedAt": "2026-02-17T21:35:34.659Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add MCP-driven secrets and containerEnv to devcontainer config",
            "description": "Extend `generateDevcontainer` to dynamically append secrets and container environment variables for each selected MCP server that has env vars defined in the registry.",
            "dependencies": [
              1
            ],
            "details": "Import `getSelectedServers` from `../registry.js`. After building the base `DevcontainerConfig`, iterate `getSelectedServers(config.selectedMcps)` and for each server that has a non-empty `env` record, add an entry to `secrets` with `{ description: 'API key for <server.description>', documentationUrl: '' }` and add matching entries to both `containerEnv` and `remoteEnv` using `'${localEnv:VAR_NAME}'` syntax. Always include the base `ANTHROPIC_API_KEY` secret and env entry (already in the static config), so skip it when iterating to avoid duplication. This mirrors the pattern used in `mcp-json.ts` where `getSelectedServers` drives env generation. The function remains synchronous since `getSelectedServers` is a pure registry lookup.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:35:41.544Z"
          },
          {
            "id": 3,
            "title": "Write unit tests in test/generators/devcontainer.test.ts",
            "description": "Create a comprehensive Vitest test file for `generateDevcontainer` covering output shape, lifecycle commands, MCP-driven secrets, and JSON validity.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create `/workspaces/ai-dev-setup/test/generators/devcontainer.test.ts` mirroring the style of `test/generators/mcp-json.test.ts`. Use `defaultConfig('/tmp/test-project')` via a `makeConfig` helper. Required test cases:\n1. Returns exactly one `FileDescriptor` with `path === '.devcontainer/devcontainer.json'`.\n2. Content is valid JSON (use `JSON.parse`).\n3. Parsed JSON has `onCreateCommand: 'ai-init on-create'`.\n4. Parsed JSON has `postCreateCommand: 'ai-init post-create'`.\n5. Parsed JSON has `postStartCommand: 'ai-init post-start'`.\n6. `name` equals `config.projectName`.\n7. `customizations.vscode.extensions` contains `'GitHub.copilot'` and `'GitHub.copilot-chat'`.\n8. When `selectedMcps` includes `'taskmaster'`, `secrets` includes `ANTHROPIC_API_KEY` and `PERPLEXITY_API_KEY` entries.\n9. `containerEnv.ANTHROPIC_API_KEY` equals `'${localEnv:ANTHROPIC_API_KEY}'`.\n10. Empty `selectedMcps` still produces the base `ANTHROPIC_API_KEY` secret.",
            "status": "done",
            "testStrategy": "Run `npm test -- test/generators/devcontainer.test.ts` — all cases must pass with zero failures.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:36:57.988Z"
          },
          {
            "id": 4,
            "title": "Export generateDevcontainer from the generators barrel",
            "description": "Add `generateDevcontainer` to the centralized exports so other modules (orchestrator, phases) can import it from a single entry point.",
            "dependencies": [
              1
            ],
            "details": "Check whether `/workspaces/ai-dev-setup/src/generators/index.ts` exists. If it does, add `export { generateDevcontainer } from './devcontainer.js';` alongside existing exports. If it does not exist yet, create it and export all current generators: `generateMcpJson` from `./mcp-json.js`, `generateClaudeMd` from `./claude-md.js`, `generateDocs` from `./docs.js`, `generateRules` from `./rules.js`, `generateSkills` from `./skills.js`, `generateHooks` from `./hooks.js`, and `generateDevcontainer` from `./devcontainer.js`. Also check `src/index.ts` (the main entry) and add a re-export if generators are surfaced there. Follow existing import patterns using `.js` extensions for ESM compatibility.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:36:08.942Z"
          },
          {
            "id": 5,
            "title": "Run full quality gate: format, lint, typecheck, build, and test",
            "description": "Execute the mandatory pre-completion checklist — format, lint, typecheck, build, and full test suite — fixing any issues found.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Run the following commands in sequence from `/workspaces/ai-dev-setup` and fix any failures before proceeding to the next step:\n1. `npm run format` — prettify all changed files.\n2. `npm run lint` (or `npx eslint . --fix`) — fix all lint errors.\n3. `npm run typecheck` (or `npx tsc --noEmit`) — ensure zero TypeScript errors; pay attention to the `DevcontainerConfig` interface shape and any missing `?` on optional fields.\n4. `npm run build` — verify the project compiles cleanly.\n5. `npm test` — run the full test suite and confirm all tests pass, including the new `devcontainer.test.ts`.\nIf any step fails, fix the root cause and re-run from that step. Log findings to the subtask via `update_subtask`.",
            "status": "done",
            "testStrategy": "All five commands must exit with code 0. The `devcontainer.test.ts` suite must show all tests as passing in the `npm test` output.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:38:16.760Z"
          }
        ],
        "updatedAt": "2026-02-17T21:38:16.760Z"
      },
      {
        "id": "12",
        "title": "Implement Commands Generator",
        "description": "Create `src/generators/commands.ts` — generates the `.claude/commands/` slash command files and `boot-prompt.txt`. Implements F8 (Custom Claude Commands).",
        "details": "Create `src/generators/commands.ts`:\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { ProjectConfig, FileDescriptor } from '../types.js';\nimport { fillTemplate } from '../utils.js';\n\nconst COMMANDS_DIR = new URL('../../templates/commands', import.meta.url).pathname;\nconst TEMPLATES_DIR = new URL('../../templates', import.meta.url).pathname;\n\nexport async function generateCommands(config: ProjectConfig): Promise<FileDescriptor[]> {\n  const vars: Record<string, string> = {\n    PROJECT_NAME: config.projectName,\n    TASK_TRACKER: config.taskTracker,\n    TASK_TRACKER_NEXT: getTrackerNextCommand(config.taskTracker),\n    TASK_TRACKER_DONE: getTrackerDoneCommand(config.taskTracker),\n  };\n\n  const devNext = await fs.readFile(path.join(COMMANDS_DIR, 'dev-next.md'), 'utf8');\n  const review = await fs.readFile(path.join(COMMANDS_DIR, 'review.md'), 'utf8');\n  const bootPrompt = await fs.readFile(path.join(TEMPLATES_DIR, 'boot-prompt.txt'), 'utf8');\n\n  return [\n    { path: '.claude/commands/dev-next.md', content: fillTemplate(devNext, vars) },\n    { path: '.claude/commands/review.md', content: fillTemplate(review, vars) },\n    { path: '.claude/boot-prompt.txt', content: fillTemplate(bootPrompt, vars) },\n  ];\n}\n\nfunction getTrackerNextCommand(tracker: string): string {\n  switch (tracker) {\n    case 'taskmaster': return 'task-master next';\n    case 'beads': return 'bd show';\n    case 'markdown': return 'Read TASKS.md and find the next pending task';\n    default: return 'task-master next';\n  }\n}\n\nfunction getTrackerDoneCommand(tracker: string): string {\n  switch (tracker) {\n    case 'taskmaster': return 'task-master set-status --id=<id> --status=done';\n    case 'beads': return 'bd update <id> --status done && bd sync';\n    case 'markdown': return 'Edit TASKS.md and mark task as [x]';\n    default: return 'task-master set-status --id=<id> --status=done';\n  }\n}\n```\n\nThe `dev-next.md` command references docs/adr/ for architecture decisions — this cross-reference makes the command self-updating as the project adds ADRs.",
        "testStrategy": "Unit test `test/generators/commands.test.ts`:\n1. With `taskTracker: 'taskmaster'` → `dev-next.md` contains `task-master next`\n2. With `taskTracker: 'beads'` → `dev-next.md` contains `bd show`\n3. Output always includes 3 files: `dev-next.md`, `review.md`, `boot-prompt.txt`\n4. `review.md` contains `git diff` reference\n5. `boot-prompt.txt` has tracker-specific content substituted",
        "priority": "medium",
        "dependencies": [
          "4",
          "6"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-17T21:44:20.714Z"
      },
      {
        "id": "13",
        "title": "Implement Lifecycle Phases (on-create, post-create, post-start)",
        "description": "Create `src/phases/on-create.ts`, `src/phases/post-create.ts`, and `src/phases/post-start.ts`. These handle the three Codespace lifecycle events, porting the existing bash phase logic into TypeScript.",
        "details": "**`src/phases/on-create.ts`** — Heavy installs, called once during Codespace creation:\n```typescript\nimport { run, commandExists } from '../utils.js';\n\nexport async function runOnCreate(): Promise<void> {\n  console.log('[ai-init] Phase: on-create — installing global tools...');\n  \n  // Install Claude Code if not present\n  if (!(await commandExists('claude'))) {\n    await run('npm', ['install', '-g', '@anthropic-ai/claude-code']);\n  }\n  \n  // Install Task Master if not present\n  if (!(await commandExists('task-master'))) {\n    await run('npm', ['install', '-g', 'task-master-ai']);\n  }\n  \n  console.log('[ai-init] on-create complete.');\n}\n```\n\n**`src/phases/post-create.ts`** — Project configuration, orchestrates all generators:\n```typescript\nimport { ProjectConfig } from '../types.js';\nimport { writeFiles } from '../utils.js';\nimport { generateMcpJson } from '../generators/mcp-json.js';\nimport { generateClaudeMd } from '../generators/claude-md.js';\nimport { generateDocs } from '../generators/docs.js';\nimport { generateRules } from '../generators/rules.js';\nimport { generateSkills } from '../generators/skills.js';\nimport { generateHooks } from '../generators/hooks.js';\nimport { generateDevcontainer } from '../generators/devcontainer.js';\nimport { generateCommands } from '../generators/commands.js';\n\nexport async function runPostCreate(\n  config: ProjectConfig,\n  overwrite = true\n): Promise<string[]> {\n  const allFiles = [\n    ...generateMcpJson(config),\n    ...generateClaudeMd(config),\n    ...(config.generateDocs ? await generateDocs(config) : []),\n    ...(config.generateRules ? await generateRules(config) : []),\n    ...(config.generateSkills ? await generateSkills(config) : []),\n    ...(config.generateHooks ? await generateHooks(config) : []),\n    ...generateDevcontainer(config),\n    ...(config.generateCommands ? await generateCommands(config) : []),\n  ];\n\n  const written = await writeFiles(allFiles, config.projectRoot, overwrite);\n  return written;\n}\n```\n\n**`src/phases/post-start.ts`** — Per-session setup:\n```typescript\nexport async function runPostStart(config: ProjectConfig): Promise<void> {\n  // 1. Sync .env from secrets (copy ANTHROPIC_API_KEY etc. to .env if not present)\n  // 2. Print welcome banner with task progress\n  // 3. Show pending task count from tasks.json or TASKS.md\n}\n```\n\nThe post-start banner reads `.taskmaster/tasks/tasks.json` if using Task Master, or `TASKS.md` if using simple markdown, and prints a summary of pending tasks.",
        "testStrategy": "Integration test `test/integration/phases.test.ts`:\n1. `runPostCreate` in a temp directory creates expected files for a minimal config\n2. With `overwrite: false`, existing files are not overwritten\n3. `runPostCreate` returns the list of written file paths\n4. All file paths in the return value actually exist on disk after the call\n5. Smoke test: `runOnCreate` doesn't throw when claude/task-master already exist",
        "priority": "high",
        "dependencies": [
          "4",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/phases/on-create.ts with global tool installation logic",
            "description": "Implement the on-create phase module that installs Claude Code and Task Master globally if not already present, using the existing `run` and `commandExists` utilities from `src/utils.ts`.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/src/phases/on-create.ts`. Import `run` and `commandExists` from `../utils.js`. Export an async `runOnCreate(): Promise<void>` function. Log `[ai-init] Phase: on-create — installing global tools...` at the start. Use `commandExists('claude')` to conditionally run `npm install -g @anthropic-ai/claude-code`. Use `commandExists('task-master')` to conditionally run `npm install -g task-master-ai`. Log `[ai-init] on-create complete.` at the end. Do not hardcode package versions — use `@latest` or no version pin so npm resolves the current version.",
            "status": "done",
            "testStrategy": "Unit test in `test/phases/on-create.test.ts` using vitest. Mock `commandExists` and `run` from `../../src/utils.js`. Verify: (1) when both commands exist, `run` is never called; (2) when `claude` is missing, `run` is called with `['install', '-g', '@anthropic-ai/claude-code']`; (3) when `task-master` is missing, `run` is called with `['install', '-g', 'task-master-ai']`; (4) function resolves without throwing when all installs succeed.",
            "updatedAt": "2026-02-17T21:54:35.375Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create src/phases/post-create.ts orchestrating all generators",
            "description": "Implement the post-create phase module that imports and invokes all 8 generators, collects their FileDescriptor arrays, and writes everything to disk via `writeFiles`, returning the list of written paths.",
            "dependencies": [
              1
            ],
            "details": "Create `/workspaces/ai-dev-setup/src/phases/post-create.ts`. Import `ProjectConfig` from `../types.js` and `writeFiles` from `../utils.js`. Import all 8 generators: `generateMcpJson` from `../generators/mcp-json.js`, `generateClaudeMd` from `../generators/claude-md.js`, `generateDocs` from `../generators/docs.js`, `generateRules` from `../generators/rules.js`, `generateSkills` from `../generators/skills.js`, `generateHooks` from `../generators/hooks.js`, `generateDevcontainer` from `../generators/devcontainer.js`, and `generateCommands` from `../generators/commands.js`. Export an async `runPostCreate(config: ProjectConfig, overwrite = true): Promise<string[]>` function. Conditionally await async generators based on config feature flags (e.g., `config.generateDocs`, `config.generateRules`, etc.) while always running `generateMcpJson`, `generateClaudeMd`, and `generateDevcontainer`. Spread all FileDescriptor arrays into `allFiles`, call `writeFiles(allFiles, config.projectRoot, overwrite)`, and return the result.",
            "status": "done",
            "testStrategy": "Integration test in `test/integration/phases.test.ts` using a temp directory (`os.tmpdir()` + random suffix, cleaned in `afterEach`). Verify: (1) `runPostCreate` with a minimal config creates expected files; (2) with `overwrite: false`, existing files are not overwritten; (3) return value contains relative paths of written files; (4) conditional generators (docs, rules, skills, hooks, commands) are skipped when their flags are false; (5) all required generators always run regardless of flags.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:54:44.451Z"
          },
          {
            "id": 3,
            "title": "Create src/phases/post-start.ts with env sync and welcome banner",
            "description": "Implement the post-start phase module that reads API key environment variables and syncs them to `.env` if missing, then reads task progress from `.taskmaster/tasks/tasks.json` or `TASKS.md` and prints a formatted welcome banner.",
            "dependencies": [
              2
            ],
            "details": "Create `/workspaces/ai-dev-setup/src/phases/post-start.ts`. Import `ProjectConfig` from `../types.js`, `readOptional` from `../utils.js`, and `fs` from `node:fs/promises` and `path` from `node:path`. Export an async `runPostStart(config: ProjectConfig): Promise<void>` function. Step 1 — env sync: define a list of well-known API key names (`ANTHROPIC_API_KEY`, `PERPLEXITY_API_KEY`, `OPENAI_API_KEY`, `GOOGLE_API_KEY`). Read `.env` from `config.projectRoot` using `readOptional`. For each key present in `process.env` but absent from the `.env` file content, append `KEY=VALUE` lines. Write the updated `.env` back only if changes were made. Step 2 — task banner: if `config.taskTracker === 'taskmaster'`, read `.taskmaster/tasks/tasks.json` and count tasks by status (`pending`, `in-progress`, `done`); otherwise read `TASKS.md` and count unchecked `- [ ]` lines vs checked `- [x]` lines. Print a banner with `[ai-init] Welcome back!`, project name, and task counts. Gracefully handle missing files (print zero counts or skip).",
            "status": "done",
            "testStrategy": "Unit test in `test/phases/post-start.test.ts` using a temp directory. Verify: (1) when `.env` is missing and env vars are set, a `.env` file is created with those vars; (2) when `.env` already contains the key, it is not duplicated; (3) with a valid `tasks.json` containing mixed-status tasks, the banner prints correct pending/done counts; (4) with a `TASKS.md` containing `- [ ]` and `- [x]` lines, markdown counts are reported; (5) when neither file exists, function completes without throwing.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:54:51.071Z"
          },
          {
            "id": 4,
            "title": "Add phases barrel export and integrate phases into src/cli.ts",
            "description": "Create a `src/phases/index.ts` barrel that re-exports all three phase functions, then wire them into `src/cli.ts` so that `ai-init on-create`, `ai-init post-create`, and `ai-init post-start` subcommands invoke the correct phase.",
            "dependencies": [
              3
            ],
            "details": "Create `/workspaces/ai-dev-setup/src/phases/index.ts` with three named re-exports: `export { runOnCreate } from './on-create.js'`, `export { runPostCreate } from './post-create.js'`, and `export { runPostStart } from './post-start.js'`. Then read the current `src/cli.ts`. Add a `switch` (or `if/else`) on `process.argv[2]` to handle three subcommands: `'on-create'` → `await runOnCreate()`, `'post-create'` → load config (from `.taskmaster/tasks/tasks.json` or a persisted `ai-init.config.json` if it exists, else use `defaultConfig(process.cwd())`) then `await runPostCreate(config)`, `'post-start'` → similarly load config then `await runPostStart(config)`. Print usage and exit with code 1 for unknown subcommands. Ensure the CLI entry is the file listed as `bin` in `package.json`.",
            "status": "done",
            "testStrategy": "Smoke test by running `npx tsx src/cli.ts --help` (or the built binary) and verifying it does not throw. Integration test: spawn `tsx src/cli.ts on-create` in a subprocess with mocked PATH (where `claude` and `task-master` are fake scripts) and assert exit code 0. Also test that an unknown subcommand exits with code 1.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:54:57.308Z"
          },
          {
            "id": 5,
            "title": "Run quality gates and fix all lint, type, build, and test failures",
            "description": "Run the full quality pipeline (format, lint, typecheck, build, test) against the new phase files and integration tests, and fix any issues found before the task can be marked done.",
            "dependencies": [
              4
            ],
            "details": "From `/workspaces/ai-dev-setup`, run the following commands in order and fix all errors before proceeding to the next step: (1) `npm run format` — applies Prettier to all source and test files; (2) `npm run lint` — runs ESLint with auto-fix, resolve any remaining errors; (3) `npm run typecheck` — runs `tsc --noEmit`, fix all TypeScript type errors in the three new phase files, the barrel export, and any updated cli.ts; (4) `npm run build` — compiles TypeScript to `dist/`, resolve any compilation errors; (5) `npm test` — runs the full Vitest suite including the new phase tests and existing generator tests, ensure all 300+ existing tests still pass and new phase tests pass. Log findings using `update_subtask` before marking the task chain done.",
            "status": "done",
            "testStrategy": "All five npm scripts must exit with code 0. Specifically: `npm run format` makes no further changes when re-run; `npm run lint` reports zero errors; `npm run typecheck` reports zero errors; `npm run build` produces `dist/phases/on-create.js`, `dist/phases/post-create.js`, `dist/phases/post-start.js`, and `dist/phases/index.js`; `npm test` reports all tests passing with no skipped phase tests.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T21:55:03.588Z"
          }
        ],
        "updatedAt": "2026-02-17T21:55:03.588Z"
      },
      {
        "id": "14",
        "title": "Implement Interactive Wizard",
        "description": "Create `src/wizard.ts` — the interactive 10-step setup flow using `@inquirer/prompts`. Each step is skippable. The wizard collects a `ProjectConfig` from user input, respecting environment variable overrides for non-interactive mode.",
        "details": "Create `src/wizard.ts`:\n\n```typescript\nimport { select, checkbox, confirm, input } from '@inquirer/prompts';\nimport { ProjectConfig, TaskTracker, Architecture } from './types.js';\nimport { MCP_REGISTRY } from './registry.js';\nimport { defaultConfig } from './defaults.js';\n\nconst NON_INTERACTIVE = process.env.SETUP_AI_NONINTERACTIVE === '1';\n\nfunction fromEnv<T>(key: string, fallback: T): T {\n  const val = process.env[key];\n  return val !== undefined ? (val as unknown as T) : fallback;\n}\n\nexport async function runWizard(projectRoot: string): Promise<ProjectConfig> {\n  const config = defaultConfig(projectRoot);\n\n  console.log('\\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━');\n  console.log('  AI Project Init — Setup Wizard');\n  console.log('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n');\n\n  // Step 0: Claude Code Bootstrap (handled in on-create phase, just verify here)\n  // Step 1: MCP Server Selection\n  if (NON_INTERACTIVE) {\n    const envMcps = process.env.SETUP_AI_MCPS;\n    config.selectedMcps = envMcps ? envMcps.split(',') : ['taskmaster'];\n  } else {\n    const choices = MCP_REGISTRY.map(s => ({\n      name: `${s.name} — ${s.description}`,\n      value: s.name,\n      checked: s.name === 'taskmaster',\n    }));\n    config.selectedMcps = await checkbox({\n      message: 'Step 1: Select MCP servers to configure:',\n      choices,\n    });\n    // Ensure taskmaster is included if selected as tracker\n  }\n\n  // Step 2: Task Tracker\n  if (NON_INTERACTIVE) {\n    config.taskTracker = fromEnv<TaskTracker>('SETUP_AI_TRACKER', 'taskmaster');\n  } else {\n    config.taskTracker = await select({\n      message: 'Step 2: Choose a task tracker:',\n      choices: [\n        { name: 'Task Master (recommended — subtasks, research, complexity analysis)', value: 'taskmaster' },\n        { name: 'Beads (multi-agent, git-native issue tracking)', value: 'beads' },\n        { name: 'Simple Markdown (for small projects, ≤20 tasks)', value: 'markdown' },\n      ],\n      default: 'taskmaster',\n    }) as TaskTracker;\n  }\n\n  // Step 3: PRD\n  // Step 4: Architecture\n  if (NON_INTERACTIVE) {\n    config.architecture = fromEnv<Architecture>('SETUP_AI_ARCH', 'skip');\n  } else {\n    config.architecture = await select({\n      message: 'Step 4: Project architecture (populates docs/architecture.md):',\n      choices: [\n        { name: 'Skip', value: 'skip' },\n        { name: 'Monolith', value: 'monolith' },\n        { name: '2-tier (frontend + backend)', value: '2-tier' },\n        { name: '3-tier (frontend + API + database)', value: '3-tier' },\n        { name: 'Microservices', value: 'microservices' },\n      ],\n    }) as Architecture;\n  }\n\n  // Steps 5-6: API and Doc generation\n  if (!NON_INTERACTIVE) {\n    config.hasApiDocs = await confirm({ message: 'Step 5: Generate API docs template?', default: config.architecture !== 'skip' && config.architecture !== 'monolith' });\n    config.hasDatabase = await confirm({ message: 'Does this project use a database?', default: config.architecture === '3-tier' });\n  }\n\n  // Step 7: Generate docs/rules/skills/hooks confirmation\n  // Step 8: Agent Teams\n  if (NON_INTERACTIVE) {\n    config.agentTeamsEnabled = process.env.SETUP_AI_AGENT_TEAMS === '1';\n  } else {\n    config.agentTeamsEnabled = await confirm({\n      message: 'Step 8 (Optional): Enable Claude Code experimental agent teams mode?',\n      default: false,\n    });\n  }\n\n  // Step 9: Audit\n  if (NON_INTERACTIVE) {\n    config.runAudit = process.env.SETUP_AI_SKIP_AUDIT !== '1';\n  } else {\n    config.runAudit = await confirm({\n      message: 'Step 9: Run AI-powered audit of generated files? (uses API credits)',\n      default: true,\n    });\n  }\n\n  return config;\n}\n```\n\n**Non-interactive env vars supported:**\n- `SETUP_AI_NONINTERACTIVE=1` — skip all prompts\n- `SETUP_AI_MCPS=taskmaster,context7` — pre-select MCPs\n- `SETUP_AI_TRACKER=taskmaster|beads|markdown`\n- `SETUP_AI_ARCH=monolith|2-tier|3-tier|microservices|skip`\n- `SETUP_AI_AGENT_TEAMS=1`\n- `SETUP_AI_SKIP_AUDIT=1`",
        "testStrategy": "Integration test `test/integration/wizard.test.ts` using `@inquirer/testing` to automate prompt responses:\n1. Wizard returns valid `ProjectConfig` with all required fields\n2. Non-interactive mode (`SETUP_AI_NONINTERACTIVE=1`) returns defaults without prompting\n3. `SETUP_AI_MCPS=taskmaster,context7` → `selectedMcps` contains both\n4. `SETUP_AI_TRACKER=beads` → `taskTracker === 'beads'`\n5. `SETUP_AI_SKIP_AUDIT=1` → `runAudit === false`",
        "priority": "high",
        "dependencies": [
          "3",
          "13"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/wizard.ts with all 10 interactive steps",
            "description": "Implement the full wizard module with all 10 setup steps using @inquirer/prompts, supporting both interactive and non-interactive (env var) modes.",
            "dependencies": [],
            "details": "Create /workspaces/ai-dev-setup/src/wizard.ts exactly as specified in the task details. The file must:\n1. Import select, checkbox, confirm, input from '@inquirer/prompts'\n2. Import ProjectConfig, TaskTracker, Architecture from './types.js'\n3. Import MCP_REGISTRY from './registry.js'\n4. Import defaultConfig from './defaults.js'\n5. Define NON_INTERACTIVE constant from env var SETUP_AI_NONINTERACTIVE\n6. Implement fromEnv<T> helper for env var overrides\n7. Export async runWizard(projectRoot: string): Promise<ProjectConfig>\n8. Step 1: MCP server selection — multi-select checkbox from MCP_REGISTRY entries, default taskmaster checked. Env override: SETUP_AI_MCPS (comma-separated)\n9. Step 2: Task tracker selection — select from taskmaster/beads/markdown. Env override: SETUP_AI_TRACKER\n10. Step 3: PRD path — input prompt asking for optional path to existing PRD file, sets config.hasPrd and config.prdPath. Env override: SETUP_AI_PRD_PATH\n11. Step 4: Architecture picker — select from skip/monolith/2-tier/3-tier/microservices. Env override: SETUP_AI_ARCH\n12. Step 5: API docs toggle — confirm prompt, default true when architecture is 2-tier/3-tier/microservices. Env override: SETUP_AI_API_DOCS\n13. Step 6: Database toggle — confirm prompt, default true when architecture is 3-tier. Env override: SETUP_AI_DB\n14. Step 7: Docs/rules/skills/hooks/commands generation flags — confirm prompts for each. Env overrides: SETUP_AI_GEN_DOCS, etc.\n15. Step 8: Agent teams opt-in — confirm, default false. Env override: SETUP_AI_AGENT_TEAMS=1\n16. Step 9: Audit toggle — confirm, default true. Env override: SETUP_AI_SKIP_AUDIT=1\n17. Step 10: Display summary of collected config (projectName, tracker, architecture, selected MCPs, feature flags) and any next-step instructions\n18. Return the populated config object",
            "status": "done",
            "testStrategy": "Manual smoke test: run `npx tsx src/cli.ts` and verify wizard prompts appear and return valid ProjectConfig.",
            "updatedAt": "2026-02-17T22:04:27.435Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate wizard into src/cli.ts as default command",
            "description": "Update cli.ts to invoke runWizard() when no lifecycle subcommand is provided, then call post-create phase with the resulting config.",
            "dependencies": [
              1
            ],
            "details": "Read /workspaces/ai-dev-setup/src/cli.ts and update the default case (currently prints version). Changes:\n1. Import runWizard from './wizard.js'\n2. Import the post-create phase runner (or relevant orchestration function) from './phases/post-create.js'\n3. In the default command handler (the else/fallthrough branch after lifecycle commands), call: const config = await runWizard(process.cwd()); then invoke the appropriate orchestration to generate and write files using that config\n4. Ensure the wizard path is async (wrap in async IIFE or mark the handler async if needed)\n5. Add a --interactive / -i flag alias if a non-interactive path also exists, for clarity\n6. Preserve existing on-create, post-create, post-start subcommand handling unchanged",
            "status": "done",
            "testStrategy": "Run `npx tsx src/cli.ts --help` to verify help output, run `npx tsx src/cli.ts on-create` to verify lifecycle commands still work.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:05:06.988Z"
          },
          {
            "id": 3,
            "title": "Write unit tests for wizard non-interactive mode",
            "description": "Create test/wizard.test.ts with tests covering non-interactive env var mode to ensure all config fields are correctly set without any prompts.",
            "dependencies": [
              1
            ],
            "details": "Create /workspaces/ai-dev-setup/test/wizard.test.ts:\n1. Import runWizard from '../src/wizard.js'\n2. Import defaultConfig from '../src/defaults.js'\n3. Use a makeConfig helper consistent with other test files\n4. Test: SETUP_AI_NONINTERACTIVE=1 with no other vars returns config with defaults (selectedMcps=['taskmaster'], taskTracker='taskmaster', architecture='skip', agentTeamsEnabled=false, runAudit=true)\n5. Test: SETUP_AI_MCPS=taskmaster,context7 sets selectedMcps to ['taskmaster','context7']\n6. Test: SETUP_AI_TRACKER=beads sets taskTracker to 'beads'\n7. Test: SETUP_AI_ARCH=3-tier sets architecture to '3-tier'\n8. Test: SETUP_AI_AGENT_TEAMS=1 sets agentTeamsEnabled to true\n9. Test: SETUP_AI_SKIP_AUDIT=1 sets runAudit to false\n10. Each test must set process.env vars before calling runWizard and restore/delete them in afterEach\n11. Use vitest describe/it/expect/beforeEach/afterEach patterns consistent with existing tests\n12. Verify returned object satisfies ProjectConfig shape (all required fields present)",
            "status": "done",
            "testStrategy": "Run `npm test` and verify all new wizard tests pass with no errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:06:00.315Z"
          },
          {
            "id": 4,
            "title": "Run formatter, linter, and type-checker",
            "description": "Execute npm run format, npm run lint, and npx tsc --noEmit to ensure wizard.ts and cli.ts changes meet project code quality standards.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Execute the project's quality gate commands in order:\n1. Run `npm run format` (prettier --write) — auto-fixes formatting issues in all files including src/wizard.ts, src/cli.ts, and test/wizard.test.ts\n2. Run `npm run lint` (eslint --fix) — auto-fixes lint errors; review any remaining warnings\n3. Run `npx tsc --noEmit` — verify TypeScript compilation passes with zero errors; fix any type errors in wizard.ts (especially the fromEnv<T> helper and type assertions for TaskTracker/Architecture)\n4. Common type issues to watch: ensuring select() return values are cast to TaskTracker/Architecture, ensuring checkbox returns string[], ensuring confirm returns boolean\n5. Fix any issues found and re-run the respective command until it passes",
            "status": "done",
            "testStrategy": "All three commands exit with code 0 and produce no errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:07:02.666Z"
          },
          {
            "id": 5,
            "title": "Run build and full test suite",
            "description": "Execute npm run build and npm test to verify the wizard implementation compiles successfully and all tests pass.",
            "dependencies": [
              4
            ],
            "details": "Execute the build and test commands:\n1. Run `npm run build` — compiles TypeScript to dist/; verify exit code 0 and dist/wizard.js is emitted\n2. Run `npm test` (vitest run) — runs all tests in test/**/*.test.ts; verify all pass including the new test/wizard.test.ts\n3. If build fails: check for missing imports, incorrect module paths (must use .js extensions for ESM), or unresolved types\n4. If tests fail: review error output, fix logic in wizard.ts or test assertions, re-run\n5. Pay attention to: ESM import paths must use .js extension (e.g., './types.js' not './types'), async/await correctness, env var cleanup between tests",
            "status": "done",
            "testStrategy": "npm run build exits 0, npm test exits 0 with all test suites passing.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:07:09.393Z"
          },
          {
            "id": 6,
            "title": "Log implementation findings and mark task complete",
            "description": "Use update_subtask to record implementation notes, confirm all quality gates passed, then set task 14 status to done.",
            "dependencies": [
              5
            ],
            "details": "Perform final task completion steps:\n1. Use the taskmaster-ai MCP tool `update_subtask` (or task-master CLI) to log implementation notes on task 14, including: what was implemented, any design decisions made (e.g. how fromEnv<T> handles type coercion), any issues encountered and fixed during format/lint/typecheck/build/test, and confirmation all checks passed\n2. Verify the summary/next-steps display (Step 10 of wizard) works correctly by reviewing the console.log output format\n3. Confirm test/wizard.test.ts covers all env var overrides defined in the task spec\n4. Call `set_task_status --id=14 --status=done` via MCP or CLI to mark the task complete\n5. If any quality gate failed and could not be fixed, set status to 'blocked' with explanation instead",
            "status": "done",
            "testStrategy": "Task 14 status transitions to 'done' in .taskmaster/tasks/tasks.json after all prior steps succeed.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:07:53.245Z"
          }
        ],
        "updatedAt": "2026-02-17T22:07:53.245Z"
      },
      {
        "id": "15",
        "title": "Implement Claude Code Bootstrap and Audit Runner",
        "description": "Create `src/audit.ts` — the Claude Code headless audit runner implementing F11. Checks for Claude Code installation, runs a structured audit of generated files, and saves results to `.ai-init-audit.md`.",
        "details": "Create `src/audit.ts`:\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport { commandExists, run } from './utils.js';\nimport { ProjectConfig } from './types.js';\n\nconst AUDIT_PROMPT_TEMPLATE = `You are auditing the output of the ai-init project bootstrap tool.\nReview ONLY the files listed below — these were just generated by the setup wizard.\nDo NOT review or comment on any other files in the project.\n\nGenerated files:\n{{GENERATED_FILES}}\n\nAudit checklist:\n1. STRUCTURE: Are all generated docs following the format in docs/doc_format.md?\n   Check: TOC present, sections <30 lines, tables for structured data, TLDR at top.\n\n2. CROSS-REFERENCES: Does CLAUDE.md accurately reference all generated docs?\n   Check: Every @import points to a file that exists. No broken references.\n\n3. RULES CONSISTENCY: Do .claude/rules/ files reference correct path patterns?\n   Check: Rules that @import docs reference docs that were actually generated.\n\n4. MCP CONFIG: Is .mcp.json valid JSON with correct package names?\n   Check: All selected MCP servers present. No duplicates.\n\n5. TEMPLATE COMPLETENESS: Which sections still have placeholder content the user MUST fill?\n   Flag each file and specific section.\n\n6. GAPS: What is missing that the user should address manually?\n\n7. AGENT INSTRUCTIONS: Are CLAUDE.md and rules well-structured and actionable?\n   Flag any vague or generic instructions.\n\nOutput format:\n- ✅ PASS: <area> — <one-line summary>\n- ⚠️  FILL: <file>:<section> — <what the user needs to add>\n- ❌ FIX: <file>:<issue> — <what's wrong and how to fix it>\n\nEnd with a numbered \"Post-Setup Checklist\" of manual actions before starting development.`;\n\nexport async function checkClaudeCodeAvailable(): Promise<boolean> {\n  if (!(await commandExists('claude'))) {\n    return false;\n  }\n  try {\n    await run('claude', ['--version']);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\nexport async function installClaudeCode(): Promise<void> {\n  console.log('[ai-init] Installing Claude Code...');\n  await run('npm', ['install', '-g', '@anthropic-ai/claude-code']);\n}\n\nexport async function runAudit(\n  config: ProjectConfig,\n  generatedFiles: string[]\n): Promise<void> {\n  if (!(await checkClaudeCodeAvailable())) {\n    console.warn('[ai-init] Skipping audit — Claude Code not available');\n    return;\n  }\n\n  const filesList = generatedFiles.map(f => `  - ${f}`).join('\\n');\n  const prompt = AUDIT_PROMPT_TEMPLATE.replace('{{GENERATED_FILES}}', filesList);\n\n  console.log('[ai-init] Running AI-powered audit of generated files...');\n\n  let auditOutput: string;\n  try {\n    auditOutput = await run('claude', ['--headless', '--print', prompt], config.projectRoot);\n  } catch (err) {\n    console.warn('[ai-init] Audit failed — review generated files manually');\n    console.warn(err);\n    return;\n  }\n\n  // Save audit results\n  const auditPath = path.join(config.projectRoot, '.ai-init-audit.md');\n  await fs.writeFile(auditPath, `# AI Init Audit Results\\n\\n${auditOutput}\\n`, 'utf8');\n  \n  console.log('\\n--- AUDIT RESULTS ---');\n  console.log(auditOutput);\n  console.log('\\nAudit saved to: .ai-init-audit.md');\n}\n```\n\nAdd `.ai-init-audit.md` to `.gitignore`.\n\n**Graceful degradation:**\n- Claude not installed → skip with message\n- No API credentials → skip with message\n- Audit errors → catch, warn, continue\n- Never block wizard completion",
        "testStrategy": "Unit test `test/generators/audit.test.ts`:\n1. `checkClaudeCodeAvailable()` returns false when 'claude' not on PATH (mock commandExists)\n2. `runAudit()` with unavailable Claude → prints warning, does NOT throw\n3. `runAudit()` with audit error → catches error, does NOT propagate\n4. Audit prompt template contains all 7 audit checklist items\n5. `{{GENERATED_FILES}}` placeholder is replaced with actual file list",
        "priority": "medium",
        "dependencies": [
          "4",
          "13"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/audit.ts with checkClaudeCodeAvailable and installClaudeCode functions",
            "description": "Implement the core module `src/audit.ts` with the `checkClaudeCodeAvailable()` and `installClaudeCode()` exported functions using the existing `commandExists` and `run` utilities from `src/utils.ts`.",
            "dependencies": [],
            "details": "Create `src/audit.ts` importing `fs` from `node:fs/promises`, `path` from `node:path`, `commandExists` and `run` from `./utils.js`, and `ProjectConfig` from `./types.js`. Implement `checkClaudeCodeAvailable(): Promise<boolean>` — calls `commandExists('claude')` first; if false returns false; otherwise calls `run('claude', ['--version'])` and returns true, catching any error to return false. Implement `installClaudeCode(): Promise<void>` — logs `[ai-init] Installing Claude Code...` then calls `run('npm', ['install', '-g', '@anthropic-ai/claude-code'])`. Define the `AUDIT_PROMPT_TEMPLATE` constant with the full audit prompt containing the `{{GENERATED_FILES}}` placeholder as specified in the task details. All three must be exported. Follow the same import pattern as other source files in the project (e.g., `src/generators/commands.ts`).",
            "status": "done",
            "testStrategy": "Unit test in `test/generators/audit.test.ts`: mock `commandExists` to return false and verify `checkClaudeCodeAvailable()` returns false without calling `run`; mock `commandExists` to return true and mock `run` to succeed and verify returns true; mock `run` to throw and verify returns false.",
            "updatedAt": "2026-02-17T22:14:11.274Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement runAudit function in src/audit.ts with graceful degradation",
            "description": "Add the `runAudit(config: ProjectConfig, generatedFiles: string[]): Promise<void>` exported function to `src/audit.ts`, implementing all graceful-degradation paths: Claude not available, run error, and the happy path that saves the audit output to `.ai-init-audit.md`.",
            "dependencies": [
              1
            ],
            "details": "Inside `runAudit`: (1) Call `checkClaudeCodeAvailable()`; if false, log `[ai-init] Skipping audit — Claude Code not available` and return. (2) Build `filesList` by mapping `generatedFiles` to `  - ${f}` lines joined by `\\n`, substitute into `AUDIT_PROMPT_TEMPLATE` via `.replace('{{GENERATED_FILES}}', filesList)`. (3) Log `[ai-init] Running AI-powered audit of generated files...`. (4) Call `run('claude', ['--headless', '--print', prompt], config.projectRoot)` in a try/catch — on error, log `[ai-init] Audit failed — review generated files manually`, log the error, and return without throwing. (5) On success, compute `auditPath = path.join(config.projectRoot, '.ai-init-audit.md')`, write `# AI Init Audit Results\\n\\n${auditOutput}\\n` with `fs.writeFile`. (6) Log the audit output to stdout and log `Audit saved to: .ai-init-audit.md`. Follow the error handling pattern from `src/phases/post-create.ts`.",
            "status": "done",
            "testStrategy": "Unit tests: (1) `runAudit` with unavailable Claude prints warning and returns without throwing; (2) `runAudit` with `run` throwing catches the error and does NOT propagate; (3) `runAudit` happy path writes `.ai-init-audit.md` with correct header and content; (4) Audit prompt template has `{{GENERATED_FILES}}` replaced with the file list.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:14:16.891Z"
          },
          {
            "id": 3,
            "title": "Wire runAudit into post-create phase in src/phases/post-create.ts",
            "description": "Update `src/phases/post-create.ts` to import `runAudit` from `../audit.js` and call it after `writeFiles()` when `config.runAudit` is true, passing `config` and the written file list.",
            "dependencies": [
              1,
              2
            ],
            "details": "In `src/phases/post-create.ts`: add `import { runAudit } from '../audit.js';` alongside existing imports. After the existing `config.generatedFiles.push(...written)` line and the `console.log` for post-create complete, add a conditional block: `if (config.runAudit) { await runAudit(config, written); }`. This ensures the audit runs on the actual list of files written to disk (not the full `allFiles` array), and only when `config.runAudit` is true (the flag already exists in `ProjectConfig` and defaults to `true` in `src/defaults.ts`). The audit must never block wizard completion — its graceful degradation is already handled inside `runAudit`.",
            "status": "done",
            "testStrategy": "Integration test in `test/integration/phases.test.ts`: add a test case with `runAudit: false` to verify no `.ai-init-audit.md` is created and post-create still completes normally. Existing tests should continue to pass since `checkClaudeCodeAvailable()` will return false in the test environment (no `claude` binary on PATH).",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:15:04.521Z"
          },
          {
            "id": 4,
            "title": "Write unit tests for audit.ts in test/generators/audit.test.ts",
            "description": "Create `test/generators/audit.test.ts` with comprehensive unit tests for all exported functions in `src/audit.ts`, using vitest's `vi.mock` to mock `commandExists` and `run` from `src/utils.ts`.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create `test/generators/audit.test.ts` following the pattern of `test/generators/commands.test.ts`. Import `describe`, `it`, `expect`, `vi`, `beforeEach`, `afterEach` from vitest. Use `vi.mock('../../src/utils.js', ...)` to mock `commandExists` and `run`. Tests to include: (1) `checkClaudeCodeAvailable()` returns false when `commandExists('claude')` returns false; (2) `checkClaudeCodeAvailable()` returns true when `commandExists` returns true and `run` succeeds; (3) `checkClaudeCodeAvailable()` returns false when `run` throws; (4) `runAudit()` with `commandExists` returning false logs warning and resolves without throwing; (5) `runAudit()` with `run` throwing catches error, logs warning, resolves without throwing, and does NOT write `.ai-init-audit.md`; (6) `runAudit()` happy path with `run` returning audit text — verify `fs.writeFile` is called with the correct path and content beginning with `# AI Init Audit Results`. Use `vi.spyOn(fs, 'writeFile')` or mock `node:fs/promises`. Also verify AUDIT_PROMPT_TEMPLATE substitution by checking the prompt passed to `run` contains the file list.",
            "status": "done",
            "testStrategy": "Run `npm test test/generators/audit.test.ts` — all test cases must pass. Run the full suite `npm test` to ensure no regressions in existing tests.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:15:58.629Z"
          },
          {
            "id": 5,
            "title": "Run quality gate checks and fix any issues",
            "description": "Execute the mandatory pre-completion checklist: format, lint, type-check, build, and full test suite. Fix any failures discovered.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Run the following in order from the project root `/workspaces/ai-dev-setup`: (1) `npm run format` — fix any formatting issues in the new files; (2) `npm run lint` — fix all lint errors in `src/audit.ts` and `test/generators/audit.test.ts`; (3) `npm run typecheck` or `npx tsc --noEmit` — ensure `src/audit.ts` and its integration in `post-create.ts` have no TypeScript errors (verify `runAudit` signature matches `ProjectConfig` from `./types.js` and `run`/`commandExists` imports from `./utils.js`); (4) `npm run build` — verify the project compiles cleanly to `dist/`; (5) `npm test` — run the full test suite including the new `test/generators/audit.test.ts` and the integration test in `test/integration/phases.test.ts`. Confirm `.ai-init-audit.md` is already present in `.gitignore` (it is, at line 31). Fix any issues found and re-run from the failing step.",
            "status": "done",
            "testStrategy": "All five quality gate steps must exit with code 0. The new audit test file must contain at least 6 passing test cases. The integration phase tests must still pass with the new `runAudit` wiring (graceful degradation handles the no-claude-binary case in CI).",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:16:51.618Z"
          }
        ],
        "updatedAt": "2026-02-17T22:16:51.618Z"
      },
      {
        "id": "16",
        "title": "Implement CLI Entry Point with Argument Parsing",
        "description": "Create `src/cli.ts` — the main entry point that wires together the wizard, phases, and audit. Uses `meow` for argument parsing. Handles all subcommands: `ai-init`, `ai-init on-create`, `ai-init post-create`, `ai-init post-start`, `ai-init --non-interactive`.",
        "details": "Create `src/cli.ts`:\n\n```typescript\n#!/usr/bin/env node\nimport meow from 'meow';\nimport path from 'node:path';\nimport { runWizard } from './wizard.js';\nimport { runOnCreate } from './phases/on-create.js';\nimport { runPostCreate } from './phases/post-create.js';\nimport { runPostStart } from './phases/post-start.js';\nimport { runAudit, checkClaudeCodeAvailable, installClaudeCode } from './audit.js';\n\nconst cli = meow(`\n  Usage\n    $ ai-init [command] [options]\n\n  Commands\n    (none)          Interactive setup wizard\n    on-create       Heavy installs — run once during Codespace creation\n    post-create     Project scaffolding — run after Codespace creation\n    post-start      Per-session setup — run on every container start\n\n  Options\n    --non-interactive   Skip prompts, use environment variables\n    --no-audit          Skip the Claude Code audit step\n    --overwrite         Overwrite existing files (default: true)\n    --version           Show version\n    --help              Show help\n\n  Environment Variables\n    SETUP_AI_NONINTERACTIVE=1   Same as --non-interactive\n    SETUP_AI_MCPS               Comma-separated MCP names\n    SETUP_AI_TRACKER            taskmaster | beads | markdown\n    SETUP_AI_ARCH               monolith | 2-tier | 3-tier | microservices | skip\n    SETUP_AI_SKIP_AUDIT=1       Skip audit step\n    SETUP_AI_AGENT_TEAMS=1      Enable agent teams\n`, {\n  importMeta: import.meta,\n  flags: {\n    nonInteractive: { type: 'boolean', default: false },\n    audit: { type: 'boolean', default: true },\n    overwrite: { type: 'boolean', default: true },\n  },\n});\n\nasync function main() {\n  const [command] = cli.input;\n  const projectRoot = process.cwd();\n\n  switch (command) {\n    case 'on-create':\n      await runOnCreate();\n      break;\n\n    case 'post-create': {\n      // In lifecycle mode, use env vars for config\n      process.env.SETUP_AI_NONINTERACTIVE = '1';\n      const config = await runWizard(projectRoot);\n      const written = await runPostCreate(config, cli.flags.overwrite);\n      console.log(`[ai-init] Generated ${written.length} files.`);\n      break;\n    }\n\n    case 'post-start':\n      // Import config from .setup-ai-mcps or use defaults\n      // Run per-session setup\n      break;\n\n    default: {\n      // Interactive wizard (default command)\n      // Step 0: Ensure Claude Code is installed\n      if (!(await checkClaudeCodeAvailable())) {\n        await installClaudeCode().catch(() => {\n          console.warn('[ai-init] Could not install Claude Code — audit will be skipped');\n        });\n      }\n\n      const config = await runWizard(projectRoot);\n      const written = await runPostCreate(config, cli.flags.overwrite);\n      config.generatedFiles = written;\n\n      console.log(`\\n[ai-init] Generated ${written.length} files:`);\n      written.forEach(f => console.log(`  ✓ ${f}`));\n\n      if (config.runAudit && cli.flags.audit) {\n        await runAudit(config, written);\n      }\n\n      console.log('\\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━');\n      console.log('  Setup complete! Next steps:');\n      console.log(`  1. Fill in docs/prd.md with your project requirements`);\n      console.log(`  2. Run 'task-master parse-prd docs/prd.md' to generate tasks`);\n      console.log(`  3. Open Claude Code and run /dev-next to start building`);\n      console.log('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n');\n    }\n  }\n}\n\nmain().catch(err => {\n  console.error('[ai-init] Fatal error:', err.message);\n  process.exit(1);\n});\n```\n\nAdd shebang `#!/usr/bin/env node` at the top. The `bin` field in package.json points to `dist/cli.js`.",
        "testStrategy": "Integration test `test/integration/cli.test.ts` — run the compiled CLI in a temp directory:\n1. `ai-init --non-interactive` in a temp dir creates expected files without prompting\n2. `ai-init on-create` doesn't throw (skips installs if tools already present)\n3. `ai-init --help` prints usage info\n4. `ai-init --version` prints version from package.json\n5. With `SETUP_AI_NONINTERACTIVE=1 SETUP_AI_TRACKER=markdown`, generated CLAUDE.md contains TASKS.md reference",
        "priority": "high",
        "dependencies": [
          "13",
          "14",
          "15"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Install meow dependency and verify package.json bin field",
            "description": "Confirm meow is installed at latest version and the bin field in package.json correctly points to dist/cli.js. Verify meow ESM import works with the project's tsconfig (ESNext modules, NodeNext resolution).",
            "dependencies": [],
            "details": "Run `npm view meow version` to confirm current version. Check package.json `bin` field is `{ \"ai-init\": \"./dist/cli.js\" }`. Verify meow is listed in `dependencies` (not devDependencies) since it's needed at runtime. Check tsconfig.json `moduleResolution` supports named ESM imports from meow. If meow is missing, run `npm install meow@latest`. Verify the import `import meow from 'meow'` compiles without TypeScript errors by running `npx tsc --noEmit`.",
            "status": "pending",
            "testStrategy": "Run `npx tsc --noEmit` — zero errors confirms meow types resolve correctly. Run `npm ls meow` to confirm it appears in the dependency tree.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Refactor src/cli.ts to use meow for argument parsing",
            "description": "Replace the manual `process.argv.slice(2)` parsing in src/cli.ts with a meow-based CLI definition. Define all flags (`--non-interactive`, `--audit`, `--overwrite`) with correct types and defaults. Wire the meow `cli.input[0]` value into the existing switch-case command dispatch.",
            "dependencies": [
              1
            ],
            "details": "Rewrite src/cli.ts to match the target implementation:\n\n1. Add `import meow from 'meow'` at the top (after the shebang).\n2. Define the meow instance with the full help text string covering all commands and environment variables.\n3. Set `importMeta: import.meta` for ESM compatibility.\n4. Define flags: `nonInteractive: { type: 'boolean', default: false }`, `audit: { type: 'boolean', default: true }`, `overwrite: { type: 'boolean', default: true }`.\n5. Replace `args[0]` with `cli.input[0]` and `args.includes('--non-interactive')` with `cli.flags.nonInteractive`.\n6. Remove the manual `--version`, `--help`, and `--non-interactive` branches — meow handles `--version` and `--help` automatically; `--non-interactive` becomes a flag.\n7. In the `post-create` case, pass `cli.flags.overwrite` to `runPostCreate`.\n8. In the default case, conditionally call `runAudit` only if `config.runAudit && cli.flags.audit`.\n9. Add `checkClaudeCodeAvailable` and `installClaudeCode` imports from `./audit.js`.\n10. Keep the `#!/usr/bin/env node` shebang as the very first line.",
            "status": "pending",
            "testStrategy": "Run `npm run build` to compile. Run `node dist/cli.js --help` to confirm meow-formatted help is printed. Run `node dist/cli.js --version` to confirm version is printed. Run `node dist/cli.js --unknown-flag` to confirm meow error handling works.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add file list output and next-steps banner to default command",
            "description": "In the default (interactive wizard) command branch, print the list of generated files with checkmarks and display a formatted next-steps banner after setup completes. Also wire the `--overwrite` flag into the `runPostCreate` call.",
            "dependencies": [
              2
            ],
            "details": "In the `default` switch case of `main()`:\n\n1. After `runPostCreate(config, cli.flags.overwrite)`, capture the returned `written` array.\n2. Set `config.generatedFiles = written`.\n3. Print `\\n[ai-init] Generated ${written.length} files:` followed by each file path prefixed with `  ✓ `.\n4. Add the separator banner:\n```\nconsole.log('\\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━');\nconsole.log('  Setup complete! Next steps:');\nconsole.log('  1. Fill in docs/prd.md with your project requirements');\nconsole.log('  2. Run \\'task-master parse-prd docs/prd.md\\' to generate tasks');\nconsole.log('  3. Open Claude Code and run /dev-next to start building');\nconsole.log('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n');\n```\n5. Also update the `post-create` case to capture and log the written file count: `console.log('[ai-init] Generated ${written.length} files.')`.\n6. Verify `runPostCreate` signature accepts an `overwrite: boolean` second parameter and returns `Promise<string[]>`.",
            "status": "pending",
            "testStrategy": "Run integration test in a temp directory with `SETUP_AI_NONINTERACTIVE=1` — stdout should contain `Generated N files:` and individual file paths with `✓` prefix, followed by the banner. Check `config.generatedFiles` is populated.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add Claude Code availability check to default command flow",
            "description": "In the default (interactive wizard) command branch, call `checkClaudeCodeAvailable()` before running the wizard. If not available, attempt installation via `installClaudeCode()` with graceful error handling.",
            "dependencies": [
              2
            ],
            "details": "In the `default` switch case, before calling `runWizard(projectRoot)`:\n\n1. Add:\n```typescript\nif (!(await checkClaudeCodeAvailable())) {\n  await installClaudeCode().catch(() => {\n    console.warn('[ai-init] Could not install Claude Code — audit will be skipped');\n  });\n}\n```\n2. Ensure `checkClaudeCodeAvailable` and `installClaudeCode` are imported from `'./audit.js'`. Check the current audit.ts exports to confirm these function names exist — if named differently, use the actual export names.\n3. This step must not block the wizard from running even if installation fails (the `.catch()` handles the error gracefully).\n4. The `--non-interactive` flow via `cli.flags.nonInteractive` should also set `process.env.SETUP_AI_NONINTERACTIVE = '1'` before calling `runWizard` — wire this in the default case based on the flag value.",
            "status": "pending",
            "testStrategy": "Mock `checkClaudeCodeAvailable` to return false and `installClaudeCode` to reject — the wizard should still run and the warning should be printed. Mock both to succeed — wizard runs without any warning. Verify audit.ts exports these function names.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write integration tests for the CLI entry point",
            "description": "Create `test/integration/cli.test.ts` with integration tests that compile and invoke the CLI in a temporary directory. Cover: non-interactive mode creates expected files, `on-create` subcommand runs without error, `--help` prints usage info, and `--version` prints the version string.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Create `test/integration/cli.test.ts`:\n\n1. Use `vitest` with `beforeEach`/`afterEach` hooks that create and remove a temp directory (`os.tmpdir() + '/' + crypto.randomUUID()`).\n2. Use Node's `child_process.execFile` (promisified) to invoke `node dist/cli.js` in the temp directory.\n3. Test cases:\n   - `--help`: stdout contains `Usage` and `Commands`, process exits 0.\n   - `--version`: stdout contains `0.` (version string), process exits 0.\n   - `SETUP_AI_NONINTERACTIVE=1 node dist/cli.js`: runs without hanging, exits 0, temp dir contains expected files like `CLAUDE.md` or `.mcp.json`.\n   - `node dist/cli.js on-create`: exits 0 (tools may already be installed — should not throw).\n4. Each test should set a 30-second timeout to accommodate the non-interactive run.\n5. Run `npm run build` before tests to ensure `dist/cli.js` is up to date — add a `beforeAll` hook or note in test that build must be run first.\n6. Run the full quality gate: `npm run format`, `npm run lint`, `npm run typecheck`, `npm run build`, `npm test`.",
            "status": "pending",
            "testStrategy": "All four test cases pass. `npm run typecheck` reports zero errors. `npm run build` compiles without errors. `npm test` exits 0 with all tests green.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-02-17T22:25:28.515Z"
      },
      {
        "id": "17",
        "title": "Create install.sh Bootstrap Script",
        "description": "Create the `install.sh` single-line install script that ensures Node.js ≥ 20 is available (via fnm if needed), clones/pulls the repo, runs `npm ci`, and symlinks `ai-init` to `~/.local/bin/`. This implements F1's single-line install.",
        "details": "Create `install.sh`:\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# ============================================================\n# AI Helper Tools — Bootstrap Installer\n# Usage: curl -fsSL https://raw.githubusercontent.com/.../install.sh | bash\n# ============================================================\n\nAI_HELPER_HOME=\"${AI_HELPER_HOME:-$HOME/.ai-helper-tools}\"\nBIN_DIR=\"${HOME}/.local/bin\"\nREPO_URL=\"https://github.com/potgieterdl/ai-helper-tools.git\"\nNODE_MIN_VERSION=20\n\necho \"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\necho \"  AI Helper Tools — Installer\"\necho \"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\n\n# 1. Check for Node.js ≥ 20, install via fnm if missing\nensure_node() {\n  if command -v node &>/dev/null; then\n    local version\n    version=$(node --version | sed 's/v//' | cut -d. -f1)\n    if [ \"$version\" -ge \"$NODE_MIN_VERSION\" ]; then\n      echo \"✓ Node.js $(node --version) found\"\n      return 0\n    fi\n    echo \"Node.js $(node --version) is too old (need >= ${NODE_MIN_VERSION}). Installing newer version via fnm...\"\n  else\n    echo \"Node.js not found. Installing via fnm...\"\n  fi\n\n  # Install fnm\n  curl -fsSL https://fnm.vercel.app/install | bash\n  export PATH=\"$HOME/.local/share/fnm:$PATH\"\n  eval \"$(fnm env)\"\n  fnm install \"$NODE_MIN_VERSION\"\n  fnm use \"$NODE_MIN_VERSION\"\n  fnm default \"$NODE_MIN_VERSION\"\n  echo \"✓ Node.js $(node --version) installed via fnm\"\n}\n\n# 2. Clone or update the repository\nensure_repo() {\n  if [ -d \"$AI_HELPER_HOME/.git\" ]; then\n    echo \"Updating ai-helper-tools...\"\n    git -C \"$AI_HELPER_HOME\" pull --ff-only\n  else\n    echo \"Cloning ai-helper-tools to $AI_HELPER_HOME...\"\n    git clone \"$REPO_URL\" \"$AI_HELPER_HOME\"\n  fi\n  echo \"✓ Repository ready at $AI_HELPER_HOME\"\n}\n\n# 3. Install dependencies\ninstall_deps() {\n  echo \"Installing dependencies...\"\n  npm ci --prefix \"$AI_HELPER_HOME\" --silent\n  npm run build --prefix \"$AI_HELPER_HOME\" --silent\n  echo \"✓ Dependencies installed\"\n}\n\n# 4. Symlink ai-init to PATH\nsetup_bin() {\n  mkdir -p \"$BIN_DIR\"\n  ln -sf \"$AI_HELPER_HOME/dist/cli.js\" \"$BIN_DIR/ai-init\"\n  chmod +x \"$AI_HELPER_HOME/dist/cli.js\"\n  echo \"✓ ai-init symlinked to $BIN_DIR/ai-init\"\n\n  # Ensure BIN_DIR is in PATH\n  if [[ \":$PATH:\" != *\":$BIN_DIR:\"* ]]; then\n    echo \"\"\n    echo \"Add this to your shell config (~/.bashrc or ~/.zshrc):\"\n    echo \"  export PATH=\\\"$BIN_DIR:\\$PATH\\\"\"\n  fi\n}\n\nensure_node\nensure_repo\ninstall_deps\nsetup_bin\n\necho \"\"\necho \"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\necho \"  Installation complete!\"\necho \"  Run 'ai-init' in any project directory to get started.\"\necho \"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\n```\n\nMake executable: `chmod +x install.sh`\n\nAdd `.gitignore` entries: `dist/`, `node_modules/`, `.ai-init-audit.md`",
        "testStrategy": "Manual test in a clean environment: run `bash install.sh` with Node.js absent (using Docker `node:alpine` with Node removed), verify fnm is installed and Node 20 is available after script runs. Verify `ai-init --version` works after install. Automated: test that `install.sh` is executable and contains `fnm.vercel.app/install`, `npm ci`, and symlink logic.",
        "priority": "high",
        "dependencies": [
          "16"
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": "18",
        "title": "Implement Agent Teams Configuration Generator",
        "description": "Create the agent teams opt-in configuration — updates `~/.claude/settings.json` with `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1` env flag and generates the `agent-teams.md` rule file. Implements F10.",
        "details": "Create `src/generators/agent-teams.ts`:\n\n```typescript\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport os from 'node:os';\nimport { ProjectConfig, FileDescriptor } from '../types.js';\n\nexport async function configureAgentTeams(\n  config: ProjectConfig\n): Promise<void> {\n  if (!config.agentTeamsEnabled) return;\n\n  // Update ~/.claude/settings.json (user-level setting)\n  const claudeSettingsPath = path.join(os.homedir(), '.claude', 'settings.json');\n  \n  let existing: Record<string, unknown> = {};\n  try {\n    const content = await fs.readFile(claudeSettingsPath, 'utf8');\n    existing = JSON.parse(content);\n  } catch {\n    // File doesn't exist — start fresh\n  }\n\n  const merged = {\n    ...existing,\n    env: {\n      ...((existing.env as Record<string, string>) ?? {}),\n      CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS: '1',\n    },\n  };\n\n  await fs.mkdir(path.dirname(claudeSettingsPath), { recursive: true });\n  await fs.writeFile(claudeSettingsPath, JSON.stringify(merged, null, 2), 'utf8');\n  console.log('[ai-init] Agent teams mode enabled in ~/.claude/settings.json');\n}\n```\n\nThe `agent-teams.md` rule is handled by the rules generator (Task 10) when `agentTeamsEnabled: true`. This function handles only the user-level settings file update, which cannot go through the normal `writeFiles()` mechanism (it's outside the project root).\n\n**Integration in post-create:** Call `configureAgentTeams(config)` at the end of `runPostCreate()` after all file generation.",
        "testStrategy": "Unit test `test/generators/agent-teams.test.ts`:\n1. With `agentTeamsEnabled: false` → `configureAgentTeams()` returns without writing any files\n2. With `agentTeamsEnabled: true` and no existing settings file → creates file with `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS: '1'`\n3. With existing settings file containing other keys → merges without overwriting existing keys\n4. With existing `env` block → merges the new key into existing env block",
        "priority": "low",
        "dependencies": [
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/generators/agent-teams.ts with configureAgentTeams function",
            "description": "Implement the agent-teams generator module that updates ~/.claude/settings.json with the CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1 env flag when agentTeamsEnabled is true.",
            "dependencies": [],
            "details": "Create `src/generators/agent-teams.ts` following the same pattern as `src/generators/hooks.ts`. The function must: (1) early-return when `config.agentTeamsEnabled` is false, (2) read existing `~/.claude/settings.json` if it exists (gracefully handling missing file), (3) merge in `env.CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS: '1'` preserving all existing keys, (4) create the `~/.claude/` directory if needed with `fs.mkdir({ recursive: true })`, (5) write the merged JSON back to `~/.claude/settings.json`, (6) log `[ai-init] Agent teams mode enabled in ~/.claude/settings.json`. Import `fs from 'node:fs/promises'`, `path from 'node:path'`, `os from 'node:os'`, and `type { ProjectConfig } from '../types.js'`. Note: do NOT import `FileDescriptor` since this function has no return value (returns `Promise<void>`).",
            "status": "done",
            "testStrategy": "Covered by unit tests in subtask 2.",
            "parentId": "undefined",
            "updatedAt": "2026-02-18T07:42:42.915Z"
          },
          {
            "id": 2,
            "title": "Write unit tests in test/generators/agent-teams.test.ts",
            "description": "Create a comprehensive unit test file that verifies all behaviour of configureAgentTeams, using a temp directory for the home dir simulation via environment stubbing.",
            "dependencies": [
              1
            ],
            "details": "Create `test/generators/agent-teams.test.ts` following the style of `test/generators/hooks.test.ts`. Use `vitest` (`describe`, `it`, `expect`, `beforeEach`, `afterEach`). In each test, redirect `os.homedir()` by mocking it (or by writing to a real `os.tmpdir()` sub-path). Tests to include: (1) with `agentTeamsEnabled: false` — function returns without writing any files; (2) with `agentTeamsEnabled: true` and no existing settings file — creates `~/.claude/settings.json` containing `env.CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS: '1'`; (3) with an existing settings file that has other keys — merges in the env flag without removing existing keys; (4) with existing settings that already have an `env` object with other vars — merges into the existing env object; (5) written file is valid JSON. Use `makeConfig` helper with `defaultConfig` + overrides pattern consistent with other test files.",
            "status": "done",
            "testStrategy": "Tests themselves are the test strategy; run with `npm test -- agent-teams`.",
            "parentId": "undefined",
            "updatedAt": "2026-02-18T07:43:18.715Z"
          },
          {
            "id": 3,
            "title": "Integrate configureAgentTeams call into runPostCreate in src/phases/post-create.ts",
            "description": "Import and call configureAgentTeams at the end of runPostCreate, after writeFiles, so the user-level settings update happens once all project files are written.",
            "dependencies": [
              1
            ],
            "details": "Edit `src/phases/post-create.ts`. Add import: `import { configureAgentTeams } from '../generators/agent-teams.js';`. After the `config.generatedFiles.push(...written)` line (and before the final `return written`), add: `await configureAgentTeams(config);`. This placement ensures: (a) all project-scoped files are already written, (b) the user-level settings update only runs when `config.agentTeamsEnabled` is true (the guard is inside `configureAgentTeams`). The function is async so the await is required. Do not change any other logic in `runPostCreate`.",
            "status": "done",
            "testStrategy": "Verify integration by running the full test suite (`npm test`) after the change. Specifically, existing post-create phase tests must still pass and not regress.",
            "parentId": "undefined",
            "updatedAt": "2026-02-18T07:43:47.986Z"
          },
          {
            "id": 4,
            "title": "Verify agent-teams.md rule template exists and is used by generateRules",
            "description": "Confirm the agent-teams.md rule template file exists under templates/rules/ and that generateRules already includes it when agentTeamsEnabled is true (Task 10 dependency), documenting that no additional rule generation logic is needed in this task.",
            "dependencies": [],
            "details": "Run `ls templates/rules/` to confirm `agent-teams.md` exists. Then read `src/generators/rules.ts` and verify the conditional block `if (config.agentTeamsEnabled)` reads and emits `.claude/rules/agent-teams.md`. Check that `test/generators/rules.test.ts` already has a passing test for this path (it does — see lines 74-86). If the template file is missing, create a minimal `templates/rules/agent-teams.md` with the agent-teams three-tier agent coordination content described in CLAUDE.md. This subtask is verification/guard-rail — no code change expected unless the template is absent.",
            "status": "done",
            "testStrategy": "Run `npm test -- rules` to confirm the conditional agent-teams rule tests pass. Visually inspect the template file content to ensure it describes the orchestrator/executor/checker roles.",
            "parentId": "undefined",
            "updatedAt": "2026-02-18T07:48:29.701Z"
          },
          {
            "id": 5,
            "title": "Run full quality gate and mark task complete",
            "description": "Execute the mandatory pre-completion checklist: format, lint, type-check, build, and full test suite. Fix any failures discovered.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Run the following commands in order per CLAUDE.md quality gate requirements: (1) `npm run format` — fix any formatting issues in new/changed files; (2) `npm run lint` — fix all lint errors (warnings acceptable); (3) `npm run typecheck` — resolve any TypeScript errors, particularly ensuring `configureAgentTeams` has correct return type annotations and no unused imports; (4) `npm run build` — verify zero compilation errors; (5) `npm test` — run the full Vitest suite and confirm all tests pass including the new `agent-teams.test.ts`. If any step fails, fix the issue and re-run from that step. After all checks pass, log findings via `update_subtask` and set task 18 status to `done`.",
            "status": "done",
            "testStrategy": "All five quality gate commands must exit with code 0. Test suite must show 0 failures across all test files including the new agent-teams tests.",
            "parentId": "undefined",
            "updatedAt": "2026-02-18T07:49:32.452Z"
          }
        ],
        "updatedAt": "2026-02-18T07:49:32.452Z"
      },
      {
        "id": "19",
        "title": "Write Integration Test Suite",
        "description": "Create a comprehensive integration test suite that runs `ai-init` against a temporary directory, verifies all generated files exist with correct content, and cleans up. This dogfoods the project's own integration-first testing philosophy from F9.",
        "details": "Create `test/integration/full-run.test.ts`:\n\n```typescript\nimport { describe, it, expect, beforeEach, afterEach } from 'vitest';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport os from 'node:os';\nimport { execFile } from 'node:child_process';\nimport { promisify } from 'node:util';\n\nconst execFileAsync = promisify(execFile);\nconst CLI_PATH = path.resolve('./dist/cli.js');\n\nasync function runCli(\n  args: string[],\n  cwd: string,\n  env: Record<string, string> = {}\n): Promise<{ stdout: string; stderr: string }> {\n  return execFileAsync('node', [CLI_PATH, ...args], {\n    cwd,\n    env: { ...process.env, ...env },\n  });\n}\n\ndescribe('ai-init integration', () => {\n  let tempDir: string;\n\n  beforeEach(async () => {\n    tempDir = await fs.mkdtemp(path.join(os.tmpdir(), 'ai-init-test-'));\n  });\n\n  afterEach(async () => {\n    await fs.rm(tempDir, { recursive: true, force: true });\n  });\n\n  it('smoke: --non-interactive creates core files', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_MCPS: 'taskmaster',\n      SETUP_AI_TRACKER: 'taskmaster',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    // Verify core files\n    const coreFiles = ['.mcp.json', '.vscode/mcp.json', 'CLAUDE.md', 'CLAUDE_MCP.md'];\n    for (const file of coreFiles) {\n      await expect(fs.access(path.join(tempDir, file))).resolves.toBeUndefined();\n    }\n  });\n\n  it('demo: task master tracker config is accurate', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_TRACKER: 'taskmaster',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    const claudeMd = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n    expect(claudeMd).toContain('task-master next');\n    expect(claudeMd).toContain('@./.taskmaster/CLAUDE.md');\n  });\n\n  it('demo: beads tracker config is accurate', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_TRACKER: 'beads',\n      SETUP_AI_MCPS: 'beads',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    const claudeMd = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n    expect(claudeMd).toContain('bd sync');\n    expect(claudeMd).toContain('beads_ready');\n  });\n\n  it('demo: doc generation creates all template files', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    const docFiles = ['docs/doc_format.md', 'docs/prd.md', 'docs/architecture.md',\n      'docs/testing_strategy.md', 'docs/onboarding.md', 'docs/cuj.md'];\n    for (const file of docFiles) {\n      await expect(fs.access(path.join(tempDir, file))).resolves.toBeUndefined();\n    }\n  });\n\n  it('demo: rules generation includes testing.md with integration-first philosophy', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    const testingRule = await fs.readFile(\n      path.join(tempDir, '.claude/rules/testing.md'),\n      'utf8'\n    );\n    expect(testingRule).toContain('Integration tests');\n    expect(testingRule).toContain('Demo checkpoints');\n    expect(testingRule).toContain('demo:');\n  });\n\n  it('demo: idempotency — running twice produces same result', async () => {\n    const env = { SETUP_AI_NONINTERACTIVE: '1', SETUP_AI_SKIP_AUDIT: '1' };\n    await runCli([], tempDir, env);\n    const firstRun = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n    await runCli([], tempDir, env);\n    const secondRun = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n    expect(firstRun).toBe(secondRun);\n  });\n\n  it('smoke: mcp.json is valid JSON with correct schema', async () => {\n    await runCli([], tempDir, {\n      SETUP_AI_NONINTERACTIVE: '1',\n      SETUP_AI_MCPS: 'taskmaster,context7',\n      SETUP_AI_SKIP_AUDIT: '1',\n    });\n\n    const mcpJson = await fs.readFile(path.join(tempDir, '.mcp.json'), 'utf8');\n    const parsed = JSON.parse(mcpJson);\n    expect(parsed).toHaveProperty('mcpServers');\n    expect(parsed.mcpServers).toHaveProperty('taskmaster-ai');\n    expect(parsed.mcpServers).toHaveProperty('context7');\n  });\n});\n```\n\nEach test:\n1. Creates fresh temp dir\n2. Runs CLI with env overrides\n3. Asserts specific outputs\n4. Cleans up in afterEach\n\nThis follows the demo-test naming pattern (`smoke:`, `demo:`) from the testing philosophy.",
        "testStrategy": "These tests ARE the test strategy — they are integration tests that prove the tool works end-to-end. Run with `npm test`. All tests must pass before any task is marked done. Verify test coverage includes at least: MCP generation, CLAUDE.md content, doc scaffolding, rules generation, and idempotency.",
        "priority": "high",
        "dependencies": [
          "16",
          "17"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create test/integration/full-run.test.ts with helper infrastructure",
            "description": "Set up the integration test file skeleton with shared helper functions, temp directory lifecycle hooks, and the runCli() wrapper that invokes dist/cli.js via execFile.",
            "dependencies": [],
            "details": "Create `test/integration/full-run.test.ts`. Import `describe`, `it`, `expect`, `beforeEach`, `afterEach` from vitest; import `fs` from `node:fs/promises`, `path` from `node:path`, `os` from `node:os`, `execFile` from `node:child_process`, `promisify` from `node:util`. Define `CLI_PATH = path.resolve('./dist/cli.js')`. Implement `runCli(args, cwd, env)` that calls `execFileAsync('node', [CLI_PATH, ...args], { cwd, env: { ...process.env, ...env } })` and returns `{ stdout, stderr }`. Add top-level `let tempDir: string` with `beforeEach` creating a temp dir via `fs.mkdtemp(path.join(os.tmpdir(), 'ai-init-test-'))` and `afterEach` cleaning it up via `fs.rm(tempDir, { recursive: true, force: true })`. Wrap all test cases in a single `describe('ai-init integration', ...)` block. Ensure the file is picked up by the existing vitest pattern `test/**/*.test.ts`.",
            "status": "done",
            "testStrategy": "Run `npm test` after creating the file skeleton — vitest should discover the file and pass (tests are stubs at this stage). Confirm `test/integration/full-run.test.ts` appears in the test run output.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:41:45.262Z"
          },
          {
            "id": 2,
            "title": "Add smoke test: non-interactive mode generates core files",
            "description": "Write the first test that runs the CLI in fully non-interactive mode and asserts that the four always-generated core files exist on disk in the temp directory.",
            "dependencies": [
              1
            ],
            "details": "Inside the `describe` block from subtask 1, add:\n```typescript\nit('smoke: --non-interactive creates core files', async () => {\n  await runCli([], tempDir, {\n    SETUP_AI_NONINTERACTIVE: '1',\n    SETUP_AI_MCPS: 'taskmaster',\n    SETUP_AI_TRACKER: 'taskmaster',\n    SETUP_AI_SKIP_AUDIT: '1',\n  });\n  const coreFiles = ['.mcp.json', '.vscode/mcp.json', 'CLAUDE.md', 'CLAUDE_MCP.md'];\n  for (const file of coreFiles) {\n    await expect(fs.access(path.join(tempDir, file))).resolves.toBeUndefined();\n  }\n});\n```\nThis test invokes the default wizard code path (no sub-command) with env overrides. `SETUP_AI_SKIP_AUDIT=1` prevents Claude Code audit from running. Requires `dist/cli.js` to exist (built beforehand). Ensure `npm run build` runs before the test suite to produce the dist artifact; update `package.json` test script to `\"tsc && vitest run\"` or document that `npm run build && npm test` is the required flow.",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. The smoke test should pass and all four core files must be found in the temp directory.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:41:50.589Z"
          },
          {
            "id": 3,
            "title": "Add MCP configuration tests (valid JSON schema, multiple servers)",
            "description": "Write tests that verify .mcp.json and .vscode/mcp.json are valid JSON, contain the expected `mcpServers` property, and include all requested MCP servers when SETUP_AI_MCPS lists multiple values.",
            "dependencies": [
              1
            ],
            "details": "Add two tests:\n1. `it('smoke: mcp.json is valid JSON with correct schema', ...)` — runs CLI with `SETUP_AI_MCPS: 'taskmaster,context7'`, reads `.mcp.json`, calls `JSON.parse()`, asserts `parsed.mcpServers` exists, `parsed.mcpServers['taskmaster-ai']` exists, and `parsed.mcpServers['context7']` exists.\n2. `it('smoke: .vscode/mcp.json mirrors .mcp.json structure', ...)` — reads `.vscode/mcp.json` and verifies it is also valid JSON with `mcpServers` property, confirming both output paths are written.\nFor both tests use env: `{ SETUP_AI_NONINTERACTIVE: '1', SETUP_AI_SKIP_AUDIT: '1', SETUP_AI_MCPS: 'taskmaster,context7' }`. Refer to `src/registry.ts` for canonical MCP server IDs (e.g., `taskmaster-ai`, `context7`).",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. Both MCP tests must pass; JSON.parse must not throw, and all required properties must be present.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:41:55.916Z"
          },
          {
            "id": 4,
            "title": "Add task tracker content tests (taskmaster and beads)",
            "description": "Write demo-labelled tests that verify the generated CLAUDE.md contains tracker-specific content depending on the SETUP_AI_TRACKER env variable.",
            "dependencies": [
              1
            ],
            "details": "Add two tests following the `demo:` naming convention:\n1. `it('demo: task master tracker config is accurate', ...)` — env `{ SETUP_AI_NONINTERACTIVE: '1', SETUP_AI_TRACKER: 'taskmaster', SETUP_AI_SKIP_AUDIT: '1' }`. Reads `CLAUDE.md` as utf8 and asserts it contains `'task-master next'` and `'@./.taskmaster/CLAUDE.md'`.\n2. `it('demo: beads tracker config is accurate', ...)` — env `{ SETUP_AI_NONINTERACTIVE: '1', SETUP_AI_TRACKER: 'beads', SETUP_AI_MCPS: 'beads', SETUP_AI_SKIP_AUDIT: '1' }`. Reads `CLAUDE.md` and asserts it contains `'bd sync'` and `'beads_ready'`.\nThese tests exercise the `src/generators/claude-md.ts` tracker-conditional logic through the full CLI stack.",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. Both tracker tests pass. Inspect actual CLAUDE.md output if assertions fail to identify the exact strings produced by the generators.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:42:01.595Z"
          },
          {
            "id": 5,
            "title": "Add document scaffolding test",
            "description": "Write a demo test that verifies all six standard documentation template files are created under docs/ when the CLI runs with default settings.",
            "dependencies": [
              1
            ],
            "details": "Add:\n```typescript\nit('demo: doc generation creates all template files', async () => {\n  await runCli([], tempDir, {\n    SETUP_AI_NONINTERACTIVE: '1',\n    SETUP_AI_SKIP_AUDIT: '1',\n  });\n  const docFiles = [\n    'docs/doc_format.md',\n    'docs/prd.md',\n    'docs/architecture.md',\n    'docs/testing_strategy.md',\n    'docs/onboarding.md',\n    'docs/cuj.md',\n  ];\n  for (const file of docFiles) {\n    await expect(fs.access(path.join(tempDir, file))).resolves.toBeUndefined();\n  }\n});\n```\nThe default config from `src/defaults.ts` enables `generateDocs: true`, so all docs should be generated. Verify the template files exist in `templates/docs/` to confirm the expected output file names match the generator logic in `src/generators/docs.ts`.",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. The doc generation test passes with all six doc paths accessible. Cross-check with `src/generators/docs.ts` if any doc file is missing.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:42:07.149Z"
          },
          {
            "id": 6,
            "title": "Add rules/skills/hooks generation test with content assertion",
            "description": "Write a demo test verifying that .claude/rules/testing.md is generated and contains the integration-first testing philosophy strings required by F9.",
            "dependencies": [
              1
            ],
            "details": "Add:\n```typescript\nit('demo: rules generation includes testing.md with integration-first philosophy', async () => {\n  await runCli([], tempDir, {\n    SETUP_AI_NONINTERACTIVE: '1',\n    SETUP_AI_SKIP_AUDIT: '1',\n  });\n  const testingRule = await fs.readFile(\n    path.join(tempDir, '.claude/rules/testing.md'),\n    'utf8'\n  );\n  expect(testingRule).toContain('Integration tests');\n  expect(testingRule).toContain('Demo checkpoints');\n  expect(testingRule).toContain('demo:');\n});\n```\nAlso add a companion assertion block for skills and hooks within the same test or as a second test `it('smoke: skills and hooks are generated', ...)` that checks `.claude/skills/testing.md` exists and `.claude/hooks/pre-commit.sh` exists. Verify against `templates/rules/testing.md` for expected string content.",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. Test passes. Read `templates/rules/testing.md` to confirm the strings `Integration tests`, `Demo checkpoints`, and `demo:` are present in the template source.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:42:12.491Z"
          },
          {
            "id": 7,
            "title": "Add idempotency test (running CLI twice yields same output)",
            "description": "Write a demo test that runs the CLI twice against the same temp directory and asserts that CLAUDE.md content is byte-for-byte identical across both runs.",
            "dependencies": [
              1
            ],
            "details": "Add:\n```typescript\nit('demo: idempotency — running twice produces same result', async () => {\n  const env = { SETUP_AI_NONINTERACTIVE: '1', SETUP_AI_SKIP_AUDIT: '1' };\n  await runCli([], tempDir, env);\n  const firstRun = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n  await runCli([], tempDir, env);\n  const secondRun = await fs.readFile(path.join(tempDir, 'CLAUDE.md'), 'utf8');\n  expect(firstRun).toBe(secondRun);\n});\n```\nThis test validates that the CLI's default `overwrite: true` behaviour produces stable, deterministic output. Also optionally read `.mcp.json` and assert equality across both runs to cover JSON generation idempotency. The test relies on `runPostCreate`'s file-writing logic not appending or generating non-deterministic content (e.g., timestamps).",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. Test passes. If the test reveals non-deterministic output (e.g., a timestamp field), that is a bug in the generator that must be fixed before the test can be marked done.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:42:17.945Z"
          },
          {
            "id": 8,
            "title": "Add CLI lifecycle sub-command tests (post-create, post-start)",
            "description": "Write smoke tests for the `post-create` and `post-start` CLI sub-commands to verify each lifecycle command runs without error and produces expected side effects.",
            "dependencies": [
              1
            ],
            "details": "Add two tests:\n1. `it('smoke: post-create command generates core files', ...)` — calls `runCli(['post-create'], tempDir, { SETUP_AI_SKIP_AUDIT: '1' })` and asserts `.mcp.json` and `CLAUDE.md` exist.\n2. `it('smoke: post-start command runs without error', ...)` — calls `runCli(['post-start'], tempDir, {})` and asserts the promise resolves (no throw). Optionally reads `.env` to confirm env-sync ran if `ANTHROPIC_API_KEY` is set in the test environment.\nFor the `post-create` command, note from `src/cli.ts:62` that it forces `SETUP_AI_NONINTERACTIVE=1` internally, so no env override is needed for non-interactive mode — only `SETUP_AI_SKIP_AUDIT=1` is required. For `post-start`, it uses `defaultConfig(projectRoot)` directly without a wizard, so it always succeeds even with an empty directory.",
            "status": "done",
            "testStrategy": "Run `npm run build && npm test`. Both lifecycle tests pass. Verify the full test suite (all tests in `test/integration/full-run.test.ts`) passes with `npm test` and that no existing tests in `test/integration/phases.test.ts` are broken.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T22:42:23.203Z"
          }
        ],
        "updatedAt": "2026-02-17T22:42:23.203Z"
      },
      {
        "id": "20",
        "title": "Configure Linting, Formatting, and Build Pipeline",
        "description": "Set up ESLint, Prettier, and the TypeScript build pipeline. Ensure `npm run lint`, `npm run format`, `npm run typecheck`, and `npm run build` all work correctly. This is the quality gate infrastructure that all other tasks depend on.",
        "details": "Create `.eslintrc.cjs`:\n```javascript\nmodule.exports = {\n  root: true,\n  parser: '@typescript-eslint/parser',\n  plugins: ['@typescript-eslint'],\n  extends: [\n    'eslint:recommended',\n    'plugin:@typescript-eslint/recommended',\n  ],\n  env: { node: true, es2022: true },\n  rules: {\n    '@typescript-eslint/no-unused-vars': ['error', { argsIgnorePattern: '^_' }],\n    '@typescript-eslint/explicit-module-boundary-types': 'off',\n    'no-console': 'off',\n  },\n};\n```\n\nCreate `.prettierrc`:\n```json\n{\n  \"semi\": true,\n  \"singleQuote\": true,\n  \"trailingComma\": \"es5\",\n  \"printWidth\": 100,\n  \"tabWidth\": 2\n}\n```\n\nCreate `.prettierignore`:\n```\ndist/\nnode_modules/\ntemplates/\n```\n\nUpdate `package.json` scripts:\n```json\n{\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"dev\": \"tsx src/cli.ts\",\n    \"test\": \"vitest run\",\n    \"test:watch\": \"vitest\",\n    \"lint\": \"eslint src test --ext .ts --fix\",\n    \"lint:check\": \"eslint src test --ext .ts\",\n    \"typecheck\": \"tsc --noEmit\",\n    \"format\": \"prettier --write 'src/**/*.ts' 'test/**/*.ts'\",\n    \"format:check\": \"prettier --check 'src/**/*.ts' 'test/**/*.ts'\"\n  }\n}\n```\n\nUpdate `.gitignore` to include:\n```\ndist/\nnode_modules/\n.ai-init-audit.md\n*.js.map\n```\n\nVerify the build pipeline: `npm run build` produces `dist/cli.js` with shebang, `npm run lint` passes on all source files, `npm run typecheck` shows zero errors.",
        "testStrategy": "Verify all pipeline commands exit with code 0 on a clean checkout: `npm run format:check`, `npm run lint:check`, `npm run typecheck`, `npm run build`. The CI gate: run all four commands sequentially and fail if any returns non-zero. Also verify that `dist/cli.js` exists after build and the shebang `#!/usr/bin/env node` is present on line 1.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create ESLint configuration file (.eslintrc.cjs)",
            "description": "Create the .eslintrc.cjs file in the project root with TypeScript-aware ESLint rules using @typescript-eslint parser and plugin.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/.eslintrc.cjs` with: `root: true`, parser set to `@typescript-eslint/parser`, plugins array with `@typescript-eslint`, extends from `eslint:recommended` and `plugin:@typescript-eslint/recommended`, env set to `{ node: true, es2022: true }`, and rules: `@typescript-eslint/no-unused-vars` as error with `argsIgnorePattern: '^_'`, `@typescript-eslint/explicit-module-boundary-types` off, `no-console` off. Use `module.exports = { ... }` CommonJS syntax (required because the file uses .cjs extension in an ESM project with `\"type\": \"module\"` in package.json).",
            "status": "done",
            "testStrategy": "Run `npx eslint src --ext .ts` and verify it exits with code 0 and correctly identifies any intentional lint errors introduced as a test.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:54:06.833Z"
          },
          {
            "id": 2,
            "title": "Create Prettier configuration files (.prettierrc and .prettierignore)",
            "description": "Create .prettierrc with formatting rules and .prettierignore to exclude generated/dependency directories from formatting.",
            "dependencies": [],
            "details": "Create `/workspaces/ai-dev-setup/.prettierrc` as JSON with: `semi: true`, `singleQuote: true`, `trailingComma: 'es5'`, `printWidth: 100`, `tabWidth: 2`. Create `/workspaces/ai-dev-setup/.prettierignore` with entries: `dist/`, `node_modules/`, `templates/`. These files ensure consistent code style across the project and prevent Prettier from reformatting generated output files or template files that must remain in their original form.",
            "status": "done",
            "testStrategy": "Run `npx prettier --check 'src/**/*.ts'` and verify it exits cleanly, confirming the config is valid and recognized by Prettier.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:54:36.439Z"
          },
          {
            "id": 3,
            "title": "Update package.json scripts for full quality pipeline",
            "description": "Replace/augment the scripts section in package.json to add lint:check, format:check, and update existing lint/format scripts to match the required pipeline commands.",
            "dependencies": [
              1,
              2
            ],
            "details": "Edit `/workspaces/ai-dev-setup/package.json` scripts section. The current scripts are missing `lint:check` and `format:check` variants needed for CI. Update `lint` to `eslint src test --ext .ts --fix`, add `lint:check` as `eslint src test --ext .ts`, update `format` to `prettier --write 'src/**/*.ts' 'test/**/*.ts'`, add `format:check` as `prettier --check 'src/**/*.ts' 'test/**/*.ts'`. Keep existing `build`, `dev`, `test`, `test:watch`, and `typecheck` scripts unchanged. The `test` directory may not exist yet — the lint scripts should not fail on missing directories (already handled by the current `--no-error-on-unmatched-pattern` flag, which should be preserved or the `test` directory created).",
            "status": "done",
            "testStrategy": "Run `npm run lint:check` and `npm run format:check` and verify both exit with code 0. Run `npm run lint` and `npm run format` and verify they complete without errors.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:55:19.606Z"
          },
          {
            "id": 4,
            "title": "Update .gitignore with missing entries",
            "description": "Add .ai-init-audit.md and *.js.map to .gitignore since the current file already has dist/ and node_modules/ but is missing these entries.",
            "dependencies": [],
            "details": "Edit `/workspaces/ai-dev-setup/.gitignore` to add `.ai-init-audit.md` (audit files generated by the tool should not be committed to the template repo) and `*.js.map` (source map files produced by `tsc` when `sourceMap` is enabled). The current .gitignore already correctly includes `dist/`, `node_modules/`, `.env`, and `*.tsbuildinfo`. Add these two entries under the Build output section alongside `*.tsbuildinfo`. Do not add `.vscode` — it is already present in the editor section.",
            "status": "done",
            "testStrategy": "Run `git status` after adding an `.ai-init-audit.md` file and verify it does not appear as an untracked file. Verify `git check-ignore -v .ai-init-audit.md` reports a match.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:55:42.534Z"
          },
          {
            "id": 5,
            "title": "Verify full build pipeline executes successfully end-to-end",
            "description": "Run all four quality gate commands sequentially and confirm each exits with code 0: npm run format:check, npm run lint:check, npm run typecheck, npm run build. Verify dist/cli.js is produced with a shebang line.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Execute the complete pipeline in order: (1) `npm run format:check` — verifies all src TypeScript files match Prettier config; (2) `npm run lint:check` — verifies ESLint passes on all src files; (3) `npm run typecheck` — runs `tsc --noEmit` and verifies zero TypeScript errors; (4) `npm run build` — runs `tsc` and produces output in `dist/`. After build, verify `dist/cli.js` exists and its first line is `#!/usr/bin/env node` (shebang). If any step fails, diagnose and fix: common issues include unused imports caught by ESLint, formatting mismatches in existing src files (run `npm run format` to fix), or TypeScript strict-mode errors in existing src files. Fix all issues in the source files to make the pipeline green.",
            "status": "done",
            "testStrategy": "Run each command and check exit codes: `npm run format:check && npm run lint:check && npm run typecheck && npm run build`. Then run `head -1 dist/cli.js` and confirm output is `#!/usr/bin/env node`. The entire sequence must complete with exit code 0.",
            "parentId": "undefined",
            "updatedAt": "2026-02-17T20:56:37.998Z"
          }
        ],
        "updatedAt": "2026-02-17T20:56:37.998Z"
      },
      {
        "id": "21",
        "title": "Write Unit Tests for All Generators",
        "description": "Create comprehensive unit tests for every generator function. Tests use pure function input/output — no temp directories, no mocks, just assert on returned FileDescriptor content. Dogfoods the project's integration-first testing philosophy.",
        "details": "Create the following test files:\n\n**`test/generators/mcp-json.test.ts`** (per Task 7 test strategy)\n\n**`test/generators/claude-md.test.ts`** (per Task 8 test strategy)\n\n**`test/generators/docs.test.ts`** (per Task 9 test strategy)\n\n**`test/generators/rules.test.ts`** (per Task 10 test strategy)\n\n**`test/generators/hooks.test.ts`** (per Task 10 test strategy)\n\n**`test/generators/devcontainer.test.ts`** (per Task 11 test strategy)\n\n**`test/generators/commands.test.ts`** (per Task 12 test strategy)\n\nEach test file follows this pattern:\n```typescript\nimport { describe, it, expect } from 'vitest';\nimport { generateMcpJson } from '../../src/generators/mcp-json.js';\nimport { defaultConfig } from '../../src/defaults.js';\n\ndescribe('generateMcpJson', () => {\n  it('demo: taskmaster config uses correct root key for Claude Code', () => {\n    const config = { ...defaultConfig('/tmp/test'), selectedMcps: ['taskmaster'] };\n    const files = generateMcpJson(config);\n    const mcpFile = files.find(f => f.path === '.mcp.json')!;\n    const parsed = JSON.parse(mcpFile.content);\n    expect(parsed).toHaveProperty('mcpServers');\n    expect(parsed.mcpServers).toHaveProperty('taskmaster-ai');\n  });\n\n  it('demo: vscode config uses correct root key for VS Code', () => {\n    const config = { ...defaultConfig('/tmp/test'), selectedMcps: ['taskmaster'] };\n    const files = generateMcpJson(config);\n    const vsFile = files.find(f => f.path === '.vscode/mcp.json')!;\n    const parsed = JSON.parse(vsFile.content);\n    expect(parsed).toHaveProperty('servers');\n    expect(parsed.servers['taskmaster-ai']).toHaveProperty('cwd');\n  });\n});\n```\n\nKey requirement: Every test uses the `smoke:` or `demo:` naming convention and tests real behavior (JSON parsing, content matching, file counts) — no trivial assertions like `expect(true).toBe(true)`.\n\nTotal test count target: ≥ 40 unit tests across all generators.",
        "testStrategy": "Run `npm test` — all 40+ unit tests must pass. Run `npm run typecheck` — zero type errors in test files. Verify test output shows individual test names matching `smoke:` or `demo:` naming convention. Coverage report (if configured) should show > 80% branch coverage on generator files.",
        "priority": "high",
        "dependencies": [
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "19",
          "20"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Write unit tests for mcp-json and devcontainer generators",
            "description": "Create test/generators/mcp-json.test.ts and test/generators/devcontainer.test.ts with pure function input/output tests covering all config permutations.",
            "dependencies": [],
            "details": "For mcp-json: test that .mcp.json uses 'mcpServers' root key, .vscode/mcp.json uses 'servers' root key with 'cwd' field, all 5 registry servers produce correct entries, empty selectedMcps produces empty objects, VS Code env vars use '${env:VAR}' syntax. For devcontainer: test that output is exactly 1 FileDescriptor at '.devcontainer/devcontainer.json', parsed JSON has onCreateCommand 'ai-init on-create', postCreateCommand 'ai-init post-create', postStartCommand 'ai-init post-start', ANTHROPIC_API_KEY secret always present, MCP-derived secrets filtered to _KEY/_SECRET/_TOKEN suffixes, no duplicate secrets. Use makeConfig() helper for override merging. All test names use 'demo:' or 'smoke:' prefix.",
            "status": "done",
            "testStrategy": "Run npm test -- test/generators/mcp-json.test.ts test/generators/devcontainer.test.ts; verify all tests pass and names follow smoke:/demo: convention.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Write unit tests for claude-md and docs generators",
            "description": "Create test/generators/claude-md.test.ts and test/generators/docs.test.ts covering conditional content generation based on taskTracker, generateDocs, hasApiDocs, and architecture flags.",
            "dependencies": [
              1
            ],
            "details": "For claude-md: test taskmaster variant includes '@./.taskmaster/CLAUDE.md' import and 'task-master next'; beads variant includes 'beads_ready' and 'bd sync'; markdown variant includes 'TASKS.md'; no MCPs → 1 file output; with MCPs → CLAUDE_MCP.md generated; quality gate section always present. For docs: test base file count is 7 (doc_format, prd, architecture, cuj, testing_strategy, onboarding, adr_template); +1 if hasApiDocs; +1 if taskTracker==='markdown' (TASKS.md); PROJECT_NAME placeholder substituted correctly; ARCHITECTURE placeholder substituted; DATE placeholder present in some files. All tests use pure function calls with no temp directories.",
            "status": "done",
            "testStrategy": "Run npm test on both files; verify file count assertions, content markers, and placeholder substitution all pass.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Write unit tests for rules and skills generators",
            "description": "Create test/generators/rules.test.ts and test/generators/skills.test.ts covering conditional rule generation and always-on skills output.",
            "dependencies": [
              1
            ],
            "details": "For rules: test base count is 6 (general, docs, testing, git, security, config); +1 if hasApiDocs (api.md); api.md content includes '@docs/api.md' import when generateDocs=true; +1 if hasDatabase (database.md); +1 if agentTeamsEnabled (agent-teams.md); PROJECT_NAME and TASK_TRACKER placeholders substituted; no leftover '{{...}}' markers. For skills: test output is always exactly 3 files (testing.md, commit.md, task-workflow.md); each has YAML frontmatter with description field; content includes Integration-First section; TASK_TRACKER placeholder substituted with actual tracker value. Use makeConfig() with flag overrides. All test names use 'demo:' or 'smoke:' prefix.",
            "status": "done",
            "testStrategy": "Run npm test on rules.test.ts and skills.test.ts; confirm conditional file counts, YAML frontmatter validity, and no unresolved placeholders.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Write unit tests for hooks and commands generators",
            "description": "Create test/generators/hooks.test.ts and test/generators/commands.test.ts covering the pre-commit shell script, settings.json hooks config, and tracker-specific command files.",
            "dependencies": [
              1
            ],
            "details": "For hooks: test output is exactly 2 files; pre-commit.sh has executable flag set to true; content starts with '#!/usr/bin/env bash' shebang; contains 'set -euo pipefail'; contains ordered steps for format, lint, typecheck, build, test using npm run --if-present; settings.json is valid JSON; settings.json hooks section has PreToolUse matcher for 'Bash(git commit)' pointing to pre-commit.sh path. For commands: test output is always exactly 3 files (dev-next.md, review.md, boot-prompt.txt); taskmaster variant has 'task-master next' in dev-next.md; beads variant has 'bd show' in dev-next.md; markdown variant has 'TASKS.md' reference; review.md contains quality gate references; PROJECT_NAME substituted; no unresolved '{{...}}' placeholders remain.",
            "status": "done",
            "testStrategy": "Run npm test on hooks.test.ts and commands.test.ts; verify executable flag, bash syntax markers, JSON validity, tracker-specific content, and placeholder resolution.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Verify total test count meets ≥40 target and all tests pass",
            "description": "Run the complete test suite, confirm all generator unit tests pass, count meets the ≥40 threshold, naming convention is followed, and typecheck reports zero errors.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Run 'npm test' and verify: all test files in test/generators/ pass with zero failures; total unit tests across mcp-json (target ≥6), claude-md (target ≥6), docs (target ≥6), rules (target ≥6), hooks (target ≥5), devcontainer (target ≥5), commands (target ≥6) sum to at least 40; test names consistently use 'smoke:' or 'demo:' prefix patterns; no trivial assertions like expect(true).toBe(true). Run 'npm run typecheck' and confirm zero TypeScript errors in test files. If any tests fail or type errors exist, fix them before marking task done. Log final test counts and typecheck result in subtask update.",
            "status": "done",
            "testStrategy": "npm test output shows all generator tests passing; npm run typecheck exits 0; grep for 'smoke:' and 'demo:' in test files confirms naming convention across all 7 required test files.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-02-17T22:57:25.352Z"
      },
      {
        "id": "22",
        "title": "Add README and Usage Documentation",
        "description": "Update `README.md` with installation instructions, feature overview, environment variable reference, and examples. The README is the user's first touchpoint — it must make the single-line install immediately obvious.",
        "details": "Update `README.md` with:\n\n**Header section:**\n- One-line description\n- Badges (license, node version)\n- Single-line install command prominently displayed:\n  ```bash\n  curl -fsSL https://raw.githubusercontent.com/potgieterdl/ai-helper-tools/main/install.sh | bash\n  ```\n\n**Usage section:**\n```bash\ncd my-project\nai-init                    # Interactive wizard\nai-init --non-interactive  # Env-var driven\nai-init on-create          # Codespace: heavy installs\nai-init post-create        # Codespace: project scaffolding\nai-init post-start         # Codespace: per-session setup\n```\n\n**What gets generated table:**\n| File/Directory | Purpose |\n|---|---|\n| `CLAUDE.md` | Agent instructions (auto-loaded by Claude Code) |\n| `.mcp.json` | MCP servers for Claude Code CLI |\n| `.vscode/mcp.json` | MCP servers for VS Code / Copilot |\n| `docs/` | Agent-optimized project documentation |\n| `.claude/rules/` | Path-scoped agent rules |\n| `.claude/skills/` | Keyword-activated agent knowledge |\n| `.claude/hooks/` | Pre-commit quality gate |\n| `.claude/commands/` | `/dev-next` and `/review` slash commands |\n| `.devcontainer/devcontainer.json` | Codespace lifecycle hooks |\n\n**Environment variables table** (all SETUP_AI_* vars)\n\n**Task tracker comparison table** (Task Master vs Beads vs Markdown)\n\n**Development section:**\n```bash\ngit clone ...\nnpm ci\nnpm run build\nnpm test\nnpm run dev -- --help\n```\n\nKeep README under 200 lines following the project's own doc_format.md standard.",
        "testStrategy": "Verify README.md contains: install curl command, all subcommand examples, the generated files table, environment variable reference. Verify README.md is under 200 lines. Verify all links in README are relative and valid (no broken anchors). Run `markdownlint README.md` if available.",
        "priority": "medium",
        "dependencies": [
          "17",
          "19"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "23",
        "title": "Verify if Bash Logic conceptually the same",
        "description": "Ensure the TypeScript CLI conceptually matches what we had in the existing `setup-ai.sh` bash script. We have built a new unitility so it wont match the exact funcionality and this ok.",
        "details": "Audit `setup-ai.sh` against the TypeScript implementation:\n\n**Bash → TypeScript function mapping:**\n| Bash function | TypeScript equivalent |\n|---|---|\n| `select_mcps()` | `wizard.ts` Step 1 + env var handling |\n| `write_mcp_json()` | `generators/mcp-json.ts:generateMcpJson()` |\n| `write_claude_md()` | `generators/claude-md.ts:generateClaudeMd()` |\n| `write_devcontainer_json()` | `generators/devcontainer.ts:generateDevcontainer()` |\n| `phase_on_create()` | `phases/on-create.ts:runOnCreate()` |\n| `phase_post_create()` | `phases/post-create.ts:runPostCreate()` |\n| `phase_post_start()` | `phases/post-start.ts:runPostStart()` |\n| Interactive MCP menu | `wizard.ts` `@inquirer/prompts` checkbox |\n| `.setup-ai-mcps` cache file | Read in `post-start.ts` for session context |\n| Welcome banner + task count | `phases/post-start.ts:runPostStart()` |\n| Shell config injection (TMPDIR) | `phases/on-create.ts` |\n| Pre-commit hook generation | `generators/hooks.ts:generateHooks()` |\n\n**Compatibility verification:**\n1. Existing `.devcontainer/devcontainer.json` calls `setup-ai.sh on-create|post-create|post-start` — update these to `ai-init on-create|post-create|post-start`\n2. Existing `.setup-ai-mcps` file (contains `taskmaster`) — `post-start.ts` should read this for context\n3. CLAUDE.md regeneration — the `<!-- SETUP-AI-MANAGED -->` header is preserved\n4. MCP config parity — verify the TypeScript-generated `.mcp.json` matches the bash-generated one for identical inputs\n\n**Transition path:** \n- Keep `setup-ai.sh` intact during development (it's the reference implementation)\n- Add a deprecation notice to `setup-ai.sh` pointing to `ai-init`\n- Update `.devcontainer/devcontainer.json` to use `ai-init` once tests pass",
        "testStrategy": "Perform end to end testing of the new binary",
        "priority": "medium",
        "dependencies": [
          "16",
          "17",
          "19"
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-02-18T07:49:32.452Z",
      "taskCount": 23,
      "completedCount": 21,
      "tags": [
        "master"
      ]
    }
  }
}