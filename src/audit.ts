import fs from "node:fs/promises";
import path from "node:path";
import { commandExists, run } from "./utils.js";
import type { ProjectConfig } from "./types.js";

/**
 * Structured audit prompt template for Claude Code headless mode.
 * {{GENERATED_FILES}} is replaced with the actual file list at runtime.
 *
 * The audit is scoped exclusively to files generated during the current
 * wizard run — it never reviews pre-existing project code.
 */
const AUDIT_PROMPT_TEMPLATE = `You are auditing the output of the ai-init project bootstrap tool.
Review ONLY the files listed below — these were just generated by the setup wizard.
Do NOT review or comment on any other files in the project.

Generated files:
{{GENERATED_FILES}}

Audit checklist:
1. STRUCTURE: Are all generated docs following the format defined in docs/doc_format.md?
   Check: TOC present, sections <30 lines, tables used for structured data, TLDR at top.

2. CROSS-REFERENCES: Does CLAUDE.md accurately reference all generated docs?
   Check: Every @import and link points to a file that exists. No broken references.

3. RULES CONSISTENCY: Do .claude/rules/ files reference correct path patterns?
   Check: Path globs in frontmatter match the project's actual directory structure.
   Check: Rules that @import docs reference docs that were actually generated.

4. MCP CONFIG: Is .mcp.json valid JSON with correct package names and args?
   Check: All selected MCP servers are present. No duplicate entries.

5. TEMPLATE COMPLETENESS: Which template sections still have placeholder content
   that the user MUST fill in before agents can work effectively?
   Flag: List each file and the specific sections that need user input.

6. GAPS: What is missing that the user should address manually?
   Flag: Missing docs that would help agents (e.g., no API doc for an API project),
   rules that reference tools not yet installed, skills that assume config not present.

7. PROMPTS & INSTRUCTIONS: Are the agent instructions in CLAUDE.md and rules
   well-structured, specific, and actionable? Flag any that are vague or generic.

Output format:
- PASS: <area> — <one-line summary>
- FILL: <file>:<section> — <what the user needs to add>
- FIX: <file>:<issue> — <what's wrong and how to fix it>

End with a "Post-Setup Checklist" — a numbered list of manual actions
the user should take before starting development with an AI agent.`;

/**
 * Check if Claude Code is installed and can run.
 * Returns true only if the `claude` binary is on PATH and responds to --version.
 */
export async function checkClaudeCodeAvailable(): Promise<boolean> {
  if (!(await commandExists("claude"))) {
    return false;
  }
  try {
    await run("claude", ["--version"]);
    return true;
  } catch {
    return false;
  }
}

/**
 * Install Claude Code globally via npm.
 * Called during Step 0 (bootstrap) if Claude is not found on PATH.
 */
export async function installClaudeCode(): Promise<void> {
  console.log("[ai-init] Installing Claude Code...");
  await run("npm", ["install", "-g", "@anthropic-ai/claude-code"]);
  console.log("[ai-init] Claude Code installed.");
}

/**
 * Build the audit prompt by replacing the {{GENERATED_FILES}} placeholder
 * with the actual list of generated file paths.
 */
export function buildAuditPrompt(generatedFiles: string[]): string {
  const filesList = generatedFiles.map((f) => `  - ${f}`).join("\n");
  return AUDIT_PROMPT_TEMPLATE.replace("{{GENERATED_FILES}}", filesList);
}

/**
 * Run Claude Code in headless mode to audit the generated files.
 *
 * Graceful degradation:
 *   - Claude not installed → skip with warning message
 *   - No API credentials → skip with warning message
 *   - Audit errors → catch, warn, continue (never throw)
 *   - Never blocks wizard completion
 *
 * When successful, prints audit results to stdout and saves them
 * to .ai-init-audit.md in the project root.
 */
export async function runAudit(
  config: ProjectConfig,
  generatedFiles: string[]
): Promise<string | undefined> {
  if (generatedFiles.length === 0) {
    console.warn("[ai-init] Skipping audit — no files were generated.");
    return undefined;
  }

  if (!(await checkClaudeCodeAvailable())) {
    console.warn("[ai-init] Skipping audit — Claude Code not available.");
    return undefined;
  }

  const prompt = buildAuditPrompt(generatedFiles);

  console.log("[ai-init] Running AI-powered audit of generated files...");

  let auditOutput: string;
  try {
    auditOutput = await run("claude", ["--print", prompt], config.projectRoot);
  } catch (err) {
    console.warn("[ai-init] Audit failed — review generated files manually.");
    console.warn(err);
    return undefined;
  }

  // Save audit results to project root
  const auditPath = path.join(config.projectRoot, ".ai-init-audit.md");
  const auditContent = `# AI Init Audit Results\n\n${auditOutput}\n`;
  await fs.writeFile(auditPath, auditContent, "utf8");

  console.log("\n--- AUDIT RESULTS ---");
  console.log(auditOutput);
  console.log(`\nAudit saved to: .ai-init-audit.md`);

  return auditOutput;
}
